[{"path":"https://aravind-j.github.io/avial/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"J. Aravind. Author, maintainer.","code":""},{"path":"https://aravind-j.github.io/avial/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Aravind, J. ().  avial: Mixed Bag R Functions. R package version 0.0.0.9000, https://aravind-j.github.io/avial/.","code":"@Manual{,   title = {avial: A Mixed Bag of R Functionss},   author = {J. Aravind},   note = {R package version 0.0.0.9000 https://aravind-j.github.io/avial/}, }"},{"path":[]},{"path":[]},{"path":"https://aravind-j.github.io/avial/index.html","id":"aravind-j","dir":"","previous_headings":"avial: A Mixed Bag of R Functions","what":"Aravind, J.","title":"A Mixed Bag of R Functions","text":"Division Germplasm Conservation, ICAR-National Bureau Plant Genetic Resources, New Delhi.","code":""},{"path":"https://aravind-j.github.io/avial/index.html","id":"description","dir":"","previous_headings":"","what":"Description","title":"A Mixed Bag of R Functions","text":"bunch miscellaneous R functions primarly personal use.functions may migrated packages future required.","code":""},{"path":"https://aravind-j.github.io/avial/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Mixed Bag of R Functions","text":"development version can installed github follows:","code":"# Install development version from Github devtools::install_github(\"aravind-j/avial\")"},{"path":"https://aravind-j.github.io/avial/index.html","id":"whats-new","dir":"","previous_headings":"","what":"What’s new","title":"A Mixed Bag of R Functions","text":"know whats new version type:","code":"news(package='avial')"},{"path":"https://aravind-j.github.io/avial/index.html","id":"links","dir":"","previous_headings":"","what":"Links","title":"A Mixed Bag of R Functions","text":"Github page Documentation website","code":""},{"path":"https://aravind-j.github.io/avial/index.html","id":"citing-avial","dir":"","previous_headings":"","what":"Citing avial","title":"A Mixed Bag of R Functions","text":"cite methods package use:","code":"citation(\"avial\") To cite the R package 'augmentedRCBD' in publications use:    Aravind, J. ().  avial: A Mixed Bag of R Functions. R package version   0.0.0.9000, https://aravind-j.github.io/avial/.  A BibTeX entry for LaTeX users is    @Manual{,     title = {avial: A Mixed Bag of R Functionss},     author = {J. Aravind},     note = {R package version 0.0.0.9000 https://aravind-j.github.io/avial/},   }  This free and open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the package."},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Bin Width for Plotting Histograms — binw","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"Calculate Bin Width Plotting Histograms","code":""},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"","code":"binw(x, method = c(\"fd\", \"scott\", \"sturges\"))"},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"x numeric vector values histogram generated. method method compute number classes histogram.","code":""},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"bin width.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"","code":"set.seed(1) x <- stats::rnorm(1111)  binw(x = x, method = \"fd\") #> [1] 0.2 binw(x = x, method = \"scott\") #> [1] 0.5 binw(x = x, method = \"sturges\") #> [1] 0.5"},{"path":"https://aravind-j.github.io/avial/reference/bootstrap.ci.html","id":null,"dir":"Reference","previous_headings":"","what":"Bootstrap Confidence Intervals — bootstrap.ci","title":"Bootstrap Confidence Intervals — bootstrap.ci","text":"function generates bootstrap resamples using boot computes confidence intervals using several standard bootstrap methods via boot.ci. indexing statistic function handled internally.","code":""},{"path":"https://aravind-j.github.io/avial/reference/bootstrap.ci.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bootstrap Confidence Intervals — bootstrap.ci","text":"","code":"bootstrap.ci(   x,   fun,   R = 1000,   conf = 0.95,   type = c(\"norm\", \"basic\", \"stud\", \"perc\", \"bca\"),   parallel = c(\"no\", \"multicore\", \"snow\"),   ncpus = getOption(\"boot.ncpus\", 1L),   cl = NULL,   ... )"},{"path":"https://aravind-j.github.io/avial/reference/bootstrap.ci.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bootstrap Confidence Intervals — bootstrap.ci","text":"x numeric factor vector observations. fun function summarize observations. R Integer specifying number permutations. Default 1000. conf Confidence level interval. Default 0.95. type vector character strings representing type intervals required. value subset values c(\"norm\", \"basic\", \"stud\", \"perc\", \"bca\") simply \"\" compute five types intervals. parallel type parallel operation used ().  missing,     default taken option \"boot.parallel\" (    set, \"\"). ncpus integer: number processes used parallel operation:     typically one chose number available CPUs. cl optional parallel snow cluster use     parallel = \"snow\".  supplied, cluster     local machine created duration boot call. ... Additional arguments passed fun.","code":""},{"path":"https://aravind-j.github.io/avial/reference/bootstrap.ci.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bootstrap Confidence Intervals — bootstrap.ci","text":"named list confidence intervals, containing lower   upper bounds, additional attributes storing observed statistic   mean bootstrap replicates.","code":""},{"path":"https://aravind-j.github.io/avial/reference/bootstrap.ci.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bootstrap Confidence Intervals — bootstrap.ci","text":"Supported interval types include normal approximation, basic, studentized (bootstrap-t), percentile, bias-corrected accelerated (BCa) intervals. requested interval type computed (example, studentized BCa intervals), function falls back percentile intervals.","code":""},{"path":"https://aravind-j.github.io/avial/reference/bootstrap.ci.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bootstrap Confidence Intervals — bootstrap.ci","text":"","code":"library(EvaluateCore) #> Registered S3 method overwritten by 'vegan': #>   method     from       #>   rev.hclust dendextend #>  #> -------------------------------------------------------------------------------- #> Welcome to EvaluateCore version 0.1.4 #>  #>  #> # To know whats new in this version type: #>   news(package='EvaluateCore') #>   for the NEWS file. #>  #> # To cite the methods in the package type: #>   citation(package='EvaluateCore') #>  #> # To suppress this message use: #>   suppressPackageStartupMessages(library(EvaluateCore)) #> --------------------------------------------------------------------------------  pdata <- cassava_CC  qual <- c(\"CUAL\", \"LNGS\", \"PTLC\", \"DSTA\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Conver '#t qualitative data columns to factor pdata[, qual] <- lapply(pdata[, qual], as.factor)  str(pdata) #> 'data.frame':\t168 obs. of  26 variables: #>  $ CUAL  : Factor w/ 4 levels \"Dark green\",\"Green purple\",..: 3 1 2 2 2 2 4 2 2 1 ... #>  $ LNGS  : Factor w/ 3 levels \"Long\",\"Medium\",..: 3 1 2 2 2 2 2 1 1 1 ... #>  $ PTLC  : Factor w/ 5 levels \"Dark green\",\"Green purple\",..: 3 4 4 4 4 5 4 2 2 5 ... #>  $ DSTA  : Factor w/ 5 levels \"Absent\",\"Central part\",..: 1 5 5 5 5 5 5 4 2 5 ... #>  $ LFRT  : Factor w/ 4 levels \"25-50% leaf retention\",..: 1 1 1 1 3 2 2 2 2 2 ... #>  $ LBTEF : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 5 4 4 3 2 ... #>  $ CBTR  : Factor w/ 3 levels \"Cream\",\"White\",..: 2 2 2 2 1 2 1 1 1 1 ... #>  $ NMLB  : Factor w/ 9 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 4 4 3 3 4 ... #>  $ ANGB  : Factor w/ 4 levels \"150-300\",\"450-600\",..: 1 4 1 4 2 2 2 1 2 2 ... #>  $ CUAL9M: Factor w/ 5 levels \"Dark green\",\"Green\",..: 1 1 3 5 3 3 5 5 5 4 ... #>  $ LVC9M : Factor w/ 5 levels \"Dark green\",\"Green\",..: 4 3 3 3 3 1 3 1 4 3 ... #>  $ TNPR9M: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 5 4 2 5 4 2 5 5 5 ... #>  $ PL9M  : Factor w/ 2 levels \"Long (25-30cm)\",..: 2 2 1 1 1 1 1 1 2 2 ... #>  $ STRP  : Factor w/ 4 levels \"Absent\",\"Intermediate\",..: 2 3 1 1 1 1 4 1 1 4 ... #>  $ STRC  : Factor w/ 2 levels \"Absent\",\"Present\": 2 2 1 2 1 1 2 1 1 2 ... #>  $ PSTR  : Factor w/ 2 levels \"Irregular\",\"Tending toward horizontal\": 1 2 2 2 1 2 2 2 1 2 ... #>  $ NMSR  : num  6 2 6 2 20 13 4 14 10 5 ... #>  $ TTRN  : num  3 0.5 3 2 5 ... #>  $ TFWSR : num  1.4 2.6 1.2 1.6 5 7 4.2 2.8 2.8 4 ... #>  $ TTRW  : num  0.7 0.65 0.6 1.6 1.25 ... #>  $ TFWSS : num  1 2.8 2.8 2.4 16 12 9 4.4 6.2 5 ... #>  $ TTSW  : num  0.5 0.7 1.4 2.4 4 ... #>  $ TTPW  : num  2.4 5.4 4 4 21 19 13.2 7.2 9 9 ... #>  $ AVPW  : num  1.2 1.35 2 4 5.25 4.75 3.3 2.4 1.8 2.25 ... #>  $ ARSR  : num  2 0 2 0 3 0 0 6 0 0 ... #>  $ SRDM  : num  42 39.8 29.7 43 37.9 37 38.9 36.9 41 37.9 ...  # Bootstrap CIs ----  bootstrap.ci(pdata$NMSR, mean, type = \"norm\") #> $norm #>     lower     upper  #>  9.715342 12.056122  #>  #> attr(,\"observed\") #> [1] 10.89286 #> attr(,\"mean\") #> [1] 10.89998 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 bootstrap.ci(pdata$NMSR, mean, type = \"basic\") #> $basic #>     lower     upper  #>  9.619048 12.154157  #>  #> attr(,\"observed\") #> [1] 10.89286 #> attr(,\"mean\") #> [1] 10.85815 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 bootstrap.ci(pdata$NMSR, mean, type = \"perc\") #> $perc #>     lower     upper  #>  9.607292 12.011756  #>  #> attr(,\"observed\") #> [1] 10.89286 #> attr(,\"mean\") #> [1] 10.82457 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 bootstrap.ci(pdata$NMSR, mean, type = \"bca\") #> $bca #>    lower    upper  #>  9.77381 12.17054  #>  #> attr(,\"observed\") #> [1] 10.89286 #> attr(,\"mean\") #> [1] 10.8912 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95  # Simpson's Index (d) simpson <- function(x) {   x <- droplevels(x)   sum(prop.table(table(x)) ^ 2) }  # Shannon-Wiener Diversity Index (H) shannon <- function(x, base = 2) {   x <- droplevels(x)   p <- prop.table(table(x))   -sum(p * log(p, base = base)) }  # McIntosh Index mcintosh_diversity <- function(x) {   x <- droplevels(x)   n <- as.vector(table(x))   N <- sum(n)   U <- sqrt(sum(n^2))   (N - U) / (N - sqrt(N)) }  # McIntosh Evenness mcintosh_evenness <- function(x) {   x <- droplevels(x)   n <- as.vector(table(x))   N <- sum(n)   U <- sqrt(sum(n^2))   S <- length(levels(x))   (N - U) / (N - (N / sqrt(S))) }  bootstrap.ci(pdata$LNGS, shannon, type = \"norm\") #> $norm #>    lower    upper  #> 1.368283 1.547521  #>  #> attr(,\"observed\") #> [1] 1.448816 #> attr(,\"mean\") #> [1] 1.439729 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 bootstrap.ci(pdata$PTLC, simpson, type = \"basic\") #> $basic #>     lower     upper  #> 0.3457791 0.4766541  #>  #> attr(,\"observed\") #> [1] 0.4213435 #> attr(,\"mean\") #> [1] 0.4256122 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 bootstrap.ci(pdata$LFRT, mcintosh_evenness, type = \"perc\") #> $perc #>     lower     upper  #> 0.6340200 0.8288887  #>  #> attr(,\"observed\") #> [1] 0.693727 #> attr(,\"mean\") #> [1] 0.7047665 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 bootstrap.ci(pdata$LBTEF, mcintosh_diversity, type = \"bca\") #> $bca #>     lower     upper  #> 0.5857378 0.6147432  #>  #> attr(,\"observed\") #> [1] 0.5983483 #> attr(,\"mean\") #> [1] 0.5928669 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95  # Studentised intervals require a `fun` returning # variances in addition to an estimate  bootstrap.ci(pdata$NMSR, mean, type = \"stud\") #> Warning: Studentized CI requires fun() to return c(estimate, SE); falling back to percentile CI. #> $stud #>     lower     upper  #>  9.762351 12.083333  #>  #> attr(,\"observed\") #> [1] 10.89286 #> attr(,\"mean\") #> [1] 10.8994 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95  stat_fun_mean <- function(x) {   est <- mean(x)   se  <- sd(x) / sqrt(length(x))   c(est, se) }  bootstrap.ci(pdata$NMSR, stat_fun_mean, type = \"stud\") #> Warning: Studentized CI failed; falling back to percentile CI. #> $stud #>     lower     upper  #>  9.726339 12.220089  #>  #> attr(,\"observed\") #> [1] 10.892857  0.631431 #> attr(,\"mean\") #> [1] 10.9124226  0.6297806 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95  bootstrap.ci(pdata$DSTA, shannon, type = \"stud\") #> Warning: Studentized CI requires fun() to return c(estimate, SE); falling back to percentile CI. #> $stud #>    lower    upper  #> 1.784899 2.066101  #>  #> attr(,\"observed\") #> [1] 1.946525 #> attr(,\"mean\") #> [1] 1.930617 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95  stat_fun_shannon <- function(x, base = 2) {   x <- droplevels(x)          # drop unused factor levels   p <- prop.table(table(x))   # Only keep p > 0 to avoid log(0)   p <- p[p > 0]   est <- -sum(p * log(p, base = base))   # Approximate SE using sqrt(Var(p * log(p)))   se <- sqrt(sum((p * log(p, base = base))^2) / length(x))   c(est, se) }  bootstrap.ci(pdata$DSTA, stat_fun_shannon, type = \"stud\") #> Warning: Studentized CI failed; falling back to percentile CI. #> $stud #>    lower    upper  #> 1.769157 2.062549  #>  #> attr(,\"observed\") #> [1] 1.94652505 0.07067869 #> attr(,\"mean\") #> [1] 1.92825723 0.07030688 #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95"},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Diversity Measures — diversity.calc","title":"Compute Diversity Measures — diversity.calc","text":"function diversity.calc calculates following diversity measures Margalef's Richness Index (\\(D_{Margalef}\\)) (Margalef 1973) Menhinick's Index (\\(D_{Menhinick}\\)) (Menhinick 1964) Berger–Parker Index (\\(D_{BP}\\)) (Berger Parker 1970) Reciprocal Berger–Parker Index (\\(D_{BP_{R}}\\)) (Magurran 2011) Simpson's Index (\\(d\\)) (Simpson 1949; Peet 1974) Simpson's Index Diversity Gini's Diversity Index Gini-Simpson Index Nei's Diversity Index Nei's Variation Index (\\(D\\)) Hurlbert’s probability interspecific encounter (\\(PIE\\)) (Gini 1912, 1912; Greenberg 1956; Berger Parker 1970; Hurlbert 1971; Nei 1973; Peet 1974) Maximum Simpson's Index Diversity Maximum Nei's Diversity/Variation Index (\\(D_{max}\\)) (Hennink Zeven 1990) Simpson's Reciprocal Index Hill's \\(N_{2}\\) (\\(D_{R}\\)) Effective number Species (\\(ENS_{d}\\)) (Williams 1964; Hill 1973) Relative Simpson's Index Diversity Relative Nei's Diversity/Variation Index (\\(D'\\)) (Hennink Zeven 1990) Simpson’s evenness equitability (\\(D_{e}\\)) (Pielou 1966; Hill 1973) Shannon Shannon-Weaver Shannon-Wiener Diversity Index Shannon entropy (\\(H\\)) (Shannon Weaver 1949; Peet 1974) Maximum Shannon-Weaver Diversity Index (\\(H_{max}\\)) (Hennink Zeven 1990) Relative Shannon-Weaver Diversity Index Shannon Equitability Index (\\(H'\\)) Pielou's Evenness (\\(J\\)) (Pielou 1966; Hennink Zeven 1990) Effective number species Shannon - Weaver Diversity Index (\\(ENS_{H}\\)) Hill's \\(N_{1}\\) (Macarthur 1965; Hill 1973) Heip's Evenness Index (\\(E_{Heip}\\)) (Heip 1974) McIntosh Diversity Index (\\(D_{Mc}\\)) (McIntosh 1967; Peet 1974) McIntosh Evenness Index (\\(E_{Mc}\\)) (Pielou 1975) Smith & Wilson's Evenness Index (\\(E_{var}\\)) (Smith Wilson 1996) Brillouin Diversity Index (\\(D_{Brillouin}\\)) (Brillouin 2013) Rényi Entropy (\\({}^q H_{Rényi}\\)) (Renyi 1960) Tsallis HCDT Entropy (\\({}^q H_{Tsallis}\\)) (Havrda Charvat 1967; Daroczy 1970; Tsallis 1988) Hill Numbers (\\({}^q D\\)) (Hill 1973) MathJax = {output: {font: \"mathjax-newcm\", fontPath: \"../../mathjaxr/doc/mathjax/font\",}};","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Diversity Measures — diversity.calc","text":"","code":"diversity.calc(x, base = exp(1), na.omit = TRUE)"},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Diversity Measures — diversity.calc","text":"x factor vector categories (e.g., species, traits). frequency level treated abundance category. base logarithm base used computation shannon family diversity indices. Default exp(1). na.omit logical. TRUE, missing values (NA) ignored included distinct factor level computation. Default TRUE.","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Diversity Measures — diversity.calc","text":"list different richness number classes   x richness. margalef_index Margalef's Richness   Index (\\(D_{Margalef}\\))   (Margalef 1973) menhinick_index Menhinick's Index (\\(D_{Menhinick}\\))   (Menhinick 1964) berger_parker Berger–Parker Index (\\(D_{BP}\\))   (Berger Parker 1970) berger_parker_reciprocal Reciprocal Berger–Parker Index   (\\(D_{BP_{R}}\\)) (Magurran 2011) simpson Simpson's index (\\(d\\))   (Simpson 1949; Peet 1974) gini_simpson Simpson's Index Diversity Gini's Diversity Index   Gini-Simpson Index Nei's Diversity Index Nei's Variation Index   (\\(D\\)) Hurlbert’s probability interspecific encounter   (\\(PIE\\))   (Gini 1912, 1912; Greenberg 1956; Berger Parker 1970; Hurlbert 1971; Nei 1973; Peet 1974) simpson_max Maximum Simpson's Index Diversity Maximum Nei's   Diversity/Variation Index (\\(D_{max}\\))   (Hennink Zeven 1990) simpson_reciprocal Simpson's Reciprocal Index Hill's   \\(N_{2}\\) (\\(D_{R}\\)) Effective number Species   (\\(ENS_{d}\\))   (Williams 1964; Hill 1973) simpson_relative Relative Simpson's Index Diversity Relative   Nei's Diversity/Variation Index (\\(D'\\))   (Hennink Zeven 1990) simpson_evenness Simpson’s evenness equitability   (\\(D_{e}\\))   (Pielou 1966; Hill 1973) shannon Shannon Shannon-Weaver Shannon-Wiener Diversity Index   Shannon entropy (\\(H\\))   (Shannon Weaver 1949; Peet 1974) shannon_max Maximum Shannon-Weaver Diversity Index   (\\(H_{max}\\)) (Hennink Zeven 1990) shannon_relative Relative Shannon-Weaver Diversity Index Shannon   Equitability Index (\\(H'\\)) Pielou's Evenness (\\(J\\))   (Pielou 1966; Hennink Zeven 1990) shannon_ens Effective number species Shannon - Weaver   Diversity Index (\\(ENS_{H}\\)) Hill's \\(N_{1}\\)   (Macarthur 1965; Hill 1973) heip_evenness Heip's Evenness Index (\\(E_{Heip}\\))   (Heip 1974) mcintosh_diversity McIntosh   Diversity Index (\\(D_{Mc}\\))   (McIntosh 1967; Peet 1974) mcintosh_evenness McIntosh Evenness Index (\\(E_{Mc}\\))   (Pielou 1975) smith_wilson Smith & Wilson's Evenness Index (\\(E_{var}\\))   (Smith Wilson 1996) brillouin_index Brillouin Diversity Index   (Brillouin 2013) renyi_entropy_0 Rényi Entropy order 0   (Renyi 1960) renyi_entropy_1 Rényi Entropy order 1 renyi_entropy_2 Rényi Entropy order 2 tsallis_entropy_0 Tsallis Entropy order 0   (Havrda Charvat 1967; Daroczy 1970; Tsallis 1988) tsallis_entropy_1 Tsallis Entropy order 1 tsallis_entropy_2 Tsallis Entropy order 2 hill_number_0 Hill Number order 0 (\\({}^0 D\\))   (Hill 1973) hill_number_1 Hill Number   order 1 (\\({}^1 D\\)) hill_number_2 Hill Number order 2   (\\({}^2 D\\))","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute Diversity Measures — diversity.calc","text":"diversity indices implemented diversity.calc   computed follows.","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"richness-indices","dir":"Reference","previous_headings":"","what":"Richness Indices","title":"Compute Diversity Measures — diversity.calc","text":"number classes phenotypic trait (species richness)   (\\(k\\)) can described adjusting sample size (\\(N\\))   Margalef's Richness Index (\\(D_{Margalef}\\))   (Margalef 1973)  Menhinick's Index   (\\(D_{Menhinick}\\)) (Menhinick 1964) computed follows. \\[D_{Margalef} = \\frac{k - 1}{\\ln(N)}\\] \\[D_{Menhinick} = \\frac{k}{\\sqrt{N}}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"berger-parker-index","dir":"Reference","previous_headings":"","what":"Berger–Parker Index","title":"Compute Diversity Measures — diversity.calc","text":"proportion   individuals belonging abundant class trait (species   community) computed . \\[D_{BP} = \\max(p_i)\\] , \\(p_{}\\) denotes proportion/fraction/frequency   accessions \\(\\)th phenotypic class trait (number   individuals \\(\\)th species). reciprocal estimates relative diversity class. \\[D_{BP_{R}} = \\frac{1}{D_{BP}}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"simpson-s-and-related-indices","dir":"Reference","previous_headings":"","what":"Simpson's and Related Indices","title":"Compute Diversity Measures — diversity.calc","text":"Simpson's index (\\(d\\))   estimates probability two accessions randomly selected   belong phenotypic class trait (species   community), computed follows   (Simpson 1949; Peet 1974) . \\[d = \\sum_{= 1}^{k}p_{}^{2}\\] , \\(k\\) number phenotypic classes trait (  number species community). value \\(d\\) can range 0 1 0 representing maximum   diversity 1, diversity. \\(d\\) subtracted 1 give Simpson's index diversity   (\\(D\\))   (Greenberg 1956; Berger Parker 1970; Hurlbert 1971; Peet 1974; Hennink Zeven 1990)     originally suggested   Gini (1912, 1912)     described literature Gini's diversity index Gini-Simpson   index. Nei's diversity index Nei's variation index   (Nei 1973; Hennink Zeven 1990) .   Greater value \\(D\\), greater diversity range 0   1. \\[D = 1 - d\\] maximum value \\(D\\), \\(D_{max}\\) occurs accessions   uniformly distributed across phenotypic classes (individuals   uniformly distributed across species community) computed   follows (Hennink Zeven 1990) . \\[D_{max} = 1 - \\frac{1}{k}\\] Reciprocal \\(d\\) gives Simpson's reciprocal index   (\\(D_{R}\\))   (Williams 1964; Hennink Zeven 1990)     can range 1 \\(k\\). also described   Hill (1973)  \\(N_{2}\\)   Effective number Species (classes) (\\(ENS_{d}\\)). \\[D_{R} = \\frac{1}{d}\\] Relative Simpson's index diversity Relative Nei's diversity/variation   index (\\(H'\\)) (Hennink Zeven 1990)    defined follows (Peet 1974) . \\[D' = \\frac{D}{D_{max}}\\] Simpson’s evenness equitability (\\(D_{e}\\) described follows   (Pielou 1966; Hill 1973) . \\[D_{e} = \\frac{1}{d \\cdot k}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"shannon-weaver-and-related-indices","dir":"Reference","previous_headings":"","what":"Shannon-Weaver and Related Indices","title":"Compute Diversity Measures — diversity.calc","text":"index information   \\(H\\), described   Shannon Weaver (1949)  follows. \\[H = -\\sum_{=1}^{k}p_{} \\log_{2}(p_{})\\] \\(H\\) described Shannon Shannon-Weaver Shannon-Wiener   diversity index Shannon entropy literature   (Shannon Weaver 1949; Peet 1974) . Alternatively, \\(H\\) also computed using natural logarithm instead   logarithm base 2. \\[H = -\\sum_{=1}^{k}p_{} \\ln(p_{})\\] maximum value \\(H\\) (\\(H_{max}\\)) \\(\\ln(k)\\).   value occurs phenotypic class trait proportion   accessions (species community proportion   individuals) (Hennink Zeven 1990) . \\[H_{max} = \\log_{2}(k)\\;\\; \\textrm{} \\;\\; H_{max} = \\ln(k)\\] relative Shannon-Weaver diversity index Shannon equitability index   (\\(H'\\)) Pielou's Evenness (\\(J\\)) Shannon diversity   index (\\(\\)) divided maximum diversity (\\(H_{max}\\))   (Pielou 1966; Hennink Zeven 1990) . \\[H' = \\frac{H}{H_{max}}\\] Macarthur (1965)  described Effective   number species (classes) Shannon index (\\(ENS_{H}\\))   follows. \\[ENS_{H} = e^{H}\\] Heip’s index Heip's Evenness index transformation   Shannon–Wiener diversity index standardizes relative number   classes trait (species richness) computed follows   (Heip 1974) . \\[E_{Heip} = \\frac{e^{H} - 1}{k - 1}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"mcintosh-s-measure-of-diversity","dir":"Reference","previous_headings":"","what":"McIntosh's Measure of Diversity","title":"Compute Diversity Measures — diversity.calc","text":"similar index diversity   described McIntosh (1967)    follows (\\(D_{Mc}\\)) (Peet 1974) . \\[D_{Mc} = \\frac{N - \\sqrt{\\sum_{=1}^{k}n_{}^2}}{N - \\sqrt{N}}\\] , \\(n_{}\\) denotes number accessions \\(\\)th   phenotypic class trait (number individuals \\(\\)th   species community) \\(N\\) total number accessions   \\(p_{} = {n_{}}/{N}\\). additional measure evenness proposed   Pielou (1975)  follows. \\[E_{Mc} = \\frac{N - \\sqrt{\\sum_{=1}^{k}n_{}^2}}{N -   \\frac{N}{\\sqrt{S}}}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"smith-amp-wilson-s-evenness-index","dir":"Reference","previous_headings":"","what":"Smith & Wilson's Evenness Index","title":"Compute Diversity Measures — diversity.calc","text":"index measures   equally accessions/genotypes distributed among different trait classes   (individuals individuals distributed among different species   community). less sensitive rare classes species   computed follows. \\[E_{var} = 1 - \\frac{2}{\\pi} \\arctan{\\left ( \\frac{1}{k}   \\sum_{=1}^{k}(\\ln{n_{} - \\overline{\\ln{n}}})^{2} \\right )}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"brillouin-diversity-index","dir":"Reference","previous_headings":"","what":"Brillouin Diversity Index","title":"Compute Diversity Measures — diversity.calc","text":"information-theoretic   measure appropriate complete censuses computed follows   (Brillouin 2013) . \\[H_{B} = \\frac{\\ln(N!) - \\sum \\ln(n_i!)}{N}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"parametric-indices","dir":"Reference","previous_headings":"","what":"Parametric Indices","title":"Compute Diversity Measures — diversity.calc","text":"Parametric indices, also known multivariate compound indices, use   sensitivity parameter (\\(q\\)) weigh frequent rare classes   within trait (common rare species within community). Rényi entropy extends several entropy measures, including Shannon   entropy, computed follows   (Renyi 1960) . \\[{}^q H_{Rényi} = \\frac{1}{1-q} \\ln \\sum_{=1}^{k} p_{}^{q} ,   \\quad q \\ge 0, q \\neq 1\\] frequently computed using natural logarithm instead   logarithm base 2. index undefined (\\(q = 1\\)),   Shannon entropy limiting case. Tsallis proposed similar measure, HCDT Tsallis entropy   (Havrda Charvat 1967; Daroczy 1970; Tsallis 1988) ,   matches species richness \\(q = 0\\), Shannon entropy   \\(q = 1\\), Gini-Simpson index \\(q = 2\\). \\[{}^q H_{Tsallis} = \\frac{1}{q - 1} \\left (   1 - \\sum_{=1}^{k} p_i^q \\right ), \\quad q \\ge 0, q \\neq 1\\] Hill showed species richness, Shannon entropy, Simpson's   index related diversity indices, collectively known   Hill numbers defined   (Hill 1973) . \\[{}^q D = {\\left ( \\sum_{=1}^{k} p_{}^{q}   \\right )}^{\\frac{1}{1-q}} , \\quad q \\ge 0, q \\neq 1\\] , \\[{}^0 D = k\\] \\[{}^1 D = e^{H}\\] \\[{}^2 D = D_{R}\\]","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Compute Diversity Measures — diversity.calc","text":"Berger WH, Parker FL (1970). “Diversity planktonic foraminifera deep-sea sediments.” Science, 168(3937), 1345–1347. Brillouin L (2013). Science information theory, Dover edition. Dover Publications, Inc., Mineola, New York. ISBN 978-0-486-31641-3. Daroczy Z (1970). “Generalized information functions.” Information Control, 16(1), 36–51. Gini C (1912). Variabilita e Mutabilita. Contributo allo Studio delle Distribuzioni e delle Relazioni Statistiche. [Fasc. .]. Tipogr. di P. Cuppini, Bologna. Gini C (1912). “Variabilita e mutabilita.” Pizetti E, Salvemini T (eds.), Memorie di Metodologica Statistica. Liberia Eredi Virgilio Veschi, Roma, Italy. Greenberg JH (1956). “measurement linguistic diversity.” Language, 32(1), 109. Havrda J, Charvat F (1967). “Quantification method classification processes. Concept structural <p>&alpha;<\/p>-entropy.” Kybernetika, 3(1), (30)–35. Heip C (1974). “new index measuring evenness.” Journal Marine Biological Association United Kingdom, 54(3), 555–557. Hennink S, Zeven AC (1990). “interpretation Nei Shannon-Weaver within population variation indices.” Euphytica, 51(3), 235–240. Hill MO (1973). “Diversity evenness: unifying notation consequences.” Ecology, 54(2), 427–432. Hurlbert SH (1971). “nonconcept species diversity: critique alternative parameters.” Ecology, 52(4), 577–586. Macarthur RH (1965). “Patterns species diversity.” Biological Reviews, 40(4), 510–533. Magurran AE (2011). Measuring biological diversity, 9 [Nachdr.] edition. Blackwell, Malden, Mass. ISBN 978-0-632-05633-0. Margalef R (1973). “Information theory ecology.” International Journal General Systems, 3, 36–71. McIntosh RP (1967). “index diversity relation certain concepts diversity.” Ecology, 48(3), 392–404. Menhinick EF (1964). “comparison species-individuals diversity indices applied samples field insects.” Ecology, 45(4), 859–861. Nei M (1973). “Analysis gene diversity subdivided populations.” Proceedings National Academy Sciences, 70(12), 3321–3323. Peet RK (1974). “measurement species diversity.” Annual Review Ecology Systematics, 5(1), 285–307. Pielou EC (1966). “measurement diversity different types biological collections.” Journal Theoretical Biology, 13, 131–144. Pielou EC (1975). Ecological diversity. New York : Wiley. ISBN 978-0-471-68925-6. Renyi (1960). “measures entropy information.” Neyman J (ed.), Proceedings Fourth Berkeley Symposium Mathematical Statistics Probability (June 20-July 30 1960), Volume : Contributions Theory Statistics, 547–561. University California Press. Shannon CE, Weaver W (1949). Mathematical Theory Communication,  number v. 2 Mathematical Theory Communication. University Illinois Press. Simpson EH (1949). “Measurement diversity.” Nature, 163(4148), 688–688. Smith B, Wilson JB (1996). “consumer's guide evenness indices.” Oikos, 76(1), 70. Tsallis C (1988). “Possible generalization Boltzmann-Gibbs statistics.” Journal Statistical Physics, 52(1-2), 479–487. Williams CB (1964). Patterns Balance Nature Related Problems Quantitative Ecology. Academic Press.","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.calc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Diversity Measures — diversity.calc","text":"","code":"#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # Qualitative trait data ---- #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  library(EvaluateCore)  pdata <- cassava_CC  qual <- c(\"CUAL\", \"LNGS\", \"PTLC\", \"DSTA\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Convert qualitative data columns to factor pdata[, qual] <- lapply(pdata[, qual], as.factor)  # Get diversity measures diversity.calc(x = pdata$CUAL) #> $richness #> [1] 4 #>  #> $margalef_index #> [1] 0.5854842 #>  #> $menhinick_index #> [1] 0.3086067 #>  #> $berger_parker #> [1] 0.5297619 #>  #> $berger_parker_reciprocal #> [1] 1.88764 #>  #> $simpson #> [1] 0.3784722 #>  #> $gini_simpson #> [1] 0.6215278 #>  #> $simpson_max #> [1] 0.75 #>  #> $simpson_reciprocal #> [1] 2.642202 #>  #> $simpson_relative #> [1] 0.8287037 #>  #> $simpson_evenness #> [1] 0.4022346 #>  #> $shannon #> [1] 1.113994 #>  #> $shannon_max #> [1] 1.386294 #>  #> $shannon_relative #> [1] 0.803577 #>  #> $shannon_ens #> [1] 3.046503 #>  #> $heip_evenness #> [1] 1.329531 #>  #> $mcintosh_diversity #> [1] 0.4169689 #>  #> $mcintosh_evenness #> [1] 0.7695981 #>  #> $smith_wilson #> [1] 0.4591224 #>  #> $brillouin_index #> [1] 1.072686 #>  #> $renyi_entropy_0 #> [1] 1.386294 #>  #> $renyi_entropy_1 #> [1] 1.113994 #>  #> $renyi_entropy_2 #> [1] 0.9716126 #>  #> $tsallis_entropy_0 #> [1] 3 #>  #> $tsallis_entropy_1 #> [1] 1.113994 #>  #> $tsallis_entropy_2 #> [1] 0.6215278 #>  #> $hill_number_0 #> [1] 4 #>  #> $hill_number_1 #> [1] 3.046503 #>  #> $hill_number_2 #> [1] 2.642202 #>   # Get diversity measures for multiple qualitative traits div_out1 <- lapply(pdata[, qual], diversity.calc) do.call(rbind, div_out1) #>        richness margalef_index menhinick_index berger_parker #> CUAL   4        0.5854842      0.3086067       0.5297619     #> LNGS   3        0.3903228      0.231455        0.4285714     #> PTLC   5        0.7806456      0.3857584       0.5892857     #> DSTA   5        0.7806456      0.3857584       0.4404762     #> LFRT   4        0.5854842      0.3086067       0.5238095     #> LBTEF  6        0.975807       0.46291         0.2559524     #> CBTR   3        0.3903228      0.231455        0.5595238     #> NMLB   9        1.561291       0.6943651       0.3214286     #> ANGB   4        0.5854842      0.3086067       0.452381      #> CUAL9M 5        0.7806456      0.3857584       0.3333333     #> LVC9M  5        0.7806456      0.3857584       0.5           #> TNPR9M 5        0.7806456      0.3857584       0.3214286     #> PL9M   2        0.1951614      0.1543033       0.5119048     #> STRP   4        0.5854842      0.3086067       0.3392857     #> STRC   2        0.1951614      0.1543033       0.5952381     #> PSTR   2        0.1951614      0.1543033       0.6666667     #>        berger_parker_reciprocal simpson   gini_simpson simpson_max #> CUAL   1.88764                  0.3784722 0.6215278    0.75        #> LNGS   2.333333                 0.3877551 0.6122449    0.6666667   #> PTLC   1.69697                  0.4213435 0.5786565    0.8         #> DSTA   2.27027                  0.3035006 0.6964994    0.8         #> LFRT   1.909091                 0.4265873 0.5734127    0.75        #> LBTEF  3.906977                 0.2005385 0.7994615    0.8333333   #> CBTR   1.787234                 0.4872449 0.5127551    0.6666667   #> NMLB   3.111111                 0.1980584 0.8019416    0.8888889   #> ANGB   2.210526                 0.3421202 0.6578798    0.75        #> CUAL9M 3                        0.2880527 0.7119473    0.8         #> LVC9M  2                        0.3886763 0.6113237    0.8         #> TNPR9M 3.111111                 0.2218679 0.7781321    0.8         #> PL9M   1.953488                 0.5002834 0.4997166    0.5         #> STRP   2.947368                 0.3118622 0.6881378    0.75        #> STRC   1.68                     0.5181406 0.4818594    0.5         #> PSTR   1.5                      0.5555556 0.4444444    0.5         #>        simpson_reciprocal simpson_relative simpson_evenness shannon   #> CUAL   2.642202           0.8287037        0.4022346        1.113994  #> LNGS   2.578947           0.9183673        0.5444444        1.004242  #> PTLC   2.37336            0.7233206        0.3456282        1.113785  #> DSTA   3.294887           0.8706243        0.2871503        1.349228  #> LFRT   2.344186           0.7645503        0.4359862        0.9661827 #> LBTEF  4.986572           0.9593537        0.2084737        1.651889  #> CBTR   2.052356           0.7691327        0.6500829        0.778669  #> NMLB   5.049016           0.9021843        0.1385526        1.775015  #> ANGB   2.922949           0.8771731        0.3800086        1.176272  #> CUAL9M 3.471587           0.8899341        0.2809197        1.335612  #> LVC9M  2.572835           0.7641546        0.3271589        1.10697   #> TNPR9M 4.507186           0.9726651        0.2570258        1.557711  #> PL9M   1.998867           0.9994331        1.000567         0.6928637 #> STRP   3.206544           0.917517         0.3632994        1.21246   #> STRC   1.929978           0.9637188        1.037647         0.6748953 #> PSTR   1.8                0.8888889        1.125            0.6365142 #>        shannon_max shannon_relative shannon_ens heip_evenness #> CUAL   1.386294    0.803577         3.046503    1.329531      #> LNGS   1.098612    0.9141009        2.729839    1.629034      #> PTLC   1.609438    0.6920334        3.045865    0.9967717     #> DSTA   1.609438    0.8383227        3.85445     1.501076      #> LFRT   1.386294    0.6969535        2.627894    1.010189      #> LBTEF  1.791759    0.9219369        5.216826    1.967847      #> CBTR   1.098612    0.7087751        2.178571    1.037618      #> NMLB   2.197225    0.807844         5.900367    1.493279      #> ANGB   1.386294    0.8485006        3.242263    1.485852      #> CUAL9M 1.609438    0.8298622        3.802321    1.467013      #> LVC9M  1.609438    0.6877994        3.025179    0.9845744     #> TNPR9M 1.609438    0.9678601        4.74794     2.115542      #> PL9M   0.6931472   0.999591         1.999433    1.71717       #> STRP   1.386294    0.8746048        3.361743    1.583352      #> STRC   0.6931472   0.9736681        1.963827    1.647638      #> PSTR   0.6931472   0.9182958        1.889882    1.505018      #>        mcintosh_diversity mcintosh_evenness smith_wilson brillouin_index #> CUAL   0.4169689          0.7695981         0.4591224    1.072686        #> LNGS   0.4088431          0.8927017         0.6401521    0.9736063       #> PTLC   0.3802252          0.6347663         0.4467806    1.06311         #> DSTA   0.4866359          0.8124135         0.5123229    1.294889        #> LFRT   0.3758619          0.693727          0.3336841    0.9291107       #> LBTEF  0.5983483          0.9331358         0.5031023    1.584527        #> CBTR   0.327216           0.7144704         0.3314674    0.7525541       #> NMLB   0.6013583          0.8324437         0.3647054    1.686068        #> ANGB   0.4497918          0.8301792         0.5028364    1.133948        #> CUAL9M 0.5020268          0.8381077         0.4522341    1.282613        #> LVC9M  0.408042           0.6812051         0.3707252    1.058812        #> TNPR9M 0.5731943          0.9569183         0.7843173    1.49946         #> PL9M   0.3171624          0.9993158         0.9785679    0.6762626       #> STRP   0.4784684          0.8831074         0.4699856    1.170253        #> STRC   0.3036037          0.9565949         0.8305121    0.6584021       #> PSTR   0.2759327          0.869409          0.7098798    0.6202605       #>        renyi_entropy_0 renyi_entropy_1 renyi_entropy_2 tsallis_entropy_0 #> CUAL   1.386294        1.113994        0.9716126       3                 #> LNGS   1.098612        1.004242        0.9473813       2                 #> PTLC   1.609438        1.113785        0.8643068       4                 #> DSTA   1.609438        1.349228        1.192372        4                 #> LFRT   1.386294        0.9661827       0.8519382       3                 #> LBTEF  1.791759        1.651889        1.606749        5                 #> CBTR   1.098612        0.778669        0.7189884       2                 #> NMLB   2.197225        1.775015        1.619193        8                 #> ANGB   1.386294        1.176272        1.072593        3                 #> CUAL9M 1.609438        1.335612        1.244612        4                 #> LVC9M  1.609438        1.10697         0.9450084       4                 #> TNPR9M 1.609438        1.557711        1.505673        4                 #> PL9M   0.6931472       0.6928637       0.6925804       1                 #> STRP   1.386294        1.21246         1.165194        3                 #> STRC   0.6931472       0.6748953       0.6575087       1                 #> PSTR   0.6931472       0.6365142       0.5877867       1                 #>        tsallis_entropy_1 tsallis_entropy_2 hill_number_0 hill_number_1 #> CUAL   1.113994          0.6215278         4             3.046503      #> LNGS   1.004242          0.6122449         3             2.729839      #> PTLC   1.113785          0.5786565         5             3.045865      #> DSTA   1.349228          0.6964994         5             3.85445       #> LFRT   0.9661827         0.5734127         4             2.627894      #> LBTEF  1.651889          0.7994615         6             5.216826      #> CBTR   0.778669          0.5127551         3             2.178571      #> NMLB   1.775015          0.8019416         9             5.900367      #> ANGB   1.176272          0.6578798         4             3.242263      #> CUAL9M 1.335612          0.7119473         5             3.802321      #> LVC9M  1.10697           0.6113237         5             3.025179      #> TNPR9M 1.557711          0.7781321         5             4.74794       #> PL9M   0.6928637         0.4997166         2             1.999433      #> STRP   1.21246           0.6881378         4             3.361743      #> STRC   0.6748953         0.4818594         2             1.963827      #> PSTR   0.6365142         0.4444444         2             1.889882      #>        hill_number_2 #> CUAL   2.642202      #> LNGS   2.578947      #> PTLC   2.37336       #> DSTA   3.294887      #> LFRT   2.344186      #> LBTEF  4.986572      #> CBTR   2.052356      #> NMLB   5.049016      #> ANGB   2.922949      #> CUAL9M 3.471587      #> LVC9M  2.572835      #> TNPR9M 4.507186      #> PL9M   1.998867      #> STRP   3.206544      #> STRC   1.929978      #> PSTR   1.8            #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ # Species abundance data ---- #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  library(vegan) #> Loading required package: permute data(dune)  abundance_site1 <- unlist(dune[1, ]) abundance_site1[abundance_site1 != 0] #> Achimill Elymrepe Lolipere  Poaprat  Poatriv  #>        1        4        7        4        2   # Convert to raw counts for use in diversity.calc using rep abundance_site1_raw <- factor(rep(names(abundance_site1),                                   times = abundance_site1))  # Get diversity measures diversity.calc(x = abundance_site1_raw) #> $richness #> [1] 5 #>  #> $margalef_index #> [1] 1.383905 #>  #> $menhinick_index #> [1] 1.178511 #>  #> $berger_parker #> [1] 0.3888889 #>  #> $berger_parker_reciprocal #> [1] 2.571429 #>  #> $simpson #> [1] 0.2654321 #>  #> $gini_simpson #> [1] 0.7345679 #>  #> $simpson_max #> [1] 0.8 #>  #> $simpson_reciprocal #> [1] 3.767442 #>  #> $simpson_relative #> [1] 0.9182099 #>  #> $simpson_evenness #> [1] 0.2722689 #>  #> $shannon #> [1] 1.440482 #>  #> $shannon_max #> [1] 1.609438 #>  #> $shannon_relative #> [1] 0.8950216 #>  #> $shannon_ens #> [1] 4.22273 #>  #> $heip_evenness #> [1] 1.74747 #>  #> $mcintosh_diversity #> [1] 0.6343064 #>  #> $mcintosh_evenness #> [1] 0.8770096 #>  #> $smith_wilson #> [1] 0.5900996 #>  #> $brillouin_index #> [1] 1.156724 #>  #> $renyi_entropy_0 #> [1] 1.609438 #>  #> $renyi_entropy_1 #> [1] 1.440482 #>  #> $renyi_entropy_2 #> [1] 1.326396 #>  #> $tsallis_entropy_0 #> [1] 4 #>  #> $tsallis_entropy_1 #> [1] 1.440482 #>  #> $tsallis_entropy_2 #> [1] 0.7345679 #>  #> $hill_number_0 #> [1] 5 #>  #> $hill_number_1 #> [1] 4.22273 #>  #> $hill_number_2 #> [1] 3.767442 #>   # Convert multiple site abundance data to raw counts abundance_site_raw <- apply(dune, 1, function(x) {   factor(rep(names(x), times = x)) })  # Get diversity measures for multiple sites div_out2 <- lapply(abundance_site_raw, diversity.calc) do.call(rbind, div_out2) #>    richness margalef_index menhinick_index berger_parker #> 1  5        1.383905       1.178511        0.3888889     #> 2  10       2.407917       1.543033        0.1666667     #> 3  10       2.439765       1.581139        0.175         #> 4  13       3.152368       1.937926        0.1777778     #> 5  14       3.456344       2.13498         0.1395349     #> 6  11       2.583178       1.587713        0.125         #> 7  13       3.25302        2.05548         0.15          #> 8  12       2.981935       1.897367        0.125         #> 9  13       3.210557       2.005944        0.1428571     #> 10 12       2.924598       1.829983        0.1395349     #> 11 9        2.308312       1.59099         0.21875       #> 12 9        2.250131       1.521278        0.2285714     #> 13 10       2.573997       1.740777        0.2727273     #> 14 7        1.887948       1.428869        0.25          #> 15 8        2.232503       1.668115        0.2173913     #> 16 8        2.001998       1.392621        0.2424242     #> 17 7        2.215616       1.807392        0.2666667     #> 18 9        2.427305       1.732051        0.2222222     #> 19 9        2.329653       1.616448        0.1935484     #> 20 8        2.038447       1.436842        0.1612903     #>    berger_parker_reciprocal simpson    gini_simpson simpson_max #> 1  2.571429                 0.2654321  0.7345679    0.8         #> 2  6                        0.1099773  0.8900227    0.9         #> 3  5.714286                 0.12125    0.87875      0.9         #> 4  5.625                    0.09925926 0.9007407    0.9230769   #> 5  7.166667                 0.08599243 0.9140076    0.9285714   #> 6  8                        0.09982639 0.9001736    0.9090909   #> 7  6.666667                 0.0925     0.9075       0.9230769   #> 8  8                        0.09125    0.90875      0.9166667   #> 9  7                        0.08843537 0.9115646    0.9230769   #> 10 7.166667                 0.09680909 0.9031909    0.9166667   #> 11 4.571429                 0.1328125  0.8671875    0.8888889   #> 12 4.375                    0.1314286  0.8685714    0.8888889   #> 13 3.666667                 0.1478421  0.8521579    0.9         #> 14 4                        0.1666667  0.8333333    0.8571429   #> 15 4.6                      0.1493384  0.8506616    0.875       #> 16 4.125                    0.1570248  0.8429752    0.875       #> 17 3.75                     0.1644444  0.8355556    0.8571429   #> 18 4.5                      0.138546   0.861454     0.8888889   #> 19 5.166667                 0.1259105  0.8740895    0.8888889   #> 20 6.2                      0.132154   0.867846     0.875       #>    simpson_reciprocal simpson_relative simpson_evenness shannon  shannon_max #> 1  3.767442           0.9182099        0.2722689        1.440482 1.609438    #> 2  9.092784           0.9889141        0.1123567        2.252516 2.302585    #> 3  8.247423           0.9763889        0.113798         2.193749 2.302585    #> 4  10.07463           0.9758025        0.0853998        2.426779 2.564949    #> 5  11.62893           0.9843158        0.07814877       2.544421 2.639057    #> 6  10.01739           0.990191         0.1009906        2.345946 2.397895    #> 7  10.81081           0.983125         0.08476372       2.471733 2.564949    #> 8  10.9589            0.9913636        0.09170105       2.434898 2.484907    #> 9  11.30769           0.9875283        0.08438576       2.493568 2.564949    #> 10 10.32961           0.9852992        0.09226547       2.398613 2.484907    #> 11 7.529412           0.9755859        0.1281281        2.106065 2.197225    #> 12 7.608696           0.9771429        0.127924         2.114495 2.197225    #> 13 6.763975           0.9468422        0.1173491        2.099638 2.302585    #> 14 6                  0.9722222        0.1714286        1.86368  1.94591     #> 15 6.696203           0.9721847        0.1469444        1.979309 2.079442    #> 16 6.368421           0.9634002        0.1482843        1.959795 2.079442    #> 17 6.081081           0.9748148        0.1709726        1.876274 1.94591     #> 18 7.217822           0.9691358        0.1289809        2.079387 2.197225    #> 19 7.942149           0.9833507        0.1271164        2.134024 2.197225    #> 20 7.566929           0.991824         0.1440348        2.04827  2.079442    #>    shannon_relative shannon_ens heip_evenness mcintosh_diversity #> 1  0.8950216        4.22273     1.74747       0.6343064          #> 2  0.9782554        9.51164     2.753606      0.7903209          #> 3  0.9527332        8.968777    2.520738      0.7742024          #> 4  0.9461313        11.32235    2.67933       0.8049388          #> 5  0.9641403        12.73586    2.944944      0.8339282          #> 6  0.9783356        10.44315    2.85028       0.7994354          #> 7  0.9636577        11.84296    2.864442      0.8265511          #> 8  0.9798749        11.41465    2.958414      0.8290003          #> 9  0.9721705        12.10439    2.958778      0.830817           #> 10 0.9652727        11.00789    2.802893      0.8128109          #> 11 0.9585114        8.215847    2.484002      0.7720451          #> 12 0.9623483        8.285403    2.515928      0.7671394          #> 13 0.911861         8.163211    2.186597      0.7452246          #> 14 0.9577421        6.44742     2.285477      0.7435226          #> 15 0.9518463        7.237739    2.340544      0.7751964          #> 16 0.9424621        7.097871    2.271604      0.7309845          #> 17 0.9642139        6.529129    2.330436      0.8014042          #> 18 0.9463699        7.999566    2.385495      0.7773914          #> 19 0.9712362        8.448796    2.591391      0.7864035          #> 20 0.9850099        7.754478    2.600327      0.7758096          #>    mcintosh_evenness smith_wilson brillouin_index renyi_entropy_0 #> 1  0.8770096         0.5900996    1.156724        1.609438        #> 2  0.9774771         0.7851387    1.930319        2.302585        #> 3  0.9532272         0.6925466    1.86802         2.302585        #> 4  0.947825          0.6687728    2.056517        2.564949        #> 5  0.9645393         0.7407779    2.132228        2.639057        #> 6  0.9793242         0.7749676    2.02765         2.397895        #> 7  0.9629308         0.7433868    2.06382         2.564949        #> 8  0.9811605         0.7830292    2.047512        2.484907        #> 9  0.9722815         0.7640758    2.095951        2.564949        #> 10 0.968416          0.7006824    2.035221        2.484907        #> 11 0.9533483         0.733735     1.75707         2.197225        #> 12 0.9562038         0.7463774    1.784664        2.197225        #> 13 0.9001501         0.6387338    1.740005        2.302585        #> 14 0.951315          0.7317419    1.524661        1.94591         #> 15 0.9491221         0.6916521    1.590951        2.079442        #> 16 0.9339309         0.6971933    1.654509        2.079442        #> 17 0.9557051         0.7576586    1.417032        1.94591         #> 18 0.9416736         0.686416     1.693929        2.197225        #> 19 0.9677419         0.7738024    1.774003        2.197225        #> 20 0.9845671         0.814859     1.720004        2.079442        #>    renyi_entropy_1 renyi_entropy_2 tsallis_entropy_0 tsallis_entropy_1 #> 1  1.440482        1.326396        4                 1.440482          #> 2  2.252516        2.207481        9                 2.252516          #> 3  2.193749        2.109901        9                 2.193749          #> 4  2.426779        2.31002         12                2.426779          #> 5  2.544421        2.453496        13                2.544421          #> 6  2.345946        2.304323        10                2.345946          #> 7  2.471733        2.380547        12                2.471733          #> 8  2.434898        2.394152        11                2.434898          #> 9  2.493568        2.425483        12                2.493568          #> 10 2.398613        2.335014        11                2.398613          #> 11 2.106065        2.018817        8                 2.106065          #> 12 2.114495        2.029292        8                 2.114495          #> 13 2.099638        1.911611        9                 2.099638          #> 14 1.86368         1.791759        6                 1.86368           #> 15 1.979309        1.901541        7                 1.979309          #> 16 1.959795        1.851352        7                 1.959795          #> 17 1.876274        1.805182        6                 1.876274          #> 18 2.079387        1.976553        8                 2.079387          #> 19 2.134024        2.072184        8                 2.134024          #> 20 2.04827         2.023787        7                 2.04827           #>    tsallis_entropy_2 hill_number_0 hill_number_1 hill_number_2 #> 1  0.7345679         5             4.22273       3.767442      #> 2  0.8900227         10            9.51164       9.092784      #> 3  0.87875           10            8.968777      8.247423      #> 4  0.9007407         13            11.32235      10.07463      #> 5  0.9140076         14            12.73586      11.62893      #> 6  0.9001736         11            10.44315      10.01739      #> 7  0.9075            13            11.84296      10.81081      #> 8  0.90875           12            11.41465      10.9589       #> 9  0.9115646         13            12.10439      11.30769      #> 10 0.9031909         12            11.00789      10.32961      #> 11 0.8671875         9             8.215847      7.529412      #> 12 0.8685714         9             8.285403      7.608696      #> 13 0.8521579         10            8.163211      6.763975      #> 14 0.8333333         7             6.44742       6             #> 15 0.8506616         8             7.237739      6.696203      #> 16 0.8429752         8             7.097871      6.368421      #> 17 0.8355556         7             6.529129      6.081081      #> 18 0.861454          9             7.999566      7.217822      #> 19 0.8740895         9             8.448796      7.942149      #> 20 0.867846          8             7.754478      7.566929"},{"path":"https://aravind-j.github.io/avial/reference/diversity.compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare Diversity Measures — diversity.compare","title":"Compare Diversity Measures — diversity.compare","text":"Compare Diversity Measures","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare Diversity Measures — diversity.compare","text":"","code":"diversity.compare(   x,   group,   R = 1000,   base = exp(1),   na.omit = TRUE,   p.adjust.method = c(\"bonferroni\", \"holm\"),   ci.conf = 0.95,   ci.type = c(\"perc\", \"bca\"),   q = seq(0, 3, 0.1),   parallel = c(\"no\", \"multicore\", \"snow\"),   ncpus = 1L,   cl = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/diversity.compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare Diversity Measures — diversity.compare","text":"x factor vector categories (e.g., species, traits). frequency level treated abundance category. group factor vector indicating group observation. Must length x. R Integer specifying number permutations. Default 1000. base logarithm base used computation shannon family diversity indices. Default exp(1). na.omit logical. TRUE, missing values (NA) ignored included distinct factor level computation. Default TRUE. p.adjust.method (perm.test.pairwise ) Method adjusting p-values multiple comparisons. Options include \"bonferroni\" \"holm\". Default \"bonferroni\". ci.conf Confidence level bootstrap interval. Default 0.95. ci.type vector character strings representing type intervals required. options c(\"perc\", \"bca\"). q order parametric index. parallel type parallel operation used ().  missing,     default taken option \"boot.parallel\" (    set, \"\"). ncpus integer: number processes used parallel operation:     typically one chose number available CPUs. cl optional parallel snow cluster use     parallel = \"snow\".  supplied, cluster     local machine created duration boot call.","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.compare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compare Diversity Measures — diversity.compare","text":"","code":"library(EvaluateCore)  pdata <- cassava_CC  qual <- c(\"CUAL\", \"LNGS\", \"PTLC\", \"DSTA\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Convert qualitative data columns to factor pdata[, qual] <- lapply(pdata[, qual], as.factor)  str(pdata) #> 'data.frame':\t168 obs. of  26 variables: #>  $ CUAL  : Factor w/ 4 levels \"Dark green\",\"Green purple\",..: 3 1 2 2 2 2 4 2 2 1 ... #>  $ LNGS  : Factor w/ 3 levels \"Long\",\"Medium\",..: 3 1 2 2 2 2 2 1 1 1 ... #>  $ PTLC  : Factor w/ 5 levels \"Dark green\",\"Green purple\",..: 3 4 4 4 4 5 4 2 2 5 ... #>  $ DSTA  : Factor w/ 5 levels \"Absent\",\"Central part\",..: 1 5 5 5 5 5 5 4 2 5 ... #>  $ LFRT  : Factor w/ 4 levels \"25-50% leaf retention\",..: 1 1 1 1 3 2 2 2 2 2 ... #>  $ LBTEF : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 5 4 4 3 2 ... #>  $ CBTR  : Factor w/ 3 levels \"Cream\",\"White\",..: 2 2 2 2 1 2 1 1 1 1 ... #>  $ NMLB  : Factor w/ 9 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 4 4 3 3 4 ... #>  $ ANGB  : Factor w/ 4 levels \"150-300\",\"450-600\",..: 1 4 1 4 2 2 2 1 2 2 ... #>  $ CUAL9M: Factor w/ 5 levels \"Dark green\",\"Green\",..: 1 1 3 5 3 3 5 5 5 4 ... #>  $ LVC9M : Factor w/ 5 levels \"Dark green\",\"Green\",..: 4 3 3 3 3 1 3 1 4 3 ... #>  $ TNPR9M: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 5 4 2 5 4 2 5 5 5 ... #>  $ PL9M  : Factor w/ 2 levels \"Long (25-30cm)\",..: 2 2 1 1 1 1 1 1 2 2 ... #>  $ STRP  : Factor w/ 4 levels \"Absent\",\"Intermediate\",..: 2 3 1 1 1 1 4 1 1 4 ... #>  $ STRC  : Factor w/ 2 levels \"Absent\",\"Present\": 2 2 1 2 1 1 2 1 1 2 ... #>  $ PSTR  : Factor w/ 2 levels \"Irregular\",\"Tending toward horizontal\": 1 2 2 2 1 2 2 2 1 2 ... #>  $ NMSR  : num  6 2 6 2 20 13 4 14 10 5 ... #>  $ TTRN  : num  3 0.5 3 2 5 ... #>  $ TFWSR : num  1.4 2.6 1.2 1.6 5 7 4.2 2.8 2.8 4 ... #>  $ TTRW  : num  0.7 0.65 0.6 1.6 1.25 ... #>  $ TFWSS : num  1 2.8 2.8 2.4 16 12 9 4.4 6.2 5 ... #>  $ TTSW  : num  0.5 0.7 1.4 2.4 4 ... #>  $ TTPW  : num  2.4 5.4 4 4 21 19 13.2 7.2 9 9 ... #>  $ AVPW  : num  1.2 1.35 2 4 5.25 4.75 3.3 2.4 1.8 2.25 ... #>  $ ARSR  : num  2 0 2 0 3 0 0 6 0 0 ... #>  $ SRDM  : num  42 39.8 29.7 43 37.9 37 38.9 36.9 41 37.9 ...  diversity.compare(x = pdata$CUAL, group = pdata$LNGS, R = 100,                   base = exp(1), na.omit = TRUE) #> Computing diversity indices. #> Performing global permutation tests. #> Performing pairwise permutation tests. #> Computing bootstrap confidence intervals. #> Generating diversity profiles. #> Error in diversity.profile(x = x, group = group, q = q, conf = ci.conf,     R = R, parameter = \"hill\", ci.type = ci.type, parallel = parallel,     ncpus = ncpus, cl = cl): unused argument (conf = ci.conf)  diversity.compare(x = pdata$ANGB, group = pdata$LNGS, R = 100,                   base = exp(1), na.omit = TRUE) #> Computing diversity indices. #> Performing global permutation tests. #> Performing pairwise permutation tests. #> Computing bootstrap confidence intervals. #> Generating diversity profiles. #> Error in diversity.profile(x = x, group = group, q = q, conf = ci.conf,     R = R, parameter = \"hill\", ci.type = ci.type, parallel = parallel,     ncpus = ncpus, cl = cl): unused argument (conf = ci.conf)"},{"path":"https://aravind-j.github.io/avial/reference/diversity.profile.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Diversity Profiles for Parametric Indices — diversity.profile","title":"Generate Diversity Profiles for Parametric Indices — diversity.profile","text":"Generate Diversity Profiles Parametric Indices","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.profile.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Diversity Profiles for Parametric Indices — diversity.profile","text":"","code":"diversity.profile(   x,   group,   q = seq(0, 3, 0.1),   ci.conf = 0.95,   R = 1000,   parameter = c(\"hill\", \"renyi\", \"tsallis\"),   ci.type = c(\"perc\", \"bca\"),   parallel = c(\"no\", \"multicore\", \"snow\"),   ncpus = getOption(\"boot.ncpus\", 1L),   cl = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/diversity.profile.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Diversity Profiles for Parametric Indices — diversity.profile","text":"x numeric factor vector observations. group factor vector indicating group observation. Must length x. q order parametric index. ci.conf Confidence level bootstrap interval. Default 0.95. R Integer specifying number permutations. Default 1000. parameter parametric index. Options include \"hill\", \"renyi\" \"tsallis\". Default \"hill\". ci.type vector character strings representing type intervals required. options c(\"perc\", \"bca\"). parallel type parallel operation used ().  missing,     default taken option \"boot.parallel\" (    set, \"\"). ncpus integer: number processes used parallel operation:     typically one chose number available CPUs. cl optional parallel snow cluster use     parallel = \"snow\".  supplied, cluster     local machine created duration boot call.","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.profile.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Diversity Profiles for Parametric Indices — diversity.profile","text":"list data frames following columns factor   level group. q  observed  mean  lower  upper","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity.profile.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Diversity Profiles for Parametric Indices — diversity.profile","text":"","code":"library(EvaluateCore) library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union library(ggplot2)  pdata <- cassava_CC  qual <- c(\"CUAL\", \"LNGS\", \"PTLC\", \"DSTA\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Convert qualitative data columns to factor pdata[, qual] <- lapply(pdata[, qual], as.factor)  str(pdata) #> 'data.frame':\t168 obs. of  26 variables: #>  $ CUAL  : Factor w/ 4 levels \"Dark green\",\"Green purple\",..: 3 1 2 2 2 2 4 2 2 1 ... #>  $ LNGS  : Factor w/ 3 levels \"Long\",\"Medium\",..: 3 1 2 2 2 2 2 1 1 1 ... #>  $ PTLC  : Factor w/ 5 levels \"Dark green\",\"Green purple\",..: 3 4 4 4 4 5 4 2 2 5 ... #>  $ DSTA  : Factor w/ 5 levels \"Absent\",\"Central part\",..: 1 5 5 5 5 5 5 4 2 5 ... #>  $ LFRT  : Factor w/ 4 levels \"25-50% leaf retention\",..: 1 1 1 1 3 2 2 2 2 2 ... #>  $ LBTEF : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 5 4 4 3 2 ... #>  $ CBTR  : Factor w/ 3 levels \"Cream\",\"White\",..: 2 2 2 2 1 2 1 1 1 1 ... #>  $ NMLB  : Factor w/ 9 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 4 4 3 3 4 ... #>  $ ANGB  : Factor w/ 4 levels \"150-300\",\"450-600\",..: 1 4 1 4 2 2 2 1 2 2 ... #>  $ CUAL9M: Factor w/ 5 levels \"Dark green\",\"Green\",..: 1 1 3 5 3 3 5 5 5 4 ... #>  $ LVC9M : Factor w/ 5 levels \"Dark green\",\"Green\",..: 4 3 3 3 3 1 3 1 4 3 ... #>  $ TNPR9M: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 5 4 2 5 4 2 5 5 5 ... #>  $ PL9M  : Factor w/ 2 levels \"Long (25-30cm)\",..: 2 2 1 1 1 1 1 1 2 2 ... #>  $ STRP  : Factor w/ 4 levels \"Absent\",\"Intermediate\",..: 2 3 1 1 1 1 4 1 1 4 ... #>  $ STRC  : Factor w/ 2 levels \"Absent\",\"Present\": 2 2 1 2 1 1 2 1 1 2 ... #>  $ PSTR  : Factor w/ 2 levels \"Irregular\",\"Tending toward horizontal\": 1 2 2 2 1 2 2 2 1 2 ... #>  $ NMSR  : num  6 2 6 2 20 13 4 14 10 5 ... #>  $ TTRN  : num  3 0.5 3 2 5 ... #>  $ TFWSR : num  1.4 2.6 1.2 1.6 5 7 4.2 2.8 2.8 4 ... #>  $ TTRW  : num  0.7 0.65 0.6 1.6 1.25 ... #>  $ TFWSS : num  1 2.8 2.8 2.4 16 12 9 4.4 6.2 5 ... #>  $ TTSW  : num  0.5 0.7 1.4 2.4 4 ... #>  $ TTPW  : num  2.4 5.4 4 4 21 19 13.2 7.2 9 9 ... #>  $ AVPW  : num  1.2 1.35 2 4 5.25 4.75 3.3 2.4 1.8 2.25 ... #>  $ ARSR  : num  2 0 2 0 3 0 0 6 0 0 ... #>  $ SRDM  : num  42 39.8 29.7 43 37.9 37 38.9 36.9 41 37.9 ...  important_q <- c(0, 1, 2) important_labels <- c(\"0D\", \"1D\", \"2D\")  # Hill profile - Percentile CIs ----  hill_profile1 <-   diversity.profile(x = pdata$CUAL, group = pdata$LNGS,                     parameter = \"hill\", ci.type = \"perc\") hill_profile1 #> $Long #>      q observed     mean    lower    upper #> 1  0.0 3.000000 3.000000 3.000000 3.000000 #> 2  0.1 2.970041 2.964462 2.921973 2.992033 #> 3  0.2 2.940750 2.930107 2.849671 2.984036 #> 4  0.3 2.912153 2.896950 2.781453 2.976133 #> 5  0.4 2.884272 2.864995 2.716325 2.968355 #> 6  0.5 2.857124 2.834239 2.656209 2.960670 #> 7  0.6 2.830721 2.804671 2.602732 2.953078 #> 8  0.7 2.805073 2.776274 2.551969 2.945159 #> 9  0.8 2.780186 2.749025 2.501417 2.937217 #> 10 0.9 2.756060 2.722897 2.464737 2.929259 #> 11 1.0 2.732695 2.697860 2.427838 2.921292 #> 12 1.1 2.710085 2.673881 2.392008 2.913319 #> 13 1.2 2.688224 2.650925 2.353206 2.905344 #> 14 1.3 2.667101 2.628957 2.316516 2.897373 #> 15 1.4 2.646704 2.607939 2.282076 2.889409 #> 16 1.5 2.627019 2.587835 2.253681 2.881457 #> 17 1.6 2.608031 2.568607 2.229592 2.873521 #> 18 1.7 2.589722 2.550218 2.211941 2.865606 #> 19 1.8 2.572075 2.532631 2.184150 2.857716 #> 20 1.9 2.555070 2.515811 2.158142 2.849891 #> 21 2.0 2.538688 2.499723 2.133687 2.842105 #> 22 2.1 2.522908 2.484333 2.110520 2.836021 #> 23 2.2 2.507711 2.469607 2.088735 2.830098 #> 24 2.3 2.493075 2.455515 2.073086 2.824371 #> 25 2.4 2.478981 2.442025 2.059344 2.818799 #> 26 2.5 2.465409 2.429108 2.040443 2.813378 #> 27 2.6 2.452337 2.416737 2.022650 2.808105 #> 28 2.7 2.439748 2.404883 2.005895 2.802977 #> 29 2.8 2.427621 2.393522 1.990189 2.797990 #> 30 2.9 2.415938 2.382628 1.975378 2.793141 #> 31 3.0 2.404680 2.372180 1.961350 2.788426 #>  #> $Medium #>      q observed     mean    lower    upper #> 1  0.0 4.000000 3.618000 3.000000 4.000000 #> 2  0.1 3.775167 3.474824 2.928394 3.868584 #> 3  0.2 3.586424 3.349383 2.858040 3.748178 #> 4  0.3 3.427791 3.239260 2.789234 3.638082 #> 5  0.4 3.293906 3.142220 2.722250 3.540660 #> 6  0.5 3.180150 3.056280 2.657334 3.456909 #> 7  0.6 3.082659 2.979729 2.594704 3.381874 #> 8  0.7 2.998274 2.911120 2.534539 3.312275 #> 9  0.8 2.924454 2.849246 2.476981 3.244719 #> 10 0.9 2.859178 2.793112 2.420320 3.191025 #> 11 1.0 2.800853 2.741898 2.356336 3.145961 #> 12 1.1 2.748230 2.694932 2.298126 3.101231 #> 13 1.2 2.700330 2.651663 2.243046 3.049204 #> 14 1.3 2.656390 2.611637 2.200438 3.015513 #> 15 1.4 2.615811 2.574478 2.160960 2.990795 #> 16 1.5 2.578125 2.539874 2.110282 2.958994 #> 17 1.6 2.542961 2.507562 2.068316 2.924667 #> 18 1.7 2.510025 2.477321 2.029977 2.913831 #> 19 1.8 2.479080 2.448961 1.998256 2.905417 #> 20 1.9 2.449934 2.422318 1.968838 2.883376 #> 21 2.0 2.422430 2.397251 1.941573 2.857773 #> 22 2.1 2.396434 2.373634 1.916310 2.833450 #> 23 2.2 2.371835 2.351358 1.892905 2.810305 #> 24 2.3 2.348536 2.330324 1.871219 2.788239 #> 25 2.4 2.326451 2.310443 1.851119 2.767126 #> 26 2.5 2.305505 2.291636 1.832482 2.748871 #> 27 2.6 2.285629 2.273830 1.815192 2.739765 #> 28 2.7 2.266761 2.256959 1.799140 2.731614 #> 29 2.8 2.248843 2.240961 1.784226 2.724880 #> 30 2.9 2.231821 2.225780 1.770357 2.716554 #> 31 3.0 2.215647 2.211366 1.757448 2.703064 #>  #> $Short #>      q observed     mean    lower    upper #> 1  0.0 4.000000 3.870000 3.000000 4.000000 #> 2  0.1 3.933810 3.789634 2.936838 3.985467 #> 3  0.2 3.870196 3.713869 2.875438 3.971064 #> 4  0.3 3.809136 3.642581 2.815944 3.956798 #> 5  0.4 3.750595 3.575609 2.758478 3.942676 #> 6  0.5 3.694521 3.512766 2.703140 3.928704 #> 7  0.6 3.640856 3.453847 2.650006 3.914887 #> 8  0.7 3.589527 3.398636 2.606810 3.901233 #> 9  0.8 3.540459 3.346915 2.569479 3.887745 #> 10 0.9 3.493567 3.298466 2.515887 3.874428 #> 11 1.0 3.448767 3.253078 2.476843 3.861288 #> 12 1.1 3.405970 3.210545 2.436475 3.848328 #> 13 1.2 3.365087 3.170670 2.379326 3.835552 #> 14 1.3 3.326031 3.133266 2.328644 3.822963 #> 15 1.4 3.288713 3.098159 2.278574 3.810563 #> 16 1.5 3.253050 3.065183 2.237378 3.798356 #> 17 1.6 3.218958 3.034185 2.200363 3.786343 #> 18 1.7 3.186359 3.005022 2.160763 3.774525 #> 19 1.8 3.155176 2.977560 2.117549 3.762905 #> 20 1.9 3.125338 2.951677 2.078397 3.751483 #> 21 2.0 3.096774 2.927259 2.042918 3.740260 #> 22 2.1 3.069420 2.904202 2.010325 3.729235 #> 23 2.2 3.043214 2.882408 1.980602 3.718410 #> 24 2.3 3.018098 2.861788 1.953460 3.707783 #> 25 2.4 2.994016 2.842261 1.935903 3.697354 #> 26 2.5 2.970917 2.823750 1.912012 3.687122 #> 27 2.6 2.948751 2.806187 1.890173 3.677086 #> 28 2.7 2.927474 2.789507 1.870174 3.667245 #> 29 2.8 2.907041 2.773651 1.851826 3.657596 #> 30 2.9 2.887412 2.758565 1.834963 3.648139 #> 31 3.0 2.868549 2.744198 1.819435 3.638871 #>  #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 #> attr(,\"parameter\") #> [1] \"hill\" #> attr(,\"ci.type\") #> [1] \"perc\"  hill_profile1_df <- dplyr::bind_rows(hill_profile1, .id = \"group\")  hill_points1_df <- hill_profile1_df %>%   filter(q %in% important_q) %>%   mutate(order_label = factor(q, levels = important_q,                               labels = important_labels))  ggplot(hill_profile1_df, aes(x = q, y = mean,                              color = group, fill = group)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +   geom_line(linewidth = 1) +   geom_vline(xintercept = c(0, 1, 2), linetype = \"dashed\",              color = \"grey60\") +   geom_point(data = hill_points1_df, aes(shape = order_label),     size = 3, stroke = 1, inherit.aes = TRUE) +   scale_shape_manual(values = c(17, 18, 15), name = \"Important q\")  +   labs(x = \"Order (q)\", y = \"Hill number\",     color = \"Group\", fill = \"Group\") +   theme_bw()   ggplot(hill_profile1_df, aes(x = q, y = mean)) +   geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey80\") +   geom_line(color = \"black\", linewidth = 1) +   facet_wrap(~ group, scales = \"free_y\") +   labs(x = \"Order (q)\", y = \"Hill number\") +   theme_bw()   # Rényi profile - Percentile CIs ----  renyi_profile1 <-   diversity.profile(pdata$CUAL, group = pdata$LNGS,                     parameter = \"renyi\", ci.type = \"perc\") renyi_profile1 #> $Long #>      q  observed      mean     lower    upper #> 1  0.0 1.0986123 1.0986123 1.0986123 1.098612 #> 2  0.1 1.0885756 1.0865618 1.0702441 1.096196 #> 3  0.2 1.0786646 1.0747426 1.0430139 1.093809 #> 4  0.3 1.0688928 1.0631763 1.0169968 1.091453 #> 5  0.4 1.0592726 1.0518810 0.9922425 1.089129 #> 6  0.5 1.0498155 1.0408710 0.9687767 1.086839 #> 7  0.6 1.0405315 1.0301576 0.9466030 1.084581 #> 8  0.7 1.0314297 1.0197490 0.9257060 1.082357 #> 9  0.8 1.0225177 1.0096505 0.9060549 1.080167 #> 10 0.9 1.0138021 0.9998651 0.8876062 1.078012 #> 11 1.0 1.0052882 0.9903937 0.8703073 1.075892 #> 12 1.1 0.9969801 0.9812351 0.8540994 1.073806 #> 13 1.2 0.9888807 0.9723868 0.8389295 1.071756 #> 14 1.3 0.9809920 0.9638447 0.8247041 1.069741 #> 15 1.4 0.9733151 0.9556036 0.8113880 1.067721 #> 16 1.5 0.9658499 0.9476575 0.7989085 1.065502 #> 17 1.6 0.9585956 0.9399994 0.7872048 1.063285 #> 18 1.7 0.9515507 0.9326219 0.7744250 1.061072 #> 19 1.8 0.9447130 0.9255171 0.7655213 1.058863 #> 20 1.9 0.9380796 0.9186766 0.7530072 1.056660 #> 21 2.0 0.9316472 0.9120919 0.7405494 1.054463 #> 22 2.1 0.9254122 0.9057544 0.7287438 1.052595 #> 23 2.2 0.9193702 0.8996552 0.7204869 1.050761 #> 24 2.3 0.9135169 0.8937856 0.7102029 1.048959 #> 25 2.4 0.9078477 0.8881370 0.6999060 1.047190 #> 26 2.5 0.9023576 0.8827007 0.6901195 1.045453 #> 27 2.6 0.8970416 0.8774683 0.6808507 1.043747 #> 28 2.7 0.8918947 0.8724316 0.6735163 1.042073 #> 29 2.8 0.8869118 0.8675825 0.6655820 1.040430 #> 30 2.9 0.8820876 0.8629132 0.6576276 1.038818 #> 31 3.0 0.8774170 0.8584161 0.6500998 1.037236 #>  #> $Medium #>      q  observed      mean     lower    upper #> 1  0.0 1.3862944 1.2781259 1.0986123 1.386294 #> 2  0.1 1.3284446 1.2398862 1.0744540 1.353412 #> 3  0.2 1.2771555 1.2046894 1.0501362 1.322055 #> 4  0.3 1.2319160 1.1723878 1.0257822 1.292512 #> 5  0.4 1.1920742 1.1427638 1.0014999 1.265418 #> 6  0.5 1.1569284 1.1155657 0.9773893 1.238818 #> 7  0.6 1.1257925 1.0905352 0.9564127 1.215004 #> 8  0.7 1.0980367 1.0674257 0.9331855 1.194989 #> 9  0.8 1.0731077 1.0460123 0.9070404 1.176575 #> 10 0.9 1.0505342 1.0260962 0.8826597 1.160348 #> 11 1.0 1.0299241 1.0075059 0.8521366 1.146228 #> 12 1.1 1.0109570 0.9900950 0.8278343 1.132964 #> 13 1.2 0.9933740 0.9737398 0.8048512 1.121603 #> 14 1.3 0.9769680 0.9583359 0.7806750 1.110516 #> 15 1.4 0.9615743 0.9437954 0.7597311 1.100779 #> 16 1.5 0.9470625 0.9300440 0.7368298 1.091446 #> 17 1.6 0.9333293 0.9170182 0.7176593 1.082706 #> 18 1.7 0.9202928 0.9046635 0.6999353 1.075507 #> 19 1.8 0.9078876 0.8929326 0.6835520 1.068292 #> 20 1.9 0.8960613 0.8817841 0.6669937 1.063285 #> 21 2.0 0.8847711 0.8711810 0.6502165 1.057679 #> 22 2.1 0.8739819 0.8610904 0.6381780 1.051989 #> 23 2.2 0.8636641 0.8514823 0.6269048 1.046633 #> 24 2.3 0.8537922 0.8423293 0.6163470 1.041014 #> 25 2.4 0.8443440 0.8336062 0.6064569 1.035889 #> 26 2.5 0.8352998 0.8252896 0.5971891 1.032109 #> 27 2.6 0.8266413 0.8173575 0.5885003 1.028398 #> 28 2.7 0.8183519 0.8097897 0.5803502 1.024878 #> 29 2.8 0.8104158 0.8025668 0.5727006 1.021446 #> 30 2.9 0.8028180 0.7956708 0.5655160 1.018088 #> 31 3.0 0.7955444 0.7890845 0.5587634 1.013208 #>  #> $Short #>      q observed     mean     lower    upper #> 1  0.0 1.386294 1.346882 1.0986123 1.386294 #> 2  0.1 1.369609 1.326686 1.0802640 1.382655 #> 3  0.2 1.353305 1.307105 1.0622339 1.379034 #> 4  0.3 1.337402 1.288178 1.0445664 1.375435 #> 5  0.4 1.321914 1.269936 1.0273014 1.371860 #> 6  0.5 1.306851 1.252399 1.0079721 1.368309 #> 7  0.6 1.292219 1.235575 0.9898612 1.364787 #> 8  0.7 1.278021 1.219466 0.9637953 1.361293 #> 9  0.8 1.264256 1.204064 0.9364895 1.357829 #> 10 0.9 1.250923 1.189359 0.9181029 1.354398 #> 11 1.0 1.238017 1.175332 0.9002561 1.351001 #> 12 1.1 1.225530 1.161964 0.8790134 1.347639 #> 13 1.2 1.213454 1.149231 0.8425911 1.344313 #> 14 1.3 1.201780 1.137108 0.8124411 1.341026 #> 15 1.4 1.190496 1.125569 0.7979626 1.337777 #> 16 1.5 1.179593 1.114588 0.7843453 1.334568 #> 17 1.6 1.169058 1.104136 0.7715719 1.331401 #> 18 1.7 1.158879 1.094188 0.7524150 1.328275 #> 19 1.8 1.149044 1.084718 0.7335592 1.325191 #> 20 1.9 1.139542 1.075700 0.7161671 1.322151 #> 21 2.0 1.130361 1.067110 0.7001158 1.319155 #> 22 2.1 1.121489 1.058925 0.6852904 1.316203 #> 23 2.2 1.112914 1.051122 0.6715842 1.313296 #> 24 2.3 1.104627 1.043680 0.6588993 1.310434 #> 25 2.4 1.096616 1.036579 0.6471460 1.307617 #> 26 2.5 1.088871 1.029800 0.6362426 1.304846 #> 27 2.6 1.081382 1.023324 0.6261147 1.302121 #> 28 2.7 1.074140 1.017135 0.6167513 1.299441 #> 29 2.8 1.067136 1.011217 0.6082751 1.296806 #> 30 2.9 1.060361 1.005555 0.6003059 1.294217 #> 31 3.0 1.053806 1.000135 0.5928118 1.291673 #>  #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 #> attr(,\"parameter\") #> [1] \"renyi\" #> attr(,\"ci.type\") #> [1] \"perc\"  renyi_profile1_df <- dplyr::bind_rows(renyi_profile1, .id = \"group\")  renyi_points1_df <- renyi_profile1_df %>%   filter(q %in% important_q) %>%   mutate(order_label = factor(q, levels = important_q,                               labels = important_labels))  ggplot(renyi_profile1_df, aes(x = q, y = mean,                               color = group, fill = group)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +   geom_line(linewidth = 1) +   geom_vline(xintercept = c(0, 1, 2), linetype = \"dashed\",              color = \"grey60\") +   geom_point(data = renyi_points1_df, aes(shape = order_label),              size = 3, stroke = 1, inherit.aes = TRUE) +   scale_shape_manual(values = c(17, 18, 15), name = \"Important q\")  +   labs(x = \"Order (q)\", y = \"Hill number\",        color = \"Group\", fill = \"Group\") +   theme_bw()   ggplot(renyi_profile1_df, aes(x = q, y = mean)) +   geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey80\") +   geom_line(color = \"black\", linewidth = 1) +   facet_wrap(~ group, scales = \"free_y\") +   labs(x = \"Order (q)\", y = \"Hill number\") +   theme_bw()   # Tsallis profile - Percentile CIs ----  tsallis_profile1 <-   diversity.profile(pdata$CUAL, group = pdata$LNGS,                     parameter = \"tsallis\", ci.type = \"perc\") tsallis_profile1 <-   diversity.profile(x = pdata$CUAL, group = pdata$LNGS,                     parameter = \"hill\", ci.type = \"perc\") tsallis_profile1 #> $Long #>      q observed     mean    lower    upper #> 1  0.0 3.000000 3.000000 3.000000 3.000000 #> 2  0.1 2.970041 2.964005 2.918281 2.990243 #> 3  0.2 2.940750 2.929227 2.842205 2.980660 #> 4  0.3 2.912153 2.895680 2.766603 2.971255 #> 5  0.4 2.884272 2.863370 2.697276 2.962029 #> 6  0.5 2.857124 2.832294 2.634839 2.952984 #> 7  0.6 2.830721 2.802439 2.577006 2.944120 #> 8  0.7 2.805073 2.773787 2.523649 2.935440 #> 9  0.8 2.780186 2.746314 2.472455 2.926943 #> 10 0.9 2.756060 2.719991 2.420273 2.918630 #> 11 1.0 2.732695 2.694785 2.379587 2.910499 #> 12 1.1 2.710085 2.670662 2.345574 2.902551 #> 13 1.2 2.688224 2.647585 2.313866 2.894784 #> 14 1.3 2.667101 2.625515 2.276373 2.887198 #> 15 1.4 2.646704 2.604415 2.237303 2.879200 #> 16 1.5 2.627019 2.584245 2.209862 2.870806 #> 17 1.6 2.608031 2.564967 2.178353 2.862464 #> 18 1.7 2.589722 2.546540 2.148805 2.854179 #> 19 1.8 2.572075 2.528928 2.121010 2.845953 #> 20 1.9 2.555070 2.512094 2.094998 2.837791 #> 21 2.0 2.538688 2.496001 2.072276 2.829694 #> 22 2.1 2.522908 2.480614 2.053096 2.821668 #> 23 2.2 2.507711 2.465899 2.035159 2.813713 #> 24 2.3 2.493075 2.451825 2.018361 2.805834 #> 25 2.4 2.478981 2.438359 2.002699 2.798033 #> 26 2.5 2.465409 2.425471 1.987984 2.790312 #> 27 2.6 2.452337 2.413133 1.970305 2.782674 #> 28 2.7 2.439748 2.401317 1.953520 2.775120 #> 29 2.8 2.427621 2.389998 1.937734 2.767654 #> 30 2.9 2.415938 2.379150 1.923949 2.760276 #> 31 3.0 2.404680 2.368750 1.912568 2.752989 #>  #> $Medium #>      q observed     mean    lower    upper #> 1  0.0 4.000000 3.635000 3.000000 4.000000 #> 2  0.1 3.775167 3.490832 2.917189 3.870241 #> 3  0.2 3.586424 3.364547 2.836007 3.751049 #> 4  0.3 3.427791 3.253707 2.757403 3.641924 #> 5  0.4 3.293906 3.156065 2.687251 3.544876 #> 6  0.5 3.180150 3.069623 2.622189 3.453736 #> 7  0.6 3.082659 2.992660 2.560105 3.376683 #> 8  0.7 2.998274 2.923718 2.481607 3.306601 #> 9  0.8 2.924454 2.861581 2.427891 3.243245 #> 10 0.9 2.859178 2.805243 2.362856 3.182589 #> 11 1.0 2.800853 2.753879 2.311537 3.126803 #> 12 1.1 2.748230 2.706807 2.263423 3.086563 #> 13 1.2 2.700330 2.663470 2.196568 3.050088 #> 14 1.3 2.656390 2.623407 2.150506 3.014391 #> 15 1.4 2.615811 2.586237 2.108882 2.995325 #> 16 1.5 2.578125 2.551643 2.062331 2.973004 #> 17 1.6 2.542961 2.519357 2.027604 2.951542 #> 18 1.7 2.510025 2.489153 1.996164 2.928358 #> 19 1.8 2.479080 2.460840 1.968008 2.905641 #> 20 1.9 2.449934 2.434250 1.934899 2.884357 #> 21 2.0 2.422430 2.409239 1.904764 2.864088 #> 22 2.1 2.396434 2.385681 1.880430 2.845464 #> 23 2.2 2.371835 2.363464 1.860426 2.826717 #> 24 2.3 2.348536 2.342488 1.841885 2.815594 #> 25 2.4 2.326451 2.322663 1.824678 2.807810 #> 26 2.5 2.305505 2.303910 1.808691 2.795340 #> 27 2.6 2.285629 2.286154 1.793818 2.783102 #> 28 2.7 2.266761 2.269330 1.779963 2.769766 #> 29 2.8 2.248843 2.253376 1.767041 2.757046 #> 30 2.9 2.231821 2.238235 1.754973 2.744797 #> 31 3.0 2.215647 2.223857 1.743689 2.732984 #>  #> $Short #>      q observed     mean    lower    upper #> 1  0.0 4.000000 3.865000 3.000000 4.000000 #> 2  0.1 3.933810 3.784529 2.945457 3.981757 #> 3  0.2 3.870196 3.708572 2.892826 3.963499 #> 4  0.3 3.809136 3.637027 2.841679 3.945237 #> 5  0.4 3.750595 3.569751 2.790285 3.926984 #> 6  0.5 3.694521 3.506571 2.727950 3.908751 #> 7  0.6 3.640856 3.447297 2.651020 3.890550 #> 8  0.7 3.589527 3.391723 2.607068 3.872393 #> 9  0.8 3.540459 3.339638 2.550539 3.854291 #> 10 0.9 3.493567 3.290829 2.504391 3.836258 #> 11 1.0 3.448767 3.245087 2.460233 3.818303 #> 12 1.1 3.405970 3.202209 2.418476 3.800440 #> 13 1.2 3.365087 3.161999 2.376675 3.783047 #> 14 1.3 3.326031 3.124274 2.324285 3.769731 #> 15 1.4 3.288713 3.088857 2.268437 3.757029 #> 16 1.5 3.253050 3.055584 2.214538 3.744916 #> 17 1.6 3.218958 3.024304 2.165962 3.733366 #> 18 1.7 3.186359 2.994872 2.137778 3.722354 #> 19 1.8 3.155176 2.967156 2.114253 3.711856 #> 20 1.9 3.125338 2.941034 2.078278 3.701848 #> 21 2.0 3.096774 2.916393 2.042553 3.692308 #> 22 2.1 3.069420 2.893127 2.010015 3.683045 #> 23 2.2 3.043214 2.871140 1.980341 3.674198 #> 24 2.3 3.018098 2.850343 1.953239 3.665745 #> 25 2.4 2.994016 2.830652 1.928449 3.657669 #> 26 2.5 2.970917 2.811993 1.905739 3.649951 #> 27 2.6 2.948751 2.794295 1.884899 3.642573 #> 28 2.7 2.927474 2.777494 1.865745 3.635520 #> 29 2.8 2.907041 2.761530 1.848111 3.628774 #> 30 2.9 2.887412 2.746349 1.831849 3.622398 #> 31 3.0 2.868549 2.731898 1.816827 3.616389 #>  #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 #> attr(,\"parameter\") #> [1] \"hill\" #> attr(,\"ci.type\") #> [1] \"perc\"  tsallis_profile1_df <- dplyr::bind_rows(tsallis_profile1, .id = \"group\")  tsallis_points1_df <- tsallis_profile1_df %>%   filter(q %in% important_q) %>%   mutate(order_label = factor(q, levels = important_q,                               labels = important_labels))  ggplot(tsallis_profile1_df, aes(x = q, y = mean,                                 color = group, fill = group)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +   geom_line(linewidth = 1) +   geom_vline(xintercept = c(0, 1, 2), linetype = \"dashed\",              color = \"grey60\") +   geom_point(data = tsallis_points1_df, aes(shape = order_label),              size = 3, stroke = 1, inherit.aes = TRUE) +   scale_shape_manual(values = c(17, 18, 15), name = \"Important q\")  +   labs(x = \"Order (q)\", y = \"Hill number\",        color = \"Group\", fill = \"Group\") +   theme_bw()   ggplot(tsallis_profile1_df, aes(x = q, y = mean)) +   geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey80\") +   geom_line(color = \"black\", linewidth = 1) +   facet_wrap(~ group, scales = \"free_y\") +   labs(x = \"Order (q)\", y = \"Hill number\") +   theme_bw()   # Hill profile - BCa CIs ----  hill_profile2 <-   diversity.profile(pdata$CUAL, group = pdata$LNGS,                     parameter = \"hill\", ci.type = \"bca\") #> [1] \"All values of t are equal to  3 \\n Cannot calculate confidence intervals\" #> Warning: bca CI failed for component 1; using percentile CI. hill_profile2 #> $Long #>      q observed     mean    lower    upper #> 1  0.0 3.000000 3.000000 3.000000 3.000000 #> 2  0.1 2.970041 2.964834 2.933296 2.994320 #> 3  0.2 2.940750 2.930808 2.868332 2.987978 #> 4  0.3 2.912153 2.897938 2.805287 2.981887 #> 5  0.4 2.884272 2.866232 2.747428 2.975794 #> 6  0.5 2.857124 2.835688 2.685560 2.968217 #> 7  0.6 2.830721 2.806297 2.632074 2.963540 #> 8  0.7 2.805073 2.778044 2.580153 2.957384 #> 9  0.8 2.780186 2.750909 2.535030 2.951211 #> 10 0.9 2.756060 2.724868 2.491869 2.944861 #> 11 1.0 2.732695 2.699894 2.429521 2.937493 #> 12 1.1 2.710085 2.675956 2.414275 2.931765 #> 13 1.2 2.688224 2.653022 2.385625 2.934833 #> 14 1.3 2.667101 2.631059 2.354913 2.928462 #> 15 1.4 2.646704 2.610031 2.321977 2.921962 #> 16 1.5 2.627019 2.589904 2.287783 2.912226 #> 17 1.6 2.608031 2.570642 2.259315 2.902321 #> 18 1.7 2.589722 2.552210 2.229107 2.896654 #> 19 1.8 2.572075 2.534573 2.205325 2.891044 #> 20 1.9 2.555070 2.517696 2.180283 2.885492 #> 21 2.0 2.538688 2.501546 2.136851 2.876804 #> 22 2.1 2.522908 2.486091 2.117131 2.870655 #> 23 2.2 2.507711 2.471297 2.100753 2.866828 #> 24 2.3 2.493075 2.457134 2.081227 2.861725 #> 25 2.4 2.478981 2.443572 2.064863 2.856540 #> 26 2.5 2.465409 2.430583 2.037716 2.844937 #> 27 2.6 2.452337 2.418139 2.022650 2.840106 #> 28 2.7 2.439748 2.406213 2.005416 2.833891 #> 29 2.8 2.427621 2.394780 1.990114 2.827488 #> 30 2.9 2.415938 2.383817 1.980617 2.822934 #> 31 3.0 2.404680 2.373300 1.961227 2.816313 #>  #> $Medium #>      q observed     mean    lower    upper #> 1  0.0 4.000000 3.631000 3.000000 4.000000 #> 2  0.1 3.775167 3.485098 2.948328 3.916877 #> 3  0.2 3.586424 3.357390 2.897483 3.837768 #> 4  0.3 3.427791 3.245375 2.847616 3.762582 #> 5  0.4 3.293906 3.146746 2.806758 3.691204 #> 6  0.5 3.180150 3.059460 2.759059 3.623498 #> 7  0.6 3.082659 2.981757 2.707785 3.559316 #> 8  0.7 2.998274 2.912152 2.658697 3.495770 #> 9  0.8 2.924454 2.849409 2.597714 3.402135 #> 10 0.9 2.859178 2.792509 2.540062 3.332326 #> 11 1.0 2.800853 2.740616 2.467705 3.289954 #> 12 1.1 2.748230 2.693046 2.415144 3.250581 #> 13 1.2 2.700330 2.649238 2.333804 3.178921 #> 14 1.3 2.656390 2.608729 2.271972 3.133945 #> 15 1.4 2.615811 2.571138 2.203678 3.091903 #> 16 1.5 2.578125 2.536147 2.167528 3.071784 #> 17 1.6 2.542961 2.503490 2.115054 3.013638 #> 18 1.7 2.510025 2.472941 2.077652 2.985985 #> 19 1.8 2.479080 2.444307 2.044131 2.967850 #> 20 1.9 2.449934 2.417423 2.001137 2.948926 #> 21 2.0 2.422430 2.392142 1.975610 2.932127 #> 22 2.1 2.396434 2.368339 1.943755 2.914580 #> 23 2.2 2.371835 2.345900 1.914662 2.894450 #> 24 2.3 2.348536 2.324724 1.890825 2.886035 #> 25 2.4 2.326451 2.304722 1.867933 2.867149 #> 26 2.5 2.305505 2.285810 1.846558 2.846392 #> 27 2.6 2.285629 2.267916 1.827140 2.831564 #> 28 2.7 2.266761 2.250971 1.809294 2.821134 #> 29 2.8 2.248843 2.234912 1.792569 2.807983 #> 30 2.9 2.231821 2.219683 1.777714 2.800794 #> 31 3.0 2.215647 2.205230 1.763721 2.786982 #>  #> $Short #>      q observed     mean    lower    upper #> 1  0.0 4.000000 3.878000 3.000000 4.000000 #> 2  0.1 3.933810 3.796697 3.818059 3.997186 #> 3  0.2 3.870196 3.720115 3.638656 3.994379 #> 4  0.3 3.809136 3.648126 3.468122 3.991579 #> 5  0.4 3.750595 3.580561 3.317762 3.988787 #> 6  0.5 3.694521 3.517224 3.117807 3.986001 #> 7  0.6 3.640856 3.457901 2.990589 3.983223 #> 8  0.7 3.589527 3.402365 2.927570 3.980453 #> 9  0.8 3.540459 3.350385 2.926056 3.977690 #> 10 0.9 3.493567 3.301734 2.906269 3.974936 #> 11 1.0 3.448767 3.256189 2.867204 3.972189 #> 12 1.1 3.405970 3.213534 2.800024 3.969451 #> 13 1.2 3.365087 3.173566 2.721055 3.966722 #> 14 1.3 3.326031 3.136091 2.677041 3.964001 #> 15 1.4 3.288713 3.100928 2.623686 3.961288 #> 16 1.5 3.253050 3.067908 2.571539 3.958585 #> 17 1.6 3.218958 3.036875 2.458114 3.927512 #> 18 1.7 3.186359 3.007683 2.404432 3.918594 #> 19 1.8 3.155176 2.980196 2.371133 3.909777 #> 20 1.9 3.125338 2.954292 2.346311 3.903948 #> 21 2.0 3.096774 2.929854 2.317359 3.892499 #> 22 2.1 3.069420 2.906778 2.229272 3.883764 #> 23 2.2 3.043214 2.884966 2.193922 3.873837 #> 24 2.3 3.018098 2.864329 2.161406 3.837899 #> 25 2.4 2.994016 2.844785 2.131480 3.822987 #> 26 2.5 2.970917 2.826259 2.103917 3.813759 #> 27 2.6 2.948751 2.808679 2.078508 3.804187 #> 28 2.7 2.927474 2.791984 2.041816 3.798686 #> 29 2.8 2.907041 2.776112 2.021941 3.793312 #> 30 2.9 2.887412 2.761011 2.003460 3.788060 #> 31 3.0 2.868549 2.746630 1.986254 3.782930 #>  #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 #> attr(,\"parameter\") #> [1] \"hill\" #> attr(,\"ci.type\") #> [1] \"bca\"  hill_profile2_df <- dplyr::bind_rows(hill_profile2, .id = \"group\")  hill_points2_df <- hill_profile2_df %>%   filter(q %in% important_q) %>%   mutate(order_label = factor(q, levels = important_q,                               labels = important_labels))  ggplot(hill_profile2_df, aes(x = q, y = mean,                              color = group, fill = group)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +   geom_line(linewidth = 1) +   geom_vline(xintercept = c(0, 1, 2), linetype = \"dashed\",              color = \"grey60\") +   geom_point(data = hill_points2_df, aes(shape = order_label),              size = 3, stroke = 1, inherit.aes = TRUE) +   scale_shape_manual(values = c(17, 18, 15), name = \"Important q\")  +   labs(x = \"Order (q)\", y = \"Hill number\",        color = \"Group\", fill = \"Group\") +   theme_bw()   ggplot(hill_profile2_df, aes(x = q, y = mean)) +   geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey80\") +   geom_line(color = \"black\", linewidth = 1) +   facet_wrap(~ group, scales = \"free_y\") +   labs(x = \"Order (q)\", y = \"Hill number\") +   theme_bw()   # Rényi profile - BCa CIs ----  renyi_profile2 <-   diversity.profile(pdata$CUAL, group = pdata$LNGS,                     parameter = \"renyi\", ci.type = \"bca\") #> [1] \"All values of t are equal to  1.09861228866811 \\n Cannot calculate confidence intervals\" #> Warning: bca CI failed for component 1; using percentile CI. renyi_profile2 #> $Long #>      q  observed      mean     lower    upper #> 1  0.0 1.0986123 1.0986123 1.0986123 1.098612 #> 2  0.1 1.0885756 1.0865304 1.0752250 1.097043 #> 3  0.2 1.0786646 1.0746953 1.0510655 1.095080 #> 4  0.3 1.0688928 1.0631283 1.0287592 1.093340 #> 5  0.4 1.0592726 1.0518467 1.0078880 1.091618 #> 6  0.5 1.0498155 1.0408641 0.9856860 1.088453 #> 7  0.6 1.0405315 1.0301906 0.9656715 1.087894 #> 8  0.7 1.0314297 1.0198332 0.9457501 1.085845 #> 9  0.8 1.0225177 1.0097963 0.9258047 1.083714 #> 10 0.9 1.0138021 1.0000816 0.9074091 1.081498 #> 11 1.0 1.0052882 0.9906889 0.8876943 1.078010 #> 12 1.1 0.9969801 0.9816160 0.8722954 1.076279 #> 13 1.2 0.9888807 0.9728592 0.8666497 1.078464 #> 14 1.3 0.9809920 0.9644136 0.8503679 1.076748 #> 15 1.4 0.9733151 0.9562731 0.8356063 1.075006 #> 16 1.5 0.9658499 0.9484308 0.8210556 1.073238 #> 17 1.6 0.9585956 0.9408788 0.8028462 1.069005 #> 18 1.7 0.9515507 0.9336092 0.7904254 1.063537 #> 19 1.8 0.9447130 0.9266131 0.7794792 1.061525 #> 20 1.9 0.9380796 0.9198818 0.7691472 1.059491 #> 21 2.0 0.9316472 0.9134060 0.7494889 1.056680 #> 22 2.1 0.9254122 0.9071768 0.7390188 1.054949 #> 23 2.2 0.9193702 0.9011848 0.7302797 1.053253 #> 24 2.3 0.9135169 0.8954210 0.7225839 1.051591 #> 25 2.4 0.9078477 0.8898764 0.7152995 1.049962 #> 26 2.5 0.9023576 0.8845421 0.7023542 1.046995 #> 27 2.6 0.8970416 0.8794094 0.6943327 1.046179 #> 28 2.7 0.8918947 0.8744700 0.6863623 1.042145 #> 29 2.8 0.8869118 0.8697157 0.6794947 1.040515 #> 30 2.9 0.8820876 0.8651384 0.6748348 1.041701 #> 31 3.0 0.8774170 0.8607307 0.6661136 1.037347 #>  #> $Medium #>      q  observed      mean     lower    upper #> 1  0.0 1.3862944 1.2827288 1.0986123 1.386294 #> 2  0.1 1.3284446 1.2444696 1.0800194 1.364798 #> 3  0.2 1.2771555 1.2092115 1.0612282 1.344696 #> 4  0.3 1.2319160 1.1768129 1.0443952 1.325958 #> 5  0.4 1.1920742 1.1470642 1.0273632 1.308536 #> 6  0.5 1.1569284 1.1197218 1.0093490 1.291480 #> 7  0.6 1.1257925 1.0945353 0.9897522 1.257202 #> 8  0.7 1.0980367 1.0712645 0.9717405 1.228088 #> 9  0.8 1.0731077 1.0496894 0.9524782 1.208655 #> 10 0.9 1.0505342 1.0296153 0.9262639 1.190482 #> 11 1.0 1.0299241 1.0108731 0.9032885 1.175827 #> 12 1.1 1.0109570 0.9933180 0.8737111 1.159092 #> 13 1.2 0.9933740 0.9768271 0.8407928 1.142953 #> 14 1.3 0.9769680 0.9612964 0.8107183 1.130458 #> 15 1.4 0.9615743 0.9466378 0.7869190 1.118255 #> 16 1.5 0.9470625 0.9327766 0.7690664 1.108506 #> 17 1.6 0.9333293 0.9196489 0.7487027 1.093759 #> 18 1.7 0.9202928 0.9071997 0.7252592 1.086582 #> 19 1.8 0.9078876 0.8953810 0.7075616 1.079943 #> 20 1.9 0.8960613 0.8841509 0.6890120 1.068189 #> 21 2.0 0.8847711 0.8734720 0.6732840 1.063361 #> 22 2.1 0.8739819 0.8633108 0.6581026 1.053590 #> 23 2.2 0.8636641 0.8536368 0.6447409 1.048170 #> 24 2.3 0.8537922 0.8444225 0.6326191 1.041049 #> 25 2.4 0.8443440 0.8356421 0.6209520 1.034659 #> 26 2.5 0.8352998 0.8272719 0.6098012 1.029324 #> 27 2.6 0.8266413 0.8192899 0.5997354 1.023595 #> 28 2.7 0.8183519 0.8116754 0.5903534 1.019182 #> 29 2.8 0.8104158 0.8044089 0.5815813 1.015052 #> 30 2.9 0.8028180 0.7974722 0.5734025 1.011028 #> 31 3.0 0.7955444 0.7908480 0.5657655 1.006877 #>  #> $Short #>      q observed      mean     lower    upper #> 1  0.0 1.386294 1.3486080 1.0986123 1.386294 #> 2  0.1 1.369609 1.3276122 1.3459642 1.385591 #> 3  0.2 1.353305 1.3072802 1.3037263 1.384888 #> 4  0.3 1.337402 1.2876537 1.2694452 1.384187 #> 5  0.4 1.321914 1.2687615 1.2237626 1.383487 #> 6  0.5 1.306851 1.2506204 1.1823724 1.382788 #> 7  0.6 1.292219 1.2332370 1.1438281 1.382091 #> 8  0.7 1.278021 1.2166090 1.0949501 1.381396 #> 9  0.8 1.264256 1.2007268 1.0944294 1.380701 #> 10 0.9 1.250923 1.1855749 1.0850720 1.380009 #> 11 1.0 1.238017 1.1711333 1.0722551 1.379317 #> 12 1.1 1.225530 1.1573790 1.0484805 1.378628 #> 13 1.2 1.213454 1.1442861 1.0259463 1.377940 #> 14 1.3 1.201780 1.1318277 1.0045976 1.377254 #> 15 1.4 1.190496 1.1199758 0.9843791 1.376569 #> 16 1.5 1.179593 1.1087021 0.9601189 1.375887 #> 17 1.6 1.169058 1.0979784 0.9300702 1.375206 #> 18 1.7 1.158879 1.0877769 0.9184933 1.374527 #> 19 1.8 1.149044 1.0780706 0.9052338 1.373850 #> 20 1.9 1.139542 1.0688333 0.8940243 1.373174 #> 21 2.0 1.130361 1.0600396 0.8828692 1.372501 #> 22 2.1 1.121489 1.0516652 0.8406928 1.369159 #> 23 2.2 1.112914 1.0436871 0.8309463 1.367159 #> 24 2.3 1.104627 1.0360831 0.8143203 1.354959 #> 25 2.4 1.096616 1.0288325 0.8038681 1.348414 #> 26 2.5 1.088871 1.0219153 0.7967118 1.346087 #> 27 2.6 1.081382 1.0153129 0.7707513 1.336102 #> 28 2.7 1.074140 1.0090076 0.7138397 1.334655 #> 29 2.8 1.067136 1.0029828 0.7038394 1.333239 #> 30 2.9 1.060361 0.9972229 0.6940480 1.331854 #> 31 3.0 1.053806 0.9917130 0.6830015 1.330499 #>  #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 #> attr(,\"parameter\") #> [1] \"renyi\" #> attr(,\"ci.type\") #> [1] \"bca\"  renyi_profile2_df <- dplyr::bind_rows(renyi_profile2, .id = \"group\")  renyi_points2_df <- renyi_profile2_df %>%   filter(q %in% important_q) %>%   mutate(order_label = factor(q, levels = important_q,                               labels = important_labels))  ggplot(renyi_profile2_df, aes(x = q, y = mean,                               color = group, fill = group)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +   geom_line(linewidth = 1) +   geom_vline(xintercept = c(0, 1, 2), linetype = \"dashed\",              color = \"grey60\") +   geom_point(data = renyi_points2_df, aes(shape = order_label),              size = 3, stroke = 1, inherit.aes = TRUE) +   scale_shape_manual(values = c(17, 18, 15), name = \"Important q\")  +   labs(x = \"Order (q)\", y = \"Hill number\",        color = \"Group\", fill = \"Group\") +   theme_bw()   ggplot(renyi_profile2_df, aes(x = q, y = mean)) +   geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey80\") +   geom_line(color = \"black\", linewidth = 1) +   facet_wrap(~ group, scales = \"free_y\") +   labs(x = \"Order (q)\", y = \"Hill number\") +   theme_bw()   # Tsallis profile - BCa CIs ----  tsallis_profile2 <-   diversity.profile(pdata$CUAL, group = pdata$LNGS,                     parameter = \"tsallis\", ci.type = \"bca\") #> [1] \"All values of t are equal to  2 \\n Cannot calculate confidence intervals\" #> Warning: bca CI failed for component 1; using percentile CI. tsallis_profile2 #> $Long #>      q  observed      mean     lower     upper #> 1  0.0 2.0000000 2.0000000 2.0000000 2.0000000 #> 2  0.1 1.8485612 1.8434254 1.8134431 1.8714487 #> 3  0.2 1.7126236 1.7037822 1.6491094 1.7531821 #> 4  0.3 1.5903508 1.5789099 1.5090182 1.6442597 #> 5  0.4 1.4801432 1.4669538 1.3853712 1.5438392 #> 6  0.5 1.3806058 1.3663186 1.2747804 1.4506286 #> 7  0.6 1.2905205 1.2756289 1.1813325 1.3657528 #> 8  0.7 1.2088223 1.1936970 1.0963829 1.2867016 #> 9  0.8 1.1345790 1.1194948 1.0227075 1.2135858 #> 10 0.9 1.0669734 1.0521307 0.9565425 1.1459205 #> 11 1.0 1.0052882 0.9908297 0.8946554 1.0826482 #> 12 1.1 0.9488928 0.9349171 0.8424190 1.0248579 #> 13 1.2 0.8972324 0.8838039 0.7960409 0.9715417 #> 14 1.3 0.8498176 0.8369752 0.7519667 0.9214979 #> 15 1.4 0.8062167 0.7939798 0.7140237 0.8749950 #> 16 1.5 0.7660477 0.7544214 0.6767222 0.8316687 #> 17 1.6 0.7289728 0.7179513 0.6397739 0.7912826 #> 18 1.7 0.6946920 0.6842623 0.6070710 0.7532778 #> 19 1.8 0.6629392 0.6530828 0.5799747 0.7182651 #> 20 1.9 0.6334774 0.6241724 0.5550372 0.6856021 #> 21 2.0 0.6060957 0.5973179 0.5273920 0.6523920 #> 22 2.1 0.5806057 0.5723298 0.5058617 0.6241018 #> 23 2.2 0.5568392 0.5490393 0.4886703 0.5981396 #> 24 2.3 0.5346455 0.5272959 0.4710372 0.5733029 #> 25 2.4 0.5138895 0.5069650 0.4544783 0.5500446 #> 26 2.5 0.4944499 0.4879260 0.4362730 0.5282226 #> 27 2.6 0.4762176 0.4700707 0.4213441 0.5079194 #> 28 2.7 0.4590941 0.4533019 0.4068014 0.4885586 #> 29 2.8 0.4429909 0.4375320 0.3934399 0.4704869 #> 30 2.9 0.4278278 0.4226819 0.3810957 0.4536765 #> 31 3.0 0.4135320 0.4086802 0.3691134 0.4372532 #>  #> $Medium #>      q  observed      mean     lower     upper #> 1  0.0 3.0000000 2.6130000 2.0000000 3.0000000 #> 2  0.1 2.5617123 2.2914682 1.8276314 2.6742133 #> 3  0.2 2.2224763 2.0313921 1.6739360 2.3980650 #> 4  0.3 1.9552854 1.8180407 1.5390228 2.1625512 #> 5  0.4 1.7411443 1.6406757 1.4203540 1.9604716 #> 6  0.5 1.5665950 1.4913940 1.3128925 1.7860463 #> 7  0.6 1.4220235 1.3643189 1.2143051 1.6346177 #> 8  0.7 1.3004971 1.2550327 1.1274974 1.5024165 #> 9  0.8 1.1969637 1.1601760 1.0414490 1.3863793 #> 10 0.9 1.1076994 1.0771641 0.9657790 1.2762923 #> 11 1.0 1.0299241 1.0039842 0.8944185 1.1776443 #> 12 1.1 0.9615347 0.9390501 0.8279688 1.0942356 #> 13 1.2 0.9009177 0.8810970 0.7693307 1.0190388 #> 14 1.3 0.8468176 0.8291050 0.7130013 0.9521384 #> 15 1.4 0.7982434 0.7822425 0.6689238 0.8971591 #> 16 1.5 0.7544018 0.7398247 0.6342625 0.8480954 #> 17 1.6 0.7146493 0.7012822 0.6001929 0.8027499 #> 18 1.7 0.6784574 0.6661377 0.5689278 0.7623093 #> 19 1.8 0.6453869 0.6339875 0.5436654 0.7238789 #> 20 1.9 0.6150691 0.6044879 0.5159858 0.6885478 #> 21 2.0 0.5871914 0.5773445 0.4965278 0.6568768 #> 22 2.1 0.5614865 0.5523034 0.4733250 0.6271340 #> 23 2.2 0.5377246 0.5291446 0.4540934 0.5998475 #> 24 2.3 0.5157061 0.5076764 0.4374026 0.5744997 #> 25 2.4 0.4952573 0.4877314 0.4213660 0.5499692 #> 26 2.5 0.4762260 0.4691625 0.4037321 0.5270252 #> 27 2.6 0.4584783 0.4518399 0.3898226 0.5059171 #> 28 2.7 0.4418959 0.4356492 0.3768546 0.4863506 #> 29 2.8 0.4263738 0.4204885 0.3606399 0.4678838 #> 30 2.9 0.4118190 0.4062677 0.3492756 0.4506401 #> 31 3.0 0.3981481 0.3929060 0.3387426 0.4346065 #>  #> $Short #>      q  observed      mean     lower     upper #> 1  0.0 3.0000000 2.8600000 2.0000000 3.0000000 #> 2  0.1 2.7003331 2.5649175 2.5915102 2.7512819 #> 3  0.2 2.4405950 2.3117701 2.2576458 2.5275530 #> 4  0.3 2.2146076 2.0934730 1.9571954 2.3260726 #> 5  0.4 2.0172417 1.9042809 1.7171423 2.1444173 #> 6  0.5 1.8442276 1.7395142 1.5055455 1.9804424 #> 7  0.6 1.6920012 1.5953446 1.3657455 1.8322491 #> 8  0.7 1.5575795 1.4686256 1.2674411 1.6981547 #> 9  0.8 1.4384586 1.3567600 1.1966883 1.5766674 #> 10 0.9 1.3325309 1.2575943 1.1214962 1.4664635 #> 11 1.0 1.2380168 1.1693350 1.0505718 1.3663677 #> 12 1.1 1.1534096 1.0904820 0.9778393 1.2753355 #> 13 1.2 1.0774297 1.0197750 0.9071811 1.1924386 #> 14 1.3 1.0089869 0.9561510 0.8467203 1.1168504 #> 15 1.4 0.9471496 0.8987096 0.7970256 1.0478351 #> 16 1.5 0.8911198 0.8466848 0.7515947 0.9847369 #> 17 1.6 0.8402110 0.7994228 0.6950667 0.9269709 #> 18 1.7 0.7938317 0.7563632 0.6624350 0.8740149 #> 19 1.8 0.7514702 0.7170237 0.6327794 0.8254023 #> 20 1.9 0.7126828 0.6809882 0.6053282 0.7807162 #> 21 2.0 0.6770833 0.6478958 0.5798611 0.7395833 #> 22 2.1 0.6443352 0.6174332 0.5454064 0.7016694 #> 23 2.2 0.6141439 0.5893268 0.5230983 0.6666747 #> 24 2.3 0.5862509 0.5633374 0.5023573 0.6343306 #> 25 2.4 0.5604291 0.5392549 0.4830252 0.6043955 #> 26 2.5 0.5364780 0.5168947 0.4649459 0.5766526 #> 27 2.6 0.5142206 0.4960934 0.4445672 0.5509066 #> 28 2.7 0.4934997 0.4767069 0.4251102 0.5269818 #> 29 2.8 0.4741760 0.4586068 0.4101236 0.5047198 #> 30 2.9 0.4561251 0.4416792 0.3966590 0.4839778 #> 31 3.0 0.4392361 0.4258225 0.3819444 0.4644496 #>  #> attr(,\"R\") #> [1] 1000 #> attr(,\"conf\") #> [1] 0.95 #> attr(,\"parameter\") #> [1] \"tsallis\" #> attr(,\"ci.type\") #> [1] \"bca\"  tsallis_profile2_df <- dplyr::bind_rows(tsallis_profile2, .id = \"group\")  tsallis_points2_df <- tsallis_profile2_df %>%   filter(q %in% important_q) %>%   mutate(order_label = factor(q, levels = important_q,                               labels = important_labels))  ggplot(tsallis_profile2_df, aes(x = q, y = mean,                                 color = group, fill = group)) +   geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, color = NA) +   geom_line(linewidth = 1) +   geom_vline(xintercept = c(0, 1, 2), linetype = \"dashed\",              color = \"grey60\") +   geom_point(data = tsallis_points2_df, aes(shape = order_label),              size = 3, stroke = 1, inherit.aes = TRUE) +   scale_shape_manual(values = c(17, 18, 15), name = \"Important q\")  +   labs(x = \"Order (q)\", y = \"Hill number\",        color = \"Group\", fill = \"Group\") +   theme_bw()   ggplot(tsallis_profile2_df, aes(x = q, y = mean)) +   geom_ribbon(aes(ymin = lower, ymax = upper), fill = \"grey80\") +   geom_line(color = \"black\", linewidth = 1) +   facet_wrap(~ group, scales = \"free_y\") +   labs(x = \"Order (q)\", y = \"Hill number\") +   theme_bw()"},{"path":"https://aravind-j.github.io/avial/reference/diversity_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Diversity Index Functions — diversity_functions","title":"Diversity Index Functions — diversity_functions","text":"used diversity.calc.","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Diversity Index Functions — diversity_functions","text":"","code":"berger_parker(x)  berger_parker_reciprocal(x)  simpson(x)  gini_simpson(x)  simpson_max(x)  simpson_reciprocal(x)  simpson_relative(x)  simpson_evenness(x)  shannon(x, base = 2)  shannon_max(x, base = 2)  shannon_relative(x, base = 2)  shannon_ens(x, base = 2)  mcintosh_diversity(x)  mcintosh_evenness(x)  smith_wilson(x)  heip_evenness(x)  margalef_index(x)  menhinick_index(x)  brillouin_index(x)  hill_number(x, q = 1)  renyi_entropy(x, q = 1)  tsallis_entropy(x, q = 1)  hill_evenness(x, q = 1)"},{"path":"https://aravind-j.github.io/avial/reference/diversity_functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Diversity Index Functions — diversity_functions","text":"x factor vector categories (e.g., species, traits). frequency level treated abundance category. base logarithm base used computation shannon family diversity indices. Default exp(1).","code":""},{"path":"https://aravind-j.github.io/avial/reference/diversity_functions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Diversity Index Functions — diversity_functions","text":"calculated diversity index value.","code":""},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"Enhancement dnorm plot normal distribution bell curve overlay ggplot2 histograms according number records bin width.","code":""},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"","code":"dnorm_ggplot(x, mean, sd, n, bw)"},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"x vector values. mean vector means. sd vector standard deviations. n number records data points used plot histogram. bw bin width histogram.","code":""},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"density normal distribution.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"","code":"dnorm(0) * 25 == dnorm_ggplot(0, mean = 0, sd = 1, n = 5, bw = 5) #> [1] TRUE dnorm(1) * 21 == dnorm_ggplot(1, mean = 0, sd = 1, n = 7, bw = 3) #> [1] TRUE"},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"Flatten Duplicate Rows Data Frame","code":""},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"","code":"flatten.data.frame(df, cols, collapse = \":\")"},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"df data frame flattened. cols columns(s) according duplicate rows identified flattening. collapse character string separate results flattening cell. Default \":\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"data frame duplicate rows flattened.","code":""},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"","code":"data <- data.frame(team = c(rep(\"Justice League\", 17),                              rep(\"Avengers\", 13)),                    name = c(rep(\"Superman\", 5),                             rep(\"Batman\", 7),                              rep(\"Green Lantern\", 5),                              rep(\"Iron Man\", 4),                             rep(\"Captain America\", 5),                              rep(\"Thor\", 4)),                    identity = c(\"Clark Kent\", \"Christopher Kent\",                                  \"Kon-El\", \"John Henry Irons\",                                  \"Val-Zod\", \"Dick Grayson\", \"Bruce Wayne\",                                 \"Tim Drake\", \"Jean-Paul Valley\",                                  \"Terry McGinnis\", \"Jace Fox\",                                  \"Damian Wayne\",                                  \"Hal Jordan\", \" John Stewart\",                                  \" Guy Gardner\", \" Kyle Rayner\",                                  \"Jessica Cruz\", \"Tony Stark\",                                  \"Riri Williams\", \"James \\\"Rhodey\\\" Rhodes\",                                 \"Pepper Potts\",                                  \"Steve Rogers\",                                  \"Sam Wilson\", \"Bucky Barnes\",                                  \"Isaiah Bradley\", \"John Walker\",                                  \"Beta Ray Bill\", \"Jane Foster\",                                  \"Odinson\", \"Eric Masterson\"))  data_flat1 <- flatten.data.frame(df = data, cols = \"team\",                                collapse = \", \") data_flat2 <- flatten.data.frame(df = data, cols = c(\"team\", \"name\"),                                  collapse = \", \")"},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Frequency Distribution and Density Plots — freq_distribution","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"Plot Frequency Distribution Density Plots","code":""},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"","code":"freq_distribution(   data,   trait,   genotype = NULL,   hist = TRUE,   hist.col = \"gray45\",   hist.border = TRUE,   hist.border.col = \"black\",   hist.alpha = 0.8,   bw.adjust = 0.5,   density = TRUE,   density.col = \"black\",   density.fill = \"gray45\",   density.alpha = 0.1,   normal.curve = TRUE,   normal.curve.col = \"black\",   normal.curve.linetype = \"solid\",   highlight.mean = TRUE,   highlight.mean.col = \"black\",   show.counts = TRUE,   count.text.col = \"black\",   count.text.size = 3,   highlight.genotype.vline = FALSE,   highlight.genotype.pointrange = FALSE,   highlights = NULL,   highlight.col = highlight.mean.col,   standardize.xrange = TRUE )"},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"data data data frame object. data frame possess columns specifying trait (genotypes highlight.genotype.* = TRUE). trait Name column specifying trait character string. trait column type \"numeric\" quantitative traits type \"factor\" qualitative traits. genotype Name column specifying group character string. Required highlight.genotype.* =  TRUE. hist logical. TRUE, histogram plotted. Default TRUE. hist.col histograme colour. hist.border logical. TRUE, histogram border also plotted. Default TRUE. hist.border.col histogram border colour. hist.alpha Alpha transparency histogram. bw.adjust Multiplicative bin width adjustment. Default 0.5 means use half default bandwidth. density logical. TRUE, kernel density plotted. Default TRUE. density.col kernel density colour. density.fill kernel density fill colour. density.alpha Alpha transparency kernel density normal.curve logical. TRUE, normal curve plotted. Default TRUE. normal.curve.col colour normal curve. normal.curve.linetype Linetype normal curve. See aes_linetype_size_shape. highlight.mean logical. TRUE, mean value highlighted vertical line. Default TRUE. highlight.mean.col colour vertical line representing mean. show.counts TRUE, group wise counts plotted text annotation. Default TRUE. count.text.col colour count text annotation. count.text.size size count text annotation. highlight.genotype.vline logical. TRUE, mean values genotypes specified highlights plotted vertical lines. highlight.genotype.pointrange logical. TRUE, mean ± stand error values genotypes specified highlights plotted separate pointrange plot. highlights genotypes highlighted character vector. highlight.col colour(s) used highlight genotypes specified highlights plot character vector. Must valid colour values R (named colours, hexadecimal representation, index colours [1:8] default R palette() etc.). standardize.xrange logical. TRUE, original plot pointrange plot x axis ranges standardized. Default TRUE.","code":""},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"Either frequency distribution plot histogram kernel   density ggplot2 object, list containing ggplot2   pointrange plot histogram/kernel density plot.","code":""},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"","code":"library(agridat) library(ggplot2) library(patchwork) library(dplyr)  soydata <- australia.soybean soydata$gen <- as.character(soydata$gen) checks <- c(\"G01\", \"G05\")  check_cols <- c(\"#B2182B\", \"#2166AC\")  quant_traits <- c(\"yield\", \"height\", \"lodging\",                   \"size\", \"protein\", \"oil\") set.seed(123) soydata <-   soydata %>%   mutate(     across(       .cols = all_of(quant_traits),       .fns = ~factor(cut(.x, breaks = quantile(.x, na.rm = TRUE),                          include.lowest = TRUE),                      labels = sample(1:9, size = 4)),       .names = \"{.col}_score\"     )   )  qual_traits <- c(\"yield_score\", \"height_score\", \"lodging_score\",                  \"size_score\", \"protein_score\", \"oil_score\")   # Frequency distribution as histogram freq_hist1 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE,                     normal.curve = FALSE, highlight.mean = FALSE,                     show.counts = FALSE) freq_hist1   # Frequency distribution as histogram with normal curve freq_hist2 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE,                     normal.curve = TRUE, normal.curve.col = \"blue\",                     highlight.mean = FALSE,                     show.counts = FALSE) freq_hist2   # Frequency distribution as histogram with mean highlighted freq_hist3 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = TRUE, highlight.mean.col = \"red\",                     show.counts = FALSE) freq_hist3   # Frequency distribution as histogram with count value freq_hist4 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = FALSE,                     show.counts = TRUE, count.text.col = \"red\") freq_hist4   # Frequency distribution as histogram with check values # highlighted as vertical lines freq_hist5 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE, highlights = checks,                     highlight.col = check_cols) freq_hist5   # Frequency distribution as histogram with check values # highlighted as a separate pointrange plot freq_hist6 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE,                     highlight.genotype.pointrange = TRUE,                     highlights = checks,                     highlight.col = check_cols) freq_hist6[[1]] <-   freq_hist6[[1]] +   theme(axis.ticks.x = element_blank(),         axis.text.x = element_blank()) wrap_plots(freq_hist6[[1]], plot_spacer(), freq_hist6[[2]],            ncol = 1, heights = c(1, -0.5, 4))   # Frequency distribution as kernel density plot freq_dens1 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE, highlight.mean = FALSE,                     show.counts = FALSE) freq_dens1   # Frequency distribution as kernel density plot with mean highlighted freq_dens2 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = TRUE, highlight.mean.col = \"red\",                     show.counts = FALSE) freq_dens2   # Frequency distribution as kernel density plot with count value freq_dens3 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = FALSE,                     show.counts = TRUE, count.text.col = \"red\") freq_dens3   # Frequency distribution as kernel density plot with check values # highlighted as vertical lines freq_dens4 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE, highlights = checks,                     highlight.col = check_cols) freq_dens4   # Frequency distribution as kernel density plot with check values # highlighted as a separate pointrange plot freq_dens5 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE,                     highlight.genotype.pointrange = TRUE,                     highlights = checks,                     highlight.col = check_cols) freq_dens5[[1]] <-   freq_dens5[[1]] +   theme(axis.ticks.x = element_blank(),         axis.text.x = element_blank()) wrap_plots(freq_dens5[[1]], plot_spacer(), freq_dens5[[2]],            ncol = 1, heights = c(1, -0.5, 4))   # Frequency counts of categorical data as a bar plot freq_bar1 <-   freq_distribution(data = soydata, trait = \"lodging_score\",                     hist = TRUE,                     hist.col = \"lemonchiffon\") freq_bar1   # Frequency counts of categorical data as a bar plot with check values # highlighted as vertical lines freq_bar2 <-   freq_distribution(data = soydata, trait = \"lodging_score\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     genotype = \"gen\",                     highlight.genotype.vline = TRUE,                     highlights = checks,                     highlight.col = check_cols) freq_bar2   # Frequency counts of categorical data as a bar plot with check values # highlighted as a separate pointrange plot freq_bar3 <- freq_distribution(data = soydata, trait = \"lodging_score\",                                hist = TRUE,                                hist.col = \"lemonchiffon\",                                genotype = \"gen\",                                highlight.genotype.vline = TRUE,                                highlight.genotype.pointrange = TRUE,                                highlights = checks,                                highlight.col = check_cols) freq_bar3[[1]] <-   freq_bar3[[1]] +   theme(axis.ticks.x = element_blank(),         axis.text.x = element_blank()) wrap_plots(freq_bar3[[1]], plot_spacer(), freq_bar3[[2]],            ncol = 1, heights = c(1, -0.5, 4))"},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordering by keeping defined groups together — group_constrained_order","title":"Ordering by keeping defined groups together — group_constrained_order","text":"function modifies order vector specified groups elements remain together. useful keeping specific dendrogram leaves together. location group block determined earliest member group original vector. elements involved group, relative order unchanged.","code":""},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordering by keeping defined groups together — group_constrained_order","text":"","code":"group_constrained_order(x, groups, group_order = c(\"groups\", \"original\"))"},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordering by keeping defined groups together — group_constrained_order","text":"x original vector elements order. groups groups elements kept together list vectors. groups must disjoint. group_order group internal order follow one \"groups\" \"original\" order.","code":""},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ordering by keeping defined groups together — group_constrained_order","text":"vector original elements specified groups   elements kept together.","code":""},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ordering by keeping defined groups together — group_constrained_order","text":"","code":"set.seed(123)  # data mat <- matrix(rnorm(8 * 4), nrow = 8) rownames(mat) <- LETTERS[1:8]  # hierarchical clustering hc <- hclust(dist(mat)) plot(hc, main = \"Original dendrogram\")   # groups of leaves to keep together (may not be adjacent in dendrogram) groups <- list(   g1 = c(\"B\", \"F\"),   g2 = c(\"C\", \"D\", \"E\") )  # Dendrogram original order orig_order <- hc$labels[hc$order] orig_order #> [1] \"C\" \"F\" \"H\" \"B\" \"E\" \"A\" \"D\" \"G\"  # Get group constrained order new_order <-   group_constrained_order(x = orig_order,                           groups = groups,                           group_order = \"original\")  new_order #> [1] \"C\" \"E\" \"D\" \"F\" \"B\" \"H\" \"A\" \"G\"  # Reorder dendrogram labels hc2 <- hc hc2$order <- match(new_order, hc$labels)  plot(hc2, main = \"Dendrogram with grouped leaves\")"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Group-wise Bar Plots — groupwise_bar","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"Plot Group-wise Bar Plots","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"","code":"groupwise_bar(   data,   group,   trait,   bar.border = TRUE,   bar.alpha = 0.8,   by = c(\"group\", \"trait\"),   relative.freq = FALSE,   subset = c(\"facet\", \"none\"),   na.rm = TRUE,   include.overall = TRUE,   background.bar = TRUE,   background.bar.alpha = 0.25,   show.counts = TRUE,   count.text.size = 3,   position = c(\"dodge\", \"stack\"),   ncol = NULL,   nrow = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"data data data frame object. data frame possess columns specifying group trait. group Name column specifying group character string. trait Name column specifying trait character string. bar.border logical. TRUE, bar border also plotted. Default TRUE. bar.alpha Alpha transparency group-wise bar plot. factor according bars grouped. Either \"group\" \"trait\". relative.freq logical. TRUE, relative frequency proportion plotted instead counts. Default FALSE. subset method subsetting plots according argument \"group\". Either \"facet\" getting plot using faceting ggplot2 \"list\" getting list plots. na.rm logical. TRUE, NA factor levels excluded plot. Default TRUE. include.overall logical. TRUE, overall total data also plotted. Default TRUE. background.bar logical. TRUE, overall data plotted background bar plot = \"group\", include.overall = TRUE, position = \"dodge\". Default TRUE. background.bar.alpha Alpha transparency background bar plot. show.counts logical. TRUE, group wise counts plotted text annotation. Default TRUE. count.text.size size count text annotation. position Bar position adjustment. Either \"dodge\" \"stack\". ncol Number columns subset = \"facet\". nrow Number rows subset = \"facet\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"group-wise bar plot ggplot2 plot grob list   ggplot2 plot grobs.","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"","code":"library(agridat) library(ggplot2) library(patchwork)  soydata <- australia.soybean  clrs <- c(\"#B2182B\", \"#2166AC\", \"#009E53\", \"#E69F00\", \"gray25\") clrs_dark <- colorspace::darken(clrs, amount = 0.2)  soydata$lodging <- cut(soydata$lodging,                        breaks = quantile(soydata$lodging, na.rm = TRUE),                        include.lowest = TRUE) levels(soydata$lodging) <- 1:4  # soydata[soydata$loc == \"Nambour\", ]$lodging <- NA # #soydata[soydata$lodging == 1, ]$lodging # soydata[!(is.na(soydata$lodging)) & soydata$lodging == 1, ]$lodging <- NA  # set.seed(123) # ind <- sample(1:nrow(soydata), size = nrow(soydata) * 0.1, replace = FALSE) # soydata[ind, ]$lodging <- NA  # Group-wise side-by-side bar plot with counts ----  outg_group_dodge_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"none\", position = \"dodge\")  outg_group_dodge_count   outg_group_dodge_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"facet\", position = \"dodge\")  outg_group_dodge_count_facet   outg_group_dodge_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_group_dodge_count_list, nrow = 2, guides = \"collect\")   outg_group_dodge_count_list <-   lapply(seq_along(outg_group_dodge_count_list), function(i) {     outg_group_dodge_count_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_group_dodge_count_list, nrow = 2, guides = \"collect\")   # Group-wise side-by-side bar plot with relative frequencies ----  outg_group_dodge_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"none\", position = \"dodge\")  outg_group_dodge_rfreq   outg_group_dodge_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"dodge\")  outg_group_dodge_rfreq_facet   outg_group_dodge_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_group_dodge_rfreq_list, nrow = 2, guides = \"collect\")   outg_group_dodge_rfreq_list <-   lapply(seq_along(outg_group_dodge_rfreq_list), function(i) {     outg_group_dodge_rfreq_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_group_dodge_rfreq_list, nrow = 2, guides = \"collect\")   # Group-wise stacked bar plot with counts ----  outg_group_stack_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"none\", position = \"stack\")  outg_group_stack_count   outg_group_stack_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"facet\", position = \"stack\")  outg_group_stack_count_facet   outg_group_stack_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"list\", position = \"stack\")  wrap_plots(outg_group_stack_count_list, nrow = 2, guides = \"collect\")   outg_group_stack_count_list <-   lapply(seq_along(outg_group_stack_count_list), function(i) {     outg_group_stack_count_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_group_stack_count_list, nrow = 2, guides = \"collect\")   # Group-wise stacked bar plot with relative frequencies ----  outg_group_stack_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"none\", position = \"stack\")  outg_group_stack_rfreq   outg_group_stack_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"stack\")  outg_group_stack_rfreq_facet   outg_group_stack_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"list\", position = \"stack\")  wrap_plots(outg_group_stack_rfreq_list, nrow = 2, guides = \"collect\")   outg_group_stack_rfreq_list <-   lapply(seq_along(outg_group_stack_rfreq_list), function(i) {     outg_group_stack_rfreq_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_group_stack_rfreq_list, nrow = 2, guides = \"collect\")    # Trait-wise side-by-side bar plot with counts ----  outg_trait_dodge_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"none\", position = \"dodge\")  outg_trait_dodge_count   outg_trait_dodge_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"facet\", position = \"dodge\")  outg_trait_dodge_count_facet   outg_trait_dodge_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_trait_dodge_count_list, nrow = 2, guides = \"collect\")   outg_trait_dodge_count_list <-   lapply(seq_along(outg_trait_dodge_count_list), function(i) {     outg_trait_dodge_count_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_trait_dodge_count_list, nrow = 2, guides = \"collect\")   # Trait-wise side-by-side bar plot with relative frequencies ----  outg_trait_dodge_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"none\", position = \"dodge\")  outg_trait_dodge_rfreq   outg_trait_dodge_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"dodge\")  outg_trait_dodge_rfreq_facet   outg_trait_dodge_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_trait_dodge_rfreq_list, nrow = 2, guides = \"collect\")   outg_trait_dodge_rfreq_list <-   lapply(seq_along(outg_trait_dodge_rfreq_list), function(i) {     outg_trait_dodge_rfreq_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_trait_dodge_rfreq_list, nrow = 2, guides = \"collect\")   # Trait-wise stacked bar plot with counts ----  outg_trait_stack_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"none\", position = \"stack\")  outg_trait_stack_count   outg_trait_stack_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"facet\", position = \"stack\")  outg_trait_stack_count_facet   outg_trait_stack_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"list\", position = \"stack\")  wrap_plots(outg_trait_stack_count_list, nrow = 2, guides = \"collect\")   outg_trait_stack_count_list <-   lapply(seq_along(outg_trait_stack_count_list), function(i) {     outg_trait_stack_count_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_trait_stack_count_list, nrow = 2, guides = \"collect\")   # Trait-wise stacked bar plot with relative frequencies ----  outg_trait_stack_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"none\", position = \"stack\")  outg_trait_stack_rfreq   outg_trait_stack_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"stack\")  outg_trait_stack_rfreq_facet   outg_trait_stack_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"list\", position = \"stack\")  wrap_plots(outg_trait_stack_rfreq_list, nrow = 2, guides = \"collect\")   outg_trait_stack_rfreq_list <-   lapply(seq_along(outg_trait_stack_rfreq_list), function(i) {     outg_trait_stack_rfreq_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_trait_stack_rfreq_list, nrow = 2, guides = \"collect\")"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Group-wise Dumbell Plots — groupwise_dumbell","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"Plot Group-wise Dumbell Plots","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"","code":"groupwise_dumbell(   data,   group,   trait,   genotype,   subset = c(\"facet\", \"list\", \"none\"),   diff.sort = c(\"none\", \"ascending\", \"descending\"),   segment = TRUE,   segment.size = 3,   segment.colour = \"gray\",   segment.alpha = 0.5,   point.size = 3,   point.alpha = 0.8,   error.bar = TRUE,   error.bar.width = 0.1,   ncol = NULL,   nrow = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"data data data frame object. data frame possess columns specifying group trait. group Name column specifying group character string. trait Name column specifying trait character string. genotype Name column specifying genotype character string. subset method subsetting plots according argument \"group\". Either \"facet\" getting plot using faceting ggplot2 \"list\" getting list plots. diff.sort order sorting genotypes plotting. Either \"ascending\", \"descending\" \"none\". segment logical. TRUE, dumbell segment plotted. Default TRUE. segment.size size dumbell segment. segment.colour colour dumbell segment. segment.alpha Alpha transparency dumbell segment. point.size size points. point.alpha Alpha transparency points. error.bar logical. TRUE, error bars depicting standard errors plotted. Default TRUE. error.bar.width width error bars. ncol Number columns subset = \"facet\". nrow Number rows subset = \"facet\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"group-wise dumbell plot ggplot2 plot grob   list ggplot2 plot grobs.","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"","code":"library(agridat) library(ggplot2) library(patchwork)  soydata <- australia.soybean # soydata[soydata$loc == \"Nambour\", ]$lodging <- NA  checks <- c(\"G01\", \"G05\")  checkdata <- soydata[soydata$gen %in% checks, ]  clrs <- c(\"#B2182B\", \"#2166AC\", \"#009E53\", \"#E69F00\") clrs_dark <- colorspace::darken(clrs, amount = 0.2)  # Group-wise dumbell plot with error bar outg_dumbell1 <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"none\", diff.sort = \"descending\") outg_dumbell1   outg_dumbell1 +   scale_colour_manual(values = clrs)   # Group-wise dumbell plot without error bar outg_dumbell2 <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"none\", diff.sort = \"descending\",                     error.bar = FALSE) outg_dumbell2   outg_dumbell2 +   scale_colour_manual(values = clrs)   # Group-wise points with error bar as facets outg_facet <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"facet\") outg_facet   outg_facet +   scale_colour_manual(values = clrs)   # Group-wise points with error bar as list outg_list <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"list\")  wrap_plots(outg_list, nrow = 2, guides = \"collect\")   outg_list <-   lapply(seq_along(outg_list), function(i) {     outg_list[[i]] +       scale_colour_manual(values = clrs[i])   }) #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_list, nrow = 2, guides = \"collect\")"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Group-wise Histograms — groupwise_histogram","title":"Plot Group-wise Histograms — groupwise_histogram","text":"Plot Group-wise Histograms","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Group-wise Histograms — groupwise_histogram","text":"","code":"groupwise_histogram(   data,   group,   trait,   background.hist = TRUE,   background.hist.alpha = 0.25,   background.density = TRUE,   background.density.alpha = 0.1,   hist = TRUE,   hist.border = TRUE,   hist.position = c(\"identity\", \"stack\"),   hist.alpha = 0.8,   bw.adjust = 0.5,   density = TRUE,   density.alpha = 0.1,   normal.curve = TRUE,   normal.curve.linetype = \"solid\",   highlight.mean = TRUE,   show.counts = TRUE,   count.text.size = 3,   subset = c(\"facet\", \"list\", \"none\"),   ncol = NULL,   nrow = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Group-wise Histograms — groupwise_histogram","text":"data data data frame object. data frame possess columns specifying group trait. group Name column specifying group character string. trait Name column specifying trait character string. background.hist logical. TRUE, background data histogram plotted. Default TRUE. background.hist.alpha Alpha transparency background data histogram. background.density logical. TRUE, background data kernel density plotted. Default TRUE. background.density.alpha Alpha transparency background data kernel density. hist logical. TRUE, group-wise histogram plotted. Default TRUE. hist.border logical. TRUE, histogram border also plotted. Default TRUE. hist.position Histogram position adjustment. Either \"identity\" \"stack\". hist.alpha Alpha transparency group-wise histogram. bw.adjust Multiplicative bin width adjustment. Default 0.5 means use half default bandwidth. density logical. TRUE, group-wise kernel density plotted. Default TRUE. density.alpha Alpha transparency group-wise kernel density normal.curve logical. TRUE, normal curve plotted. Default TRUE. normal.curve.linetype Linetype normal curve. See aes_linetype_size_shape. highlight.mean logical. TRUE, mean value highlighted vertical line. Default TRUE. show.counts logical. TRUE, group wise counts plotted text annotation. Default TRUE. count.text.size size count text annotation. subset method subsetting plots according argument \"group\". Either \"facet\" getting plot using faceting ggplot2 \"list\" getting list plots. ncol Number columns subset = \"facet\". nrow Number rows subset = \"facet\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Group-wise Histograms — groupwise_histogram","text":"group-wise histogram ggplot2 plot grob list   ggplot2 plot grobs.","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Group-wise Histograms — groupwise_histogram","text":"","code":"library(agridat) library(ggplot2) library(patchwork)  soydata <- australia.soybean # soydata[soydata$loc == \"Nambour\", ]$lodging <- NA  clrs <- c(\"#B2182B\", \"#2166AC\", \"#009E53\", \"#E69F00\") clrs_dark <- colorspace::darken(clrs, amount = 0.2)  # Group-wise histogram ---- outg_hist <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = FALSE,                       hist.alpha = 0.5,                       density = FALSE,                       subset = \"none\") outg_hist   outg_hist +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise histogram - stacked ---- outg_hist_stack <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = FALSE,                       hist.position = \"stack\",                       density = FALSE,                       normal.curve = FALSE,                       subset = \"none\") outg_hist_stack   outg_hist_stack +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise histogram with facet ---- outg_hist_facet <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist.alpha = 0.5,                       density = FALSE,                       subset = \"facet\") outg_hist_facet   outg_hist_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise histogram as list ---- outg_hist_list <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist.alpha = 0.5,                       density = FALSE,                       subset = \"list\")  wrap_plots(outg_hist_list, nrow = 2, guides = \"collect\")   outg_hist_list <-   lapply(seq_along(outg_hist_list), function(i) {     outg_hist_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_hist_list, nrow = 2, guides = \"collect\")   # Group-wise density ---- outg_density <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = TRUE,                       hist = FALSE,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"none\") outg_density   outg_density +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density with facet ---- outg_density_facet <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = TRUE,                       hist = FALSE,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"facet\") outg_density_facet   outg_density_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density as list ---- outg_density_list <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = TRUE,                       hist = FALSE,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"list\")  wrap_plots(outg_density_list, nrow = 2, guides = \"collect\")   outg_density_list <-   lapply(seq_along(outg_density_list), function(i) {     outg_density_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_density_list, nrow = 2, guides = \"collect\")   # Group-wise density + histogram ---- outg_density_hist <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = FALSE,                       hist = TRUE,                       hist.alpha = 0.3,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"none\") outg_density_hist   outg_density_hist +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density + histogram with facet ---- outg_density_hist_facet <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist = TRUE,                       hist.alpha = 0.3,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"facet\") outg_density_hist_facet   outg_density_hist_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density + histogram as list ---- outg_density_hist_list <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist = TRUE,                       hist.alpha = 0.3,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"list\")  wrap_plots(outg_density_hist_list, nrow = 2, guides = \"collect\")   outg_density_hist_list <-   lapply(seq_along(outg_density_hist_list), function(i) {     outg_density_hist_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_density_hist_list, nrow = 2, guides = \"collect\")"},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":null,"dir":"Reference","previous_headings":"","what":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"Perform chi-square test homogeneity variance (Bartlett's test) test equality several error variances mean squared errors (Gomez Gomez 1984) . MathJax = {output: {font: \"mathjax-newcm\", fontPath: \"../../mathjaxr/doc/mathjax/font\",}};","code":""},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"","code":"mse.bartlett.test(mse, df)"},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"mse vector error variances mean squared errors environment (years /locations). df vector degrees freedom corresponding mse.","code":""},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"list chi-square value test statistic, corresponding   degrees freedom p value.","code":""},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"Gomez KA, Gomez AA (1984). Statistical Procedures Agricultural Research, 2nd ed edition. Wiley, New York. ISBN 978-0-471-87092-0.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"","code":"# Examples from Page 467-471 Gomez KA and AA Gomez (1984) Statistical # Procedures for Agricultural Research. 2nd ed. Wiley, New York, 680 p.  # Different degrees of freedom mse <- c(6.73920, 1.93496, 1.15500, 10.58450) df <- c(19, 16, 17, 19)  mse.bartlett.test(mse = c(6.73920, 1.93496, 1.15500, 10.58450),                   df = c(19, 16, 17, 19)) #> $chisq.value #> [1] 24.98754 #>  #> $df #> [1] 3 #>  #> $p.value #> [1] 1.553341e-05 #>   # Same degrees of freedom mse <- c(11.459848, 17.696970, 10.106818) df <- c(20, 20, 20)  mse.bartlett.test(mse = c(11.459848, 17.696970, 10.106818),                   df = c(20, 20, 20)) #> $chisq.value #> [1] 1.814362 #>  #> $df #> [1] 2 #>  #> $p.value #> [1] 0.4036606 #>"},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"pairwise.augmentedRCBD performs pairwise t tests adjusted means object class augmentedRCBD.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"","code":"pairwise.augmentedRCBD(   aug,   cl = NULL,   alpha = NULL,   p.adjust = c(\"none\", \"tukey\", \"sidak\") )"},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"aug object class augmentedRCBD. cl cluster object created makeCluster parallel evaluations. alpha Type error probability (Significance level). p.adjust p value adjustment method. Either \"none\", \"tukey\" \"sidak\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"data frame pairwise comparisons treatments.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"default pairwise comparison augmentedRCBD employs pairs.emmGrid function emmeans slow large number comparisons. function attempts faster parallel computing package parallel-package.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"","code":"library(augmentedRCBD) #>  #> -------------------------------------------------------------------------------- #> Welcome to augmentedRCBD version 0.1.7 #>  #>  #> # To know how to use this package type: #>   browseVignettes(package = 'augmentedRCBD') #>   for the package vignette. #>  #> # To know whats new in this version type: #>   news(package='augmentedRCBD') #>   for the NEWS file. #>  #> # To cite the methods in the package type: #>   citation(package='augmentedRCBD') #>  #> # To suppress this message use: #>   suppressPackageStartupMessages(library(augmentedRCBD)) #> --------------------------------------------------------------------------------  blk <- c(rep(1,7),rep(2,6),rep(3,7)) trt <- c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10) y1 <- c(92, 79, 87, 81, 96, 89, 82, 79, 81, 81, 91, 79, 78, 83, 77, 78, 78,         70, 75, 74) y2 <- c(258, 224, 238, 278, 347, 300, 289, 260, 220, 237, 227, 281, 311, 250,         240, 268, 287, 226, 395, 450) data <- data.frame(blk, trt, y1, y2) # Convert block and treatment to factors data$blk <- as.factor(data$blk) data$trt <- as.factor(data$trt) # Results for variable y1 out1 <- augmentedRCBD(data$blk, data$trt, data$y1, method.comp = \"lsd\",                       alpha = 0.05, group = TRUE, console = TRUE) #>  #> Augmented Design Details #> ======================== #>                                         #> Number of blocks           \"3\"          #> Number of treatments       \"12\"         #> Number of check treatments \"4\"          #> Number of test treatments  \"8\"          #> Check treatments           \"1, 2, 3, 4\" #>  #>  #> ANOVA, Treatment Adjusted #> ========================= #>                                      Df Sum Sq Mean Sq F value Pr(>F)   #> Block (ignoring Treatments)           2  360.1  180.04   6.675 0.0298 * #> Treatment (eliminating Blocks)       11  285.1   25.92   0.961 0.5499   #>   Treatment: Check                    3   52.9   17.64   0.654 0.6092   #>   Treatment: Test and Test vs. Check  8  232.2   29.02   1.076 0.4779   #> Residuals                             6  161.8   26.97                  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ANOVA, Block Adjusted #> ===================== #>                                Df Sum Sq Mean Sq F value Pr(>F) #> Treatment (ignoring Blocks)    11  575.7   52.33   1.940  0.215 #>   Treatment: Check              3   52.9   17.64   0.654  0.609 #>   Treatment: Test               7  505.9   72.27   2.679  0.125 #>   Treatment: Test vs. Check     1   16.9   16.87   0.626  0.459 #> Block (eliminating Treatments)  2   69.5   34.75   1.288  0.342 #> Residuals                       6  161.8   26.97                #>  #> Coefficient of Variation #> ======================== #> 6.372367 #>  #> Overall Adjusted Mean #> ===================== #> 81.0625 #>  #> Standard Errors #> =============== #>                                          Std. Error of Diff.  CD (5%) #> Control Treatment Means                             4.240458 10.37603 #> Two Test Treatments (Same Block)                    7.344688 17.97180 #> Two Test Treatments (Different Blocks)              8.211611 20.09309 #> A Test Treatment and a Control Treatment            6.704752 16.40594 #>  #> Treatment Means #> =============== #>  Treatment Block Means   SE r   Min   Max Adjusted Means #>          1       84.67 3.84 3 79.00 92.00          84.67 #>         10     3 74.00 <NA> 1 74.00 74.00          77.25 #>         11     1 89.00 <NA> 1 89.00 89.00          86.50 #>         12     1 82.00 <NA> 1 82.00 82.00          79.50 #>          2       79.00 1.15 3 77.00 81.00          79.00 #>          3       82.00 2.65 3 78.00 87.00          82.00 #>          4       83.33 3.93 3 78.00 91.00          83.33 #>          5     2 79.00 <NA> 1 79.00 79.00          78.25 #>          6     3 75.00 <NA> 1 75.00 75.00          78.25 #>          7     1 96.00 <NA> 1 96.00 96.00          93.50 #>          8     3 70.00 <NA> 1 70.00 70.00          73.25 #>          9     2 78.00 <NA> 1 78.00 78.00          77.25 #>  #>  #> Comparisons #> =========== #>  #> Method : lsd #>  #>                   contrast estimate   SE df t.ratio p.value sig #>    treatment1 - treatment2     5.67 4.24  6   1.336   0.230     #>    treatment1 - treatment3     2.67 4.24  6   0.629   0.553     #>    treatment1 - treatment4     1.33 4.24  6   0.314   0.764     #>    treatment1 - treatment5     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment6     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment7    -8.83 6.36  6  -1.389   0.214     #>    treatment1 - treatment8    11.42 6.36  6   1.795   0.123     #>    treatment1 - treatment9     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment10     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment11    -1.83 6.36  6  -0.288   0.783     #>   treatment1 - treatment12     5.17 6.36  6   0.812   0.448     #>    treatment2 - treatment3    -3.00 4.24  6  -0.707   0.506     #>    treatment2 - treatment4    -4.33 4.24  6  -1.022   0.346     #>    treatment2 - treatment5     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment6     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment7   -14.50 6.36  6  -2.280   0.063     #>    treatment2 - treatment8     5.75 6.36  6   0.904   0.401     #>    treatment2 - treatment9     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment10     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment11    -7.50 6.36  6  -1.179   0.283     #>   treatment2 - treatment12    -0.50 6.36  6  -0.079   0.940     #>    treatment3 - treatment4    -1.33 4.24  6  -0.314   0.764     #>    treatment3 - treatment5     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment6     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment7   -11.50 6.36  6  -1.808   0.121     #>    treatment3 - treatment8     8.75 6.36  6   1.376   0.218     #>    treatment3 - treatment9     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment10     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment11    -4.50 6.36  6  -0.707   0.506     #>   treatment3 - treatment12     2.50 6.36  6   0.393   0.708     #>    treatment4 - treatment5     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment6     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment7   -10.17 6.36  6  -1.598   0.161     #>    treatment4 - treatment8    10.08 6.36  6   1.585   0.164     #>    treatment4 - treatment9     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment10     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment11    -3.17 6.36  6  -0.498   0.636     #>   treatment4 - treatment12     3.83 6.36  6   0.603   0.569     #>    treatment5 - treatment6     0.00 8.21  6   0.000   1.000     #>    treatment5 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment5 - treatment8     5.00 8.21  6   0.609   0.565     #>    treatment5 - treatment9     1.00 7.34  6   0.136   0.896     #>   treatment5 - treatment10     1.00 8.21  6   0.122   0.907     #>   treatment5 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment5 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment6 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment6 - treatment8     5.00 7.34  6   0.681   0.521     #>    treatment6 - treatment9     1.00 8.21  6   0.122   0.907     #>   treatment6 - treatment10     1.00 7.34  6   0.136   0.896     #>   treatment6 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment6 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment7 - treatment8    20.25 8.21  6   2.466   0.049   * #>    treatment7 - treatment9    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment10    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment11     7.00 7.34  6   0.953   0.377     #>   treatment7 - treatment12    14.00 7.34  6   1.906   0.105     #>    treatment8 - treatment9    -4.00 8.21  6  -0.487   0.643     #>   treatment8 - treatment10    -4.00 7.34  6  -0.545   0.606     #>   treatment8 - treatment11   -13.25 8.21  6  -1.614   0.158     #>   treatment8 - treatment12    -6.25 8.21  6  -0.761   0.475     #>   treatment9 - treatment10     0.00 8.21  6   0.000   1.000     #>   treatment9 - treatment11    -9.25 8.21  6  -1.126   0.303     #>   treatment9 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment10 - treatment11    -9.25 8.21  6  -1.126   0.303     #>  treatment10 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment11 - treatment12     7.00 7.34  6   0.953   0.377     #>  #> Treatment Groups #> ================ #>  #> Method : lsd #>  #>  Treatment Adjusted Means   SE df lower.CL upper.CL Group #>          8          73.25 5.61  6    59.52    86.98    1  #>          9          77.25 5.61  6    63.52    90.98    12 #>         10          77.25 5.61  6    63.52    90.98    12 #>          5          78.25 5.61  6    64.52    91.98    12 #>          6          78.25 5.61  6    64.52    91.98    12 #>          2          79.00 3.00  6    71.66    86.34    12 #>         12          79.50 5.61  6    65.77    93.23    12 #>          3          82.00 3.00  6    74.66    89.34    12 #>          4          83.33 3.00  6    76.00    90.67    12 #>          1          84.67 3.00  6    77.33    92.00    12 #>         11          86.50 5.61  6    72.77   100.23    12 #>          7          93.50 5.61  6    79.77   107.23     2 # Results for variable y2 out2 <- augmentedRCBD(data$blk, data$trt, data$y1, method.comp = \"lsd\",                       alpha = 0.05, group = TRUE, console = TRUE) #>  #> Augmented Design Details #> ======================== #>                                         #> Number of blocks           \"3\"          #> Number of treatments       \"12\"         #> Number of check treatments \"4\"          #> Number of test treatments  \"8\"          #> Check treatments           \"1, 2, 3, 4\" #>  #>  #> ANOVA, Treatment Adjusted #> ========================= #>                                      Df Sum Sq Mean Sq F value Pr(>F)   #> Block (ignoring Treatments)           2  360.1  180.04   6.675 0.0298 * #> Treatment (eliminating Blocks)       11  285.1   25.92   0.961 0.5499   #>   Treatment: Check                    3   52.9   17.64   0.654 0.6092   #>   Treatment: Test and Test vs. Check  8  232.2   29.02   1.076 0.4779   #> Residuals                             6  161.8   26.97                  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ANOVA, Block Adjusted #> ===================== #>                                Df Sum Sq Mean Sq F value Pr(>F) #> Treatment (ignoring Blocks)    11  575.7   52.33   1.940  0.215 #>   Treatment: Check              3   52.9   17.64   0.654  0.609 #>   Treatment: Test               7  505.9   72.27   2.679  0.125 #>   Treatment: Test vs. Check     1   16.9   16.87   0.626  0.459 #> Block (eliminating Treatments)  2   69.5   34.75   1.288  0.342 #> Residuals                       6  161.8   26.97                #>  #> Coefficient of Variation #> ======================== #> 6.372367 #>  #> Overall Adjusted Mean #> ===================== #> 81.0625 #>  #> Standard Errors #> =============== #>                                          Std. Error of Diff.  CD (5%) #> Control Treatment Means                             4.240458 10.37603 #> Two Test Treatments (Same Block)                    7.344688 17.97180 #> Two Test Treatments (Different Blocks)              8.211611 20.09309 #> A Test Treatment and a Control Treatment            6.704752 16.40594 #>  #> Treatment Means #> =============== #>  Treatment Block Means   SE r   Min   Max Adjusted Means #>          1       84.67 3.84 3 79.00 92.00          84.67 #>         10     3 74.00 <NA> 1 74.00 74.00          77.25 #>         11     1 89.00 <NA> 1 89.00 89.00          86.50 #>         12     1 82.00 <NA> 1 82.00 82.00          79.50 #>          2       79.00 1.15 3 77.00 81.00          79.00 #>          3       82.00 2.65 3 78.00 87.00          82.00 #>          4       83.33 3.93 3 78.00 91.00          83.33 #>          5     2 79.00 <NA> 1 79.00 79.00          78.25 #>          6     3 75.00 <NA> 1 75.00 75.00          78.25 #>          7     1 96.00 <NA> 1 96.00 96.00          93.50 #>          8     3 70.00 <NA> 1 70.00 70.00          73.25 #>          9     2 78.00 <NA> 1 78.00 78.00          77.25 #>  #>  #> Comparisons #> =========== #>  #> Method : lsd #>  #>                   contrast estimate   SE df t.ratio p.value sig #>    treatment1 - treatment2     5.67 4.24  6   1.336   0.230     #>    treatment1 - treatment3     2.67 4.24  6   0.629   0.553     #>    treatment1 - treatment4     1.33 4.24  6   0.314   0.764     #>    treatment1 - treatment5     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment6     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment7    -8.83 6.36  6  -1.389   0.214     #>    treatment1 - treatment8    11.42 6.36  6   1.795   0.123     #>    treatment1 - treatment9     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment10     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment11    -1.83 6.36  6  -0.288   0.783     #>   treatment1 - treatment12     5.17 6.36  6   0.812   0.448     #>    treatment2 - treatment3    -3.00 4.24  6  -0.707   0.506     #>    treatment2 - treatment4    -4.33 4.24  6  -1.022   0.346     #>    treatment2 - treatment5     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment6     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment7   -14.50 6.36  6  -2.280   0.063     #>    treatment2 - treatment8     5.75 6.36  6   0.904   0.401     #>    treatment2 - treatment9     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment10     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment11    -7.50 6.36  6  -1.179   0.283     #>   treatment2 - treatment12    -0.50 6.36  6  -0.079   0.940     #>    treatment3 - treatment4    -1.33 4.24  6  -0.314   0.764     #>    treatment3 - treatment5     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment6     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment7   -11.50 6.36  6  -1.808   0.121     #>    treatment3 - treatment8     8.75 6.36  6   1.376   0.218     #>    treatment3 - treatment9     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment10     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment11    -4.50 6.36  6  -0.707   0.506     #>   treatment3 - treatment12     2.50 6.36  6   0.393   0.708     #>    treatment4 - treatment5     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment6     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment7   -10.17 6.36  6  -1.598   0.161     #>    treatment4 - treatment8    10.08 6.36  6   1.585   0.164     #>    treatment4 - treatment9     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment10     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment11    -3.17 6.36  6  -0.498   0.636     #>   treatment4 - treatment12     3.83 6.36  6   0.603   0.569     #>    treatment5 - treatment6     0.00 8.21  6   0.000   1.000     #>    treatment5 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment5 - treatment8     5.00 8.21  6   0.609   0.565     #>    treatment5 - treatment9     1.00 7.34  6   0.136   0.896     #>   treatment5 - treatment10     1.00 8.21  6   0.122   0.907     #>   treatment5 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment5 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment6 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment6 - treatment8     5.00 7.34  6   0.681   0.521     #>    treatment6 - treatment9     1.00 8.21  6   0.122   0.907     #>   treatment6 - treatment10     1.00 7.34  6   0.136   0.896     #>   treatment6 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment6 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment7 - treatment8    20.25 8.21  6   2.466   0.049   * #>    treatment7 - treatment9    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment10    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment11     7.00 7.34  6   0.953   0.377     #>   treatment7 - treatment12    14.00 7.34  6   1.906   0.105     #>    treatment8 - treatment9    -4.00 8.21  6  -0.487   0.643     #>   treatment8 - treatment10    -4.00 7.34  6  -0.545   0.606     #>   treatment8 - treatment11   -13.25 8.21  6  -1.614   0.158     #>   treatment8 - treatment12    -6.25 8.21  6  -0.761   0.475     #>   treatment9 - treatment10     0.00 8.21  6   0.000   1.000     #>   treatment9 - treatment11    -9.25 8.21  6  -1.126   0.303     #>   treatment9 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment10 - treatment11    -9.25 8.21  6  -1.126   0.303     #>  treatment10 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment11 - treatment12     7.00 7.34  6   0.953   0.377     #>  #> Treatment Groups #> ================ #>  #> Method : lsd #>  #>  Treatment Adjusted Means   SE df lower.CL upper.CL Group #>          8          73.25 5.61  6    59.52    86.98    1  #>          9          77.25 5.61  6    63.52    90.98    12 #>         10          77.25 5.61  6    63.52    90.98    12 #>          5          78.25 5.61  6    64.52    91.98    12 #>          6          78.25 5.61  6    64.52    91.98    12 #>          2          79.00 3.00  6    71.66    86.34    12 #>         12          79.50 5.61  6    65.77    93.23    12 #>          3          82.00 3.00  6    74.66    89.34    12 #>          4          83.33 3.00  6    76.00    90.67    12 #>          1          84.67 3.00  6    77.33    92.00    12 #>         11          86.50 5.61  6    72.77   100.23    12 #>          7          93.50 5.61  6    79.77   107.23     2  # Make cluster ---- library(parallel) # Check if running under R CMD check and adjust cores accordingly if (nzchar(Sys.getenv(\"_R_CHECK_LIMIT_CORES_\"))) {   ncores <- 2 } else {   ncores <- max(2L, parallel::detectCores() - 4) }  cl <- makeCluster(getOption(\"cl.cores\", ncores))  # Pairwise t test without p value adjustment ---- pout1 <- pairwise.augmentedRCBD(out1, cl = cl,                                 p.adjust = \"none\") pout1 #>       contrast   estimate       SE df     t.ratio    p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.68898284     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 0.79368531     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.52980915     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.77011499     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.44738825     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.23616021     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.62449310     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.62449310     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 0.23575127     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 1.86049532     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.68898284     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 0.30300033     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 0.79327167     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 0.80281330     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 0.50524328     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 0.39921196     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 0.90705045     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 0.89615381     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.09516823     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.39434533     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.00000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.62264457     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.69391854     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.47290592     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.34659999     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.64616381     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.64616381     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 0.37735543     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 1.84225214     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.69699967     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.05702215     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 0.72206273     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 0.58826470     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.11599954     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.11599954     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.10526990     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.52457343     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.20672833     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 0.50581089     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 0.34624985     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.08541796     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.08541796     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.07380701     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.57596055     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.19718670     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 0.76383979     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.40380253     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.40380253     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.13713052     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.76031005     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.49475672     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.52293671     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.52293671     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.18022066     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 1.81669827     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.60078804     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.00000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.43507937     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.10384619     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.47858869     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.09294955     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 1.95128006     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 1.90483177     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 0.64346045     stopCluster(cl)  cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout2 <- pairwise.augmentedRCBD(out2, cl = cl,                                 p.adjust = \"none\") pout2 #>       contrast   estimate       SE df     t.ratio    p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.68898284     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 0.79368531     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.52980915     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.77011499     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.44738825     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.23616021     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.62449310     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.62449310     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 0.23575127     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 1.86049532     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.68898284     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 0.30300033     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 0.79327167     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 0.80281330     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 0.50524328     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 0.39921196     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 0.90705045     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 0.89615381     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.09516823     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.39434533     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.00000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.62264457     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.69391854     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.47290592     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.34659999     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.64616381     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.64616381     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 0.37735543     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 1.84225214     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.69699967     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.05702215     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 0.72206273     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 0.58826470     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.11599954     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.11599954     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.10526990     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.52457343     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.20672833     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 0.50581089     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 0.34624985     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.08541796     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.08541796     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.07380701     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.57596055     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.19718670     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 0.76383979     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.40380253     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.40380253     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.13713052     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.76031005     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.49475672     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.52293671     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.52293671     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.18022066     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 1.81669827     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.60078804     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.00000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.43507937     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.10384619     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.47858869     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.09294955     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 1.95128006     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 1.90483177     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 0.64346045     stopCluster(cl)  # Pairwise t test with tukey adjustment ---- cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout1_tukey <- pairwise.augmentedRCBD(out1, cl = cl,                                       p.adjust = \"tukey\") pout1_tukey #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 0.9985579     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 0.9999418     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 0.9937163     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 0.9999919     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 0.9995791     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 0.9995791     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9682124     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 0.9985579     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 1.0000000     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 0.9999981     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 0.9995942     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 0.9984198     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 0.9999846     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 0.9999996     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 0.9993581     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 0.9993581     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9771900     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 0.9983271     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 1.0000000     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 0.9999482     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 1.0000000     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 0.9998426     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 0.9999975     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 0.9999975     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 1.0000000     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 0.9947262     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 0.9999740     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 0.9999501     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 0.9999501     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 1.0000000     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9855836     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 0.9997378     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 0.9999941     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 0.9999823     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.8043427     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9265988     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)  cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout2_tukey <- pairwise.augmentedRCBD(out2, cl = cl,                                       p.adjust = \"tukey\") pout2_tukey #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 0.9985579     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 0.9999418     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 0.9937163     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 0.9999919     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 0.9995791     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 0.9995791     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9682124     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 0.9985579     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 1.0000000     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 0.9999981     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 0.9995942     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 0.9984198     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 0.9999846     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 0.9999996     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 0.9993581     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 0.9993581     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9771900     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 0.9983271     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 1.0000000     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 0.9999482     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 1.0000000     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 0.9998426     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 0.9999975     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 0.9999975     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 1.0000000     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 0.9947262     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 0.9999740     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 0.9999501     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 0.9999501     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 1.0000000     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9855836     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 0.9997378     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 0.9999941     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 0.9999823     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.8043427     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9265988     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)  # Pairwise t test with sidak p value adjustment ---- cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout1_sidak <- pairwise.augmentedRCBD(out1, cl = cl,                                       p.adjust = \"sidak\") pout1_sidak #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.0000000     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.0000000     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.0000000     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.0000000     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.0000000     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.0000000     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9999506     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.0000000     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.9986402     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.0000000     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.0000000     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.0000000     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.0000000     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.0000000     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.0000000     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.0000000     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9999880     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.0000000     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.9993519     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.0000000     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.9936569     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.0000000     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.0000000     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.0000000     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.9999408     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.0000000     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.0000000     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.0000000     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.0000000     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.9999980     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9999984     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.0000000     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.0000000     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.0000000     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.9629870     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9986402     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)  cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout2_sidak <- pairwise.augmentedRCBD(out2, cl = cl,                                       p.adjust = \"sidak\") pout2_sidak #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.0000000     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.0000000     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.0000000     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.0000000     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.0000000     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.0000000     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9999506     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.0000000     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.9986402     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.0000000     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.0000000     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.0000000     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.0000000     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.0000000     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.0000000     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.0000000     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9999880     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.0000000     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.9993519     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.0000000     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.9936569     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.0000000     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.0000000     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.0000000     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.9999408     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.0000000     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.0000000     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.0000000     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.0000000     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.9999980     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9999984     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.0000000     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.0000000     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.0000000     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.9629870     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9986402     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)"},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Output Files from MStrat — parse_mstrat_out","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"Prepare raw output files generated MStrat (Schoen Brown 1993; Gouesnard et al. 2001; Gouesnard et al. 2002) .","code":""},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"","code":"parse_mstrat_out(   data.file,   genotype = NULL,   variable.file,   kernel.file,   redundance.output = NULL,   core.output = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"data.file path .dat input file used generating output files MStrat. genotype Name column/variable genotype names character string. variable.file path .var input file used generating output files MStrat. kernel.file path .ker input file used generating output files MStrat. redundance.output path redundance output file generated MStrat. core.output path core output file generated MStrat.","code":""},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"list following components: MStrat Core   Output raw output core sets constructed MStrat. MStrat Core Optimised optimised core set output   MStrat. MStrat Redundance Output raw output   Redundance estimation MStrat MStrat Redundance   Plot list plots Redundance estimation.","code":""},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2001). “MSTRAT: algorithm building germ plasm core collections maximizing allelic phenotypic richness.” Journal Heredity, 92(1), 93–94. Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2002). “MStrat Documentation 1.1.” Evolutionary genomics population management (GE\\(^{\\textrm{2}}\\)pop), Institut Amelioration Genetique et Adaptation des Plantes mediterraneennes et tropicales (agAp Institute), CIRAD, Montpellier, France. Schoen DJ, Brown AH (1993). “Conservation allelic richness wild crop relatives aided assessment genetic markers.” Proceedings National Academy Sciences, 90(22), 10623–10627.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"","code":"if (FALSE) { # interactive()  parse_mstrat_out(data.file = \"MStrat_input_data.dat\",                  genotype = \"Accession\",                  variable.file = \"MStrat_input_variable.var\",                  kernel.file = \"MStrat_input_kernel.ker\",                  redundance.output = \"MStrat - Redundance.out\",                  core.output = \"MStrat - Core.out\") }"},{"path":"https://aravind-j.github.io/avial/reference/permutation_tests.html","id":null,"dir":"Reference","previous_headings":"","what":"Permutation Tests — permutation_tests","title":"Permutation Tests — permutation_tests","text":"functions perform permutation-based hypothesis tests compare groups respect summary statistic (e.g., mean, diversity index). perm.test.global performs global test across groups simultaneously, using weighted sum squares group summary indices test statistic perm.test.pairwise performs pairwise tests combinations groups, comparing absolute difference summary statistics optionally adjusting p-values multiple comparisons.","code":""},{"path":"https://aravind-j.github.io/avial/reference/permutation_tests.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Permutation Tests — permutation_tests","text":"","code":"perm.test.global(   x,   group,   fun,   R = 1000,   parallel = c(\"no\", \"multicore\", \"snow\"),   ncpus = 1L,   cl = NULL,   ... )  perm.test.pairwise(   x,   group,   fun,   R = 1000,   p.adjust.method = c(\"bonferroni\", \"holm\"),   parallel = c(\"no\", \"multicore\", \"snow\"),   ncpus = 1L,   cl = NULL,   ... )"},{"path":"https://aravind-j.github.io/avial/reference/permutation_tests.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Permutation Tests — permutation_tests","text":"x numeric factor vector observations. group factor vector indicating group observation. Must length x. fun function summarize values within group. R Integer specifying number permutations. Default 1000. parallel type parallel operation used ().  missing,     default taken option \"boot.parallel\" (    set, \"\"). ncpus integer: number processes used parallel operation:     typically one chose number available CPUs. cl optional parallel snow cluster use     parallel = \"snow\".  supplied, cluster     local machine created duration boot call. ... Additional arguments passed fun. p.adjust.method (perm.test.pairwise ) Method adjusting p-values multiple comparisons. Options include \"bonferroni\" \"holm\". Default \"bonferroni\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/permutation_tests.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Permutation Tests — permutation_tests","text":"perm.test.global list following   elements. test_stat test statistic value. observed_values observed values group. p_value p value.  perm.test.pairwise data   frame following columns. Comparison comparison. p.value p value. adj.p.value adjusted p value.","code":""},{"path":"https://aravind-j.github.io/avial/reference/permutation_tests.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Permutation Tests — permutation_tests","text":"","code":"library(EvaluateCore)  pdata <- cassava_CC  qual <- c(\"CUAL\", \"LNGS\", \"PTLC\", \"DSTA\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Conver '#t qualitative data columns to factor pdata[, qual] <- lapply(pdata[, qual], as.factor)  str(pdata) #> 'data.frame':\t168 obs. of  26 variables: #>  $ CUAL  : Factor w/ 4 levels \"Dark green\",\"Green purple\",..: 3 1 2 2 2 2 4 2 2 1 ... #>  $ LNGS  : Factor w/ 3 levels \"Long\",\"Medium\",..: 3 1 2 2 2 2 2 1 1 1 ... #>  $ PTLC  : Factor w/ 5 levels \"Dark green\",\"Green purple\",..: 3 4 4 4 4 5 4 2 2 5 ... #>  $ DSTA  : Factor w/ 5 levels \"Absent\",\"Central part\",..: 1 5 5 5 5 5 5 4 2 5 ... #>  $ LFRT  : Factor w/ 4 levels \"25-50% leaf retention\",..: 1 1 1 1 3 2 2 2 2 2 ... #>  $ LBTEF : Factor w/ 6 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 5 4 4 3 2 ... #>  $ CBTR  : Factor w/ 3 levels \"Cream\",\"White\",..: 2 2 2 2 1 2 1 1 1 1 ... #>  $ NMLB  : Factor w/ 9 levels \"0\",\"1\",\"2\",\"3\",..: 3 1 2 1 4 4 4 3 3 4 ... #>  $ ANGB  : Factor w/ 4 levels \"150-300\",\"450-600\",..: 1 4 1 4 2 2 2 1 2 2 ... #>  $ CUAL9M: Factor w/ 5 levels \"Dark green\",\"Green\",..: 1 1 3 5 3 3 5 5 5 4 ... #>  $ LVC9M : Factor w/ 5 levels \"Dark green\",\"Green\",..: 4 3 3 3 3 1 3 1 4 3 ... #>  $ TNPR9M: Factor w/ 5 levels \"1\",\"2\",\"3\",\"4\",..: 5 5 4 2 5 4 2 5 5 5 ... #>  $ PL9M  : Factor w/ 2 levels \"Long (25-30cm)\",..: 2 2 1 1 1 1 1 1 2 2 ... #>  $ STRP  : Factor w/ 4 levels \"Absent\",\"Intermediate\",..: 2 3 1 1 1 1 4 1 1 4 ... #>  $ STRC  : Factor w/ 2 levels \"Absent\",\"Present\": 2 2 1 2 1 1 2 1 1 2 ... #>  $ PSTR  : Factor w/ 2 levels \"Irregular\",\"Tending toward horizontal\": 1 2 2 2 1 2 2 2 1 2 ... #>  $ NMSR  : num  6 2 6 2 20 13 4 14 10 5 ... #>  $ TTRN  : num  3 0.5 3 2 5 ... #>  $ TFWSR : num  1.4 2.6 1.2 1.6 5 7 4.2 2.8 2.8 4 ... #>  $ TTRW  : num  0.7 0.65 0.6 1.6 1.25 ... #>  $ TFWSS : num  1 2.8 2.8 2.4 16 12 9 4.4 6.2 5 ... #>  $ TTSW  : num  0.5 0.7 1.4 2.4 4 ... #>  $ TTPW  : num  2.4 5.4 4 4 21 19 13.2 7.2 9 9 ... #>  $ AVPW  : num  1.2 1.35 2 4 5.25 4.75 3.3 2.4 1.8 2.25 ... #>  $ ARSR  : num  2 0 2 0 3 0 0 6 0 0 ... #>  $ SRDM  : num  42 39.8 29.7 43 37.9 37 38.9 36.9 41 37.9 ...  # Global tests ----  perm.test.global(x = pdata$NMSR, group = pdata$CUAL, fun = mean,                  R = 100) #> $test_stat #> [1] 226.7998 #>  #> $observed_values #>   Dark green Green purple  Light green       Purple  #>     9.612903    11.157303     6.333333    11.928571  #>  #> $p_value #> [1] 0.3564356 #>   perm.test.global(x = pdata$LNGS, group = pdata$CUAL, fun = shannon,                  R = 100) #> $test_stat #> [1] 4.928841 #>  #> $observed_values #>   Dark green Green purple  Light green       Purple  #>    1.5143563    1.4143497    0.6500224    1.2130604  #>  #> $p_value #> [1] 0.05940594 #>   perm.test.global(x = pdata$PTLC, group = pdata$CUAL, fun = simpson,                  R = 100) #> $test_stat #> [1] 0.8980326 #>  #> $observed_values #>   Dark green Green purple  Light green       Purple  #>    0.3048907    0.4412322    0.3888889    0.5272109  #>  #> $p_value #> [1] 0.2475248 #>   # Pairwise tests ----  perm.test.pairwise(x = pdata$NMSR, group = pdata$CUAL, fun = mean,                    R = 100) #>                    Comparison   p.value adj.p.value #> 1  Dark green vs Green purple 0.4059406   1.0000000 #> 2   Dark green vs Light green 0.2178218   1.0000000 #> 3        Dark green vs Purple 0.1683168   1.0000000 #> 4 Green purple vs Light green 0.1386139   0.8316832 #> 5      Green purple vs Purple 0.6237624   1.0000000 #> 6       Light green vs Purple 0.1386139   0.8316832  perm.test.pairwise(x = pdata$LNGS, group = pdata$CUAL, fun = shannon,                    R = 100) #>                    Comparison    p.value adj.p.value #> 1  Dark green vs Green purple 0.40594059   1.0000000 #> 2   Dark green vs Light green 0.01980198   0.1188119 #> 3        Dark green vs Purple 0.06930693   0.4158416 #> 4 Green purple vs Light green 0.08910891   0.5346535 #> 5      Green purple vs Purple 0.14851485   0.8910891 #> 6       Light green vs Purple 0.14851485   0.8910891  perm.test.pairwise(x = pdata$PTLC, group = pdata$CUAL, fun = simpson,                    R = 100) #>                    Comparison    p.value adj.p.value #> 1  Dark green vs Green purple 0.11881188   0.7128713 #> 2   Dark green vs Light green 0.55445545   1.0000000 #> 3        Dark green vs Purple 0.03960396   0.2376238 #> 4 Green purple vs Light green 0.81188119   1.0000000 #> 5      Green purple vs Purple 0.36633663   1.0000000 #> 6       Light green vs Purple 0.37623762   1.0000000"},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling `rhs(lhs)`.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Input Files for MStrat — prep_mstrat_input","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"Prepare input files MStrat, software building germplasm core collections maximizing allelic phenotypic richness (Schoen Brown 1993; Gouesnard et al. 2001; Gouesnard et al. 2002) .","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"","code":"prep_mstrat_input(   data,   genotype,   qualitative,   quantitative,   active,   target,   center = TRUE,   scale = TRUE,   weights.qualitative = NULL,   weights.quantitative = NULL,   nclass.quantitative = NULL,   always.selected = NULL,   file.name = \"MStrat_input\",   folder.path = getwd() )"},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"data data data frame object. data frame possess columns genotype names multiple quantitative /qualitative trait/variable data. genotype Name column genotype names character string. qualitative Name columns qualitative traits character vector. quantitative Name columns quantitative traits character vector. active Name traits/variables declared active. target Name traits/variables declared target. center either logical value numeric-alike vector length     equal number columns x,     ‘numeric-alike’ means .numeric(.)     applied successfully .numeric(.) true. scale either logical value numeric-alike vector length     equal number columns x. weights.qualitative vector weight applied qualitative traits. NULL numeric vector length number qualitative traits. NULL, default weight 1 given. weights.quantitative vector weight applied quantitative traits. NULL numeric vector length number quantitative traits. NULL, default weight 1 given. nclass.quantitative number classes quantitative trait data divided . NULL integer vector length number quantitative traits. NULL, default 5 applied. MStat limits maximum number classes 1000. always.selected character vector names individuals genotype always selected core collection. maximum length accepted MStrat 500. file.name character string name file data saved. folder.path path folder input files saved.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"NA values considered missing value converted   9999 code MStrat.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2001). “MSTRAT: algorithm building germ plasm core collections maximizing allelic phenotypic richness.” Journal Heredity, 92(1), 93–94. Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2002). “MStrat Documentation 1.1.” Evolutionary genomics population management (GE\\(^{\\textrm{2}}\\)pop), Institut Amelioration Genetique et Adaptation des Plantes mediterraneennes et tropicales (agAp Institute), CIRAD, Montpellier, France. Schoen DJ, Brown AH (1993). “Conservation allelic richness wild crop relatives aided assessment genetic markers.” Proceedings National Academy Sciences, 90(22), 10623–10627.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"","code":"library(EvaluateCore)  data(cassava_EC) data <- cassava_EC  quant <- c(\"NMSR\", \"TTRN\", \"TFWSR\", \"TTSW\", \"TTPW\", \"AVPW\",            \"ARSR\", \"SRDM\") qual <- c(\"CUAL\", \"LNGS\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Prepare genotype column data$Accession <- rownames(data) rownames(data) <- NULL data$Accession <- as.factor(data$Accession)  # Convert qualitative data as factors data[, qual] <- lapply(data[, qual],                        function(x) factor(as.factor(x)))  active = c(\"LNGS\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",            \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\",            \"TTRN\", \"TFWSR\", \"TTSW\", \"TTPW\", \"AVPW\") target = c(\"NMSR\", \"TTRN\", \"ARSR\", \"SRDM\",            \"CUAL\", \"LNGS\", \"TNPR9M\",            \"PL9M\", \"STRP\", \"STRC\",            \"PSTR\")  sel <- c(\"TMe-2906\", \"TMe-3412\", \"TMe-1374\", \"TMe-768\", \"TMe-14\",          \"TMe-3284\", \"TMe-937\", \"TMe-1859\", \"TMe-3265\", \"TMe-1739\",          \"TMe-972\", \"TMe-769\", \"TMe-3243\", \"TMe-3719\", \"TMe-1095\",          \"TMe-893\", \"TMe-1262\", \"TMe-2083\", \"TMe-376\", \"TMe-3633\",          \"TMe-1738\", \"TMe-2428\", \"TMe-259\", \"TMe-3457\", \"TMe-1406\",          \"TMe-977\", \"TMe-3006\", \"TMe-925\", \"TMe-3671\", \"TMe-2779\",          \"TMe-1293\", \"TMe-268\", \"TMe-266\", \"TMe-3562\", \"TMe-801\")  prep_mstrat_input(data = data, genotype = \"Accession\",                   qualitative = qual, quantitative = quant,                   active = active, target = target,                   center = TRUE, scale = TRUE,                   weights.qualitative = NULL,                   weights.quantitative = NULL,                   nclass.quantitative = NULL, always.selected = sel,                   file.name = \"MStrat_input\",                   folder.path = tempdir()) #> The following MStrat input files created at /var/folders/kg/7q73ww8s3llgyl61c9z_j5g40000gn/T//RtmpkQAuRG: #> MStrat_input_data.dat #> MStrat_input_variable.var #> MStrat_input_kernel.ker"},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Input Files for PowerCore — prep_powercore_input","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"Prepare input files PowerCore, program applying advanced M strategy heuristic search establishing core sets (Kim et al. 2007; Kim et al. 2007) .","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"","code":"prep_powercore_input(   data,   genotype,   qualitative,   quantitative,   center = TRUE,   scale = TRUE,   always.selected = NULL,   file.name = \"PowerCore_input\",   folder.path = getwd() )"},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"data data data frame object. data frame possess columns genotype names multiple quantitative /qualitative trait/variable data. genotype Name column genotype names character string. qualitative Name columns qualitative traits character vector. quantitative Name columns quantitative traits character vector. center either logical value numeric-alike vector length     equal number columns x,     ‘numeric-alike’ means .numeric(.)     applied successfully .numeric(.) true. scale either logical value numeric-alike vector length     equal number columns x. always.selected character vector names individuals genotype always selected core collection. maximum length accepted MStrat 500. file.name character string name file data saved. folder.path path folder input files saved.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"Kim K, Chung H, Cho G, Ma K, Chandrabalan D, Gwag J, Kim T, Cho E, Park Y (2007). “PowerCore: program applying advanced M strategy heuristic search establishing core sets.” Bioinformatics, 23(16), 2155–2162. Kim K, Chung H, Cho G, Ma K, Chandrabalan D, Gwag J, Kim T, Cho E, Park Y (2007). “PowerCore (v. 1.0): Program Applying Advanced M Strategy Using Heuristic Search Establishing Core Allele Mining Sets - User Manual.” Genetic Resources Division, National Institute Agricultural Biotechnology (NIAB), Rural Development Administration (RDA), R. Korea.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"","code":"library(EvaluateCore)  data(cassava_EC) data <- cassava_EC  quant <- c(\"NMSR\", \"TTRN\", \"TFWSR\", \"TTSW\", \"TTPW\", \"AVPW\",            \"ARSR\", \"SRDM\") qual <- c(\"CUAL\", \"LNGS\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Prepare genotype column data$Accession <- rownames(data) rownames(data) <- NULL data$Accession <- as.factor(data$Accession)  # Convert qualitative data as factors data[, qual] <- lapply(data[, qual],                        function(x) factor(as.factor(x)))  sel <- c(\"TMe-2906\", \"TMe-3412\", \"TMe-1374\", \"TMe-768\", \"TMe-14\",          \"TMe-3284\", \"TMe-937\", \"TMe-1859\", \"TMe-3265\", \"TMe-1739\",          \"TMe-972\", \"TMe-769\", \"TMe-3243\", \"TMe-3719\", \"TMe-1095\",          \"TMe-893\", \"TMe-1262\", \"TMe-2083\", \"TMe-376\", \"TMe-3633\",          \"TMe-1738\", \"TMe-2428\", \"TMe-259\", \"TMe-3457\", \"TMe-1406\",          \"TMe-977\", \"TMe-3006\", \"TMe-925\", \"TMe-3671\", \"TMe-2779\",          \"TMe-1293\", \"TMe-268\", \"TMe-266\", \"TMe-3562\", \"TMe-801\")  prep_powercore_input(data = data, genotype = \"Accession\",                      qualitative = qual, quantitative = quant,                      center = TRUE, scale = TRUE,                      always.selected = sel,                      file.name = \"PowerCore_input\",                      folder.path = tempdir()) #> PowerCore output file created at /var/folders/kg/7q73ww8s3llgyl61c9z_j5g40000gn/T//RtmpkQAuRG/PowerCore_input.csv"},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove scales from ggplot objects — remove_scales","title":"Remove scales from ggplot objects — remove_scales","text":"Useful avoiding warning Scale * already present. Adding another scale *, replace existing scale.","code":""},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove scales from ggplot objects — remove_scales","text":"","code":"remove_scales(g, scales)"},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove scales from ggplot objects — remove_scales","text":"g ggplot object. scales scales removed character vector.","code":""},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove scales from ggplot objects — remove_scales","text":"ggplot object without scales specified.","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":null,"dir":"Reference","previous_headings":"","what":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"function reorders tips (leaves) dendrogram elements belonging predefined groups appear together. useful hierarchical clustering visualization desired clusters groups appear contiguously.","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"","code":"reorder_dend_by_groups(dend, groups_list)"},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"dend object class dendrogram. groups_list list character vectors. vector contains tip labels kept adjacent. e.g., list(g1=c(\"\",\"b\"), g2=c(\"c\",\"d\")).","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"reordered dendrogram object.","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"","code":"set.seed(42)  mat <- matrix(rnorm(12*3), nrow = 12) rownames(mat) <- paste0(\"T\", 1:12)  # Hierarchical clustering hc <- hclust(dist(mat)) dend <- as.dendrogram(hc)  # Plot plot(dend, main = \"Original dendrogram\")    # Groups that align with the dendrogram structure #' groups_preserved <- list(   GroupA = c(\"T1\", \"T2\", \"T3\"),   GroupB = c(\"T4\", \"T5\", \"T6\"),   GroupC = c(\"T7\", \"T8\") )  dend_preserved <- reorder_dend_by_groups(dend, groups_preserved) #> Warning: The following groups could not be kept together due to dendrogram topology constraints: #> T1, T2, T3 #> T4, T5, T6 #> T7, T8  plot(dend_preserved, main = \"Groups preserved (no warning)\")   # Define groups that do NOT align with dendrogram # These tips are far apart in the dendrogram groups_split <- list(   GroupX = c(\"T1\", \"T5\", \"T9\"),   GroupY = c(\"T2\", \"T6\", \"T10\") )  dend_split <- reorder_dend_by_groups(dend, groups_split) #> Warning: The following groups could not be kept together due to dendrogram topology constraints: #> T1, T5, T9 #> T2, T6, T10  # Plot plot(dend_split, main = \"Groups split (warning expected)\")"},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"Summary Statistics Qualitative Quantitative trait data","code":""},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"","code":"summary_quant(data, group = NULL, trait, out.format = c(\"long\", \"wide\"))  summary_qual(data, group = NULL, trait, out.format = c(\"long\", \"wide\"))"},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"data data data frame object. data frame possess columns specifying trait group. group Name column specifying group character string. trait Name column specifying trait character string. trait column type \"numeric\" quantitative traits type \"factor\" qualitative traits. .format output format.","code":""},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"tibble data.frame summary statistics.","code":""},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"","code":"library(agridat) library(dplyr)  soydata <- australia.soybean  soydata$year <- as.factor(soydata$year)  quant_traits <- c(\"yield\", \"height\", \"lodging\",                   \"size\", \"protein\", \"oil\") set.seed(123) soydata <-   soydata %>%   mutate(     across(       .cols = all_of(quant_traits),       .fns = ~factor(cut(.x, breaks = quantile(.x, na.rm = TRUE),                          include.lowest = TRUE),                      labels = sample(1:9, size = 4)),       .names = \"{.col}_score\"     )   )  qual_traits <- c(\"yield_score\", \"height_score\", \"lodging_score\",                  \"size_score\", \"protein_score\", \"oil_score\")  summary_quant(data = soydata, trait = quant_traits) #> # A tibble: 6 × 12 #>   Trait   count miss.count   mean    min   max    sd     se    skew skew.pvalue #>   <chr>   <int>      <int>  <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl>       <dbl> #> 1 yield     464          0  2.05   0.282  4.38 0.752 0.0349 0.0249     8.24e- 1 #> 2 height    464          0  0.883  0.25   1.73 0.272 0.0126 0.278      1.47e- 2 #> 3 lodging   464          0  2.31   1      4.75 0.976 0.0453 0.414      3.66e- 4 #> 4 size      464          0 11.1    4     23.6  4.45  0.207  0.892      4.29e-12 #> 5 protein   464          0 40.3   33.2   48.5  2.93  0.136  0.280      1.42e- 2 #> 6 oil       464          0 19.9   13.0   26.8  2.67  0.124  0.00142    9.90e- 1 #> # ℹ 2 more variables: kurt <dbl>, kurt.pvalue <dbl> summary_qual(data = soydata, trait = qual_traits) #> # A tibble: 6 × 4 #>   Trait         count miss.count freq                               #>   <chr>         <int>      <int> <chr>                              #> 1 yield_score     464          0 3 (117), 6 (115), 9 (116), 2 (116) #> 2 height_score    464          0 2 (116), 6 (116), 3 (117), 5 (115) #> 3 lodging_score   464          0 4 (143), 6 (114), 8 (107), 1 (100) #> 4 size_score      464          0 5 (116), 3 (118), 8 (116), 1 (114) #> 5 protein_score   464          0 9 (116), 1 (120), 5 (114), 3 (114) #> 6 oil_score       464          0 8 (116), 2 (116), 7 (116), 9 (116)  summary_quant(data = soydata, group = \"loc\", trait = quant_traits) #> # A tibble: 24 × 13 #>    loc        Trait   count miss.count   mean    min   max    sd     se    skew #>    <fct>      <chr>   <int>      <int>  <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl> #>  1 Brookstead yield     116          0  2.01   0.385  3.90 0.825 0.0766  0.108  #>  2 Brookstead height    116          0  0.983  0.54   1.37 0.185 0.0172 -0.529  #>  3 Brookstead lodging   116          0  2.52   1      4.5  0.744 0.0691 -0.176  #>  4 Brookstead size      116          0 11.7    5.4   23.6  4.82  0.448   0.931  #>  5 Brookstead protein   116          0 40.9   34.2   48.5  2.74  0.254   0.381  #>  6 Brookstead oil       116          0 19.2   14.7   25.0  2.23  0.207   0.441  #>  7 Lawes      yield     116          0  2.37   0.586  4.38 0.654 0.0607 -0.129  #>  8 Lawes      height    116          0  1.02   0.405  1.73 0.324 0.0300  0.219  #>  9 Lawes      lodging   116          0  2.82   1      4.75 1.11  0.103   0.0210 #> 10 Lawes      size      116          0 11.7    6.55  23.2  4.32  0.401   1.01   #> # ℹ 14 more rows #> # ℹ 3 more variables: skew.pvalue <dbl>, kurt <dbl>, kurt.pvalue <dbl> summary_qual(data = soydata, group = \"loc\", trait = qual_traits) #> # A tibble: 24 × 5 #>    loc        Trait         count miss.count freq                           #>    <fct>      <chr>         <int>      <int> <chr>                          #>  1 Brookstead yield_score     116          0 3 (35), 6 (23), 9 (31), 2 (27) #>  2 Brookstead height_score    116          0 2 (12), 6 (17), 3 (39), 5 (48) #>  3 Brookstead lodging_score   116          0 4 (18), 6 (24), 8 (53), 1 (21) #>  4 Brookstead size_score      116          0 5 (24), 3 (31), 8 (31), 1 (30) #>  5 Brookstead protein_score   116          0 9 (19), 1 (31), 5 (33), 3 (33) #>  6 Brookstead oil_score       116          0 8 (36), 2 (41), 7 (21), 9 (18) #>  7 Lawes      yield_score     116          0 3 (16), 6 (20), 9 (31), 2 (49) #>  8 Lawes      height_score    116          0 2 (17), 6 (28), 3 (23), 5 (48) #>  9 Lawes      lodging_score   116          0 4 (20), 6 (27), 8 (19), 1 (50) #> 10 Lawes      size_score      116          0 5 (16), 3 (34), 8 (37), 1 (29) #> # ℹ 14 more rows  summary_quant(data = soydata, group = c(\"loc\", \"year\"), trait = quant_traits) #> # A tibble: 48 × 14 #> # Groups:   loc [4] #>    loc     year  Trait count miss.count   mean    min   max    sd     se    skew #>    <fct>   <fct> <chr> <int>      <int>  <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl> #>  1 Brooks… 1970  yield    58          0  1.56   0.385  3.16 0.736 0.0967  0.392  #>  2 Brooks… 1970  heig…    58          0  1.02   0.54   1.32 0.186 0.0244 -0.787  #>  3 Brooks… 1970  lodg…    58          0  2.35   1      4.5  0.756 0.0993  0.0412 #>  4 Brooks… 1970  size     58          0 10.8    5.4   22.1  4.60  0.604   1.04   #>  5 Brooks… 1970  prot…    58          0 40.1   34.2   45.8  2.43  0.319   0.299  #>  6 Brooks… 1970  oil      58          0 19.5   15.7   25.0  2.16  0.284   0.595  #>  7 Brooks… 1971  yield    58          0  2.46   1.11   3.90 0.649 0.0852  0.481  #>  8 Brooks… 1971  heig…    58          0  0.949  0.57   1.37 0.179 0.0235 -0.345  #>  9 Brooks… 1971  lodg…    58          0  2.69   1      4.5  0.699 0.0917 -0.359  #> 10 Brooks… 1971  size     58          0 12.6    6.85  23.6  4.93  0.647   0.853  #> # ℹ 38 more rows #> # ℹ 3 more variables: skew.pvalue <dbl>, kurt <dbl>, kurt.pvalue <dbl> summary_qual(data = soydata, group = c(\"loc\", \"year\"), trait = qual_traits) #> # A tibble: 48 × 6 #> # Groups:   loc [4] #>    loc        year  Trait         count miss.count freq                          #>    <fct>      <fct> <chr>         <int>      <int> <chr>                         #>  1 Brookstead 1970  yield_score      58          0 3 (33), 6 (9), 9 (9), 2 (7)   #>  2 Brookstead 1970  height_score     58          0 2 (4), 6 (8), 3 (16), 5 (30)  #>  3 Brookstead 1970  lodging_score    58          0 4 (12), 6 (14), 8 (25), 1 (7) #>  4 Brookstead 1970  size_score       58          0 5 (18), 3 (14), 8 (12), 1 (1… #>  5 Brookstead 1970  protein_score    58          0 9 (12), 1 (19), 5 (17), 3 (1… #>  6 Brookstead 1970  oil_score        58          0 8 (17), 2 (18), 7 (13), 9 (1… #>  7 Brookstead 1971  yield_score      58          0 3 (2), 6 (14), 9 (22), 2 (20) #>  8 Brookstead 1971  height_score     58          0 2 (8), 6 (9), 3 (23), 5 (18)  #>  9 Brookstead 1971  lodging_score    58          0 4 (6), 6 (10), 8 (28), 1 (14) #> 10 Brookstead 1971  size_score       58          0 5 (6), 3 (17), 8 (19), 1 (16) #> # ℹ 38 more rows"},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"Plot List ggplot2 Barplots Grid Uniform Width Bars","code":""},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"","code":"uniform_barwidth_grid(plot.list, level.count, nrow, ncol)"},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"plot.list list bar plots ggplot2 objects. level.count count levels/bars plot. nrow Number rows grid. ncol Number columns grid.","code":""},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"plot grid patchwork object.","code":""},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"","code":"# Load ggplot2 library(ggplot2) library(patchwork)  # Barplot 1 ---- data1 <- data.frame(   category = c(\"A\", \"B\", \"C\"),   value = c(10, 15, 8) )  plot1 <- ggplot(data1, aes(x = category, y = value, fill = category)) +   geom_bar(stat = \"identity\") +   ggtitle(\"Plot 1: 3 bars\") +   theme_minimal()  # Barplot 2 ---- data2 <- data.frame(   category = c(\"W\", \"X\", \"Y\", \"Z\", \"V\"),   value = c(12, 9, 20, 7, 14) )  plot2 <- ggplot(data2, aes(x = category, y = value, fill = category)) +   geom_bar(stat = \"identity\") +   ggtitle(\"Plot 2: 5 bars\") +   theme_minimal()  # Barplot 3 ---- data3 <- data.frame(   category = c(\"P\", \"Q\", \"R\", \"S\"),   value = c(5, 18, 11, 9) )  plot3 <- ggplot(data3, aes(x = category, y = value, fill = category)) +   geom_bar(stat = \"identity\") +   ggtitle(\"Plot 3: 4 bars\") +   theme_minimal()  # Plot originals with patchwork wrap_plots(plot1, plot2, plot3, nrow = 2)   # Plot to get uniform bar widths uniform_barwidth_grid(list(plot1, plot2, plot3),                       level.count = c(3, 5, 4), nrow = 2, ncol = 2) #> Warning: data length is not a multiple of split variable #> Warning: data length is not a multiple of split variable"}]
