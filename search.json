[{"path":"https://aravind-j.github.io/avial/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"J. Aravind. Author, maintainer.","code":""},{"path":"https://aravind-j.github.io/avial/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Aravind, J. ().  avial: Mixed Bag R Functions. R package version 0.0.0.9000, https://aravind-j.github.io/avial/.","code":"@Manual{,   title = {avial: A Mixed Bag of R Functionss},   author = {J. Aravind},   note = {R package version 0.0.0.9000 https://aravind-j.github.io/avial/}, }"},{"path":[]},{"path":[]},{"path":"https://aravind-j.github.io/avial/index.html","id":"aravind-j","dir":"","previous_headings":"avial: A Mixed Bag of R Functions","what":"Aravind, J.","title":"A Mixed Bag of R Functions","text":"Division Germplasm Conservation, ICAR-National Bureau Plant Genetic Resources, New Delhi.","code":""},{"path":"https://aravind-j.github.io/avial/index.html","id":"description","dir":"","previous_headings":"","what":"Description","title":"A Mixed Bag of R Functions","text":"bunch miscellaneous R functions primarly personal use.functions may migrated packages future required.","code":""},{"path":"https://aravind-j.github.io/avial/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"A Mixed Bag of R Functions","text":"development version can installed github follows:","code":"# Install development version from Github devtools::install_github(\"aravind-j/avial\")"},{"path":"https://aravind-j.github.io/avial/index.html","id":"whats-new","dir":"","previous_headings":"","what":"What’s new","title":"A Mixed Bag of R Functions","text":"know whats new version type:","code":"news(package='avial')"},{"path":"https://aravind-j.github.io/avial/index.html","id":"links","dir":"","previous_headings":"","what":"Links","title":"A Mixed Bag of R Functions","text":"Github page Documentation website","code":""},{"path":"https://aravind-j.github.io/avial/index.html","id":"citing-avial","dir":"","previous_headings":"","what":"Citing avial","title":"A Mixed Bag of R Functions","text":"cite methods package use:","code":"citation(\"avial\") To cite the R package 'augmentedRCBD' in publications use:    Aravind, J. ().  avial: A Mixed Bag of R Functions. R package version 0.0.0.9000,   https://aravind-j.github.io/avial/.  A BibTeX entry for LaTeX users is    @Manual{,     title = {avial: A Mixed Bag of R Functionss},     author = {J. Aravind},     note = {R package version 0.0.0.9000 https://aravind-j.github.io/avial/},   }  This free and open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the package."},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the Bin Width for Plotting Histograms — binw","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"Calculate Bin Width Plotting Histograms","code":""},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"","code":"binw(x, method = c(\"fd\", \"scott\", \"sturges\"))"},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"x numeric vector values histogram generated. method method compute number classes histogram.","code":""},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"bin width.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/binw.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the Bin Width for Plotting Histograms — binw","text":"","code":"set.seed(1) x <- stats::rnorm(1111)  binw(x = x, method = \"fd\") #> [1] 0.2 binw(x = x, method = \"scott\") #> [1] 0.5 binw(x = x, method = \"sturges\") #> [1] 0.5"},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":null,"dir":"Reference","previous_headings":"","what":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"Enhancement dnorm plot normal distribution bell curve overlay ggplot2 histograms according number records bin width.","code":""},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"","code":"dnorm_ggplot(x, mean, sd, n, bw)"},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"x vector values. mean vector means. sd vector standard deviations. n number records data points used plot histogram. bw bin width histogram.","code":""},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"density normal distribution.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/dnorm_ggplot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Function to Add Normal Distribution Curve in ggplot2 histogram — dnorm_ggplot","text":"","code":"dnorm(0) * 25 == dnorm_ggplot(0, mean = 0, sd = 1, n = 5, bw = 5) #> [1] TRUE dnorm(1) * 21 == dnorm_ggplot(1, mean = 0, sd = 1, n = 7, bw = 3) #> [1] TRUE"},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"Flatten Duplicate Rows Data Frame","code":""},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"","code":"flatten.data.frame(df, cols, collapse = \":\")"},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"df data frame flattened. cols columns(s) according duplicate rows identified flattening. collapse character string separate results flattening cell. Default \":\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"data frame duplicate rows flattened.","code":""},{"path":"https://aravind-j.github.io/avial/reference/flatten.data.frame.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Flatten Duplicate Rows in a Data Frame — flatten.data.frame","text":"","code":"data <- data.frame(team = c(rep(\"Justice League\", 17),                              rep(\"Avengers\", 13)),                    name = c(rep(\"Superman\", 5),                             rep(\"Batman\", 7),                              rep(\"Green Lantern\", 5),                              rep(\"Iron Man\", 4),                             rep(\"Captain America\", 5),                              rep(\"Thor\", 4)),                    identity = c(\"Clark Kent\", \"Christopher Kent\",                                  \"Kon-El\", \"John Henry Irons\",                                  \"Val-Zod\", \"Dick Grayson\", \"Bruce Wayne\",                                 \"Tim Drake\", \"Jean-Paul Valley\",                                  \"Terry McGinnis\", \"Jace Fox\",                                  \"Damian Wayne\",                                  \"Hal Jordan\", \" John Stewart\",                                  \" Guy Gardner\", \" Kyle Rayner\",                                  \"Jessica Cruz\", \"Tony Stark\",                                  \"Riri Williams\", \"James \\\"Rhodey\\\" Rhodes\",                                 \"Pepper Potts\",                                  \"Steve Rogers\",                                  \"Sam Wilson\", \"Bucky Barnes\",                                  \"Isaiah Bradley\", \"John Walker\",                                  \"Beta Ray Bill\", \"Jane Foster\",                                  \"Odinson\", \"Eric Masterson\"))  data_flat1 <- flatten.data.frame(df = data, cols = \"team\",                                collapse = \", \") data_flat2 <- flatten.data.frame(df = data, cols = c(\"team\", \"name\"),                                  collapse = \", \")"},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Frequency Distribution and Density Plots — freq_distribution","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"Plot Frequency Distribution Density Plots","code":""},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"","code":"freq_distribution(   data,   trait,   genotype = NULL,   hist = TRUE,   hist.col = \"gray45\",   hist.border = TRUE,   hist.border.col = \"black\",   hist.alpha = 0.8,   bw.adjust = 0.5,   density = TRUE,   density.col = \"black\",   density.fill = \"gray45\",   density.alpha = 0.1,   normal.curve = TRUE,   normal.curve.col = \"black\",   normal.curve.linetype = \"solid\",   highlight.mean = TRUE,   highlight.mean.col = \"black\",   show.counts = TRUE,   count.text.col = \"black\",   count.text.size = 3,   highlight.genotype.vline = FALSE,   highlight.genotype.pointrange = FALSE,   highlights = NULL,   highlight.col = highlight.mean.col,   standardize.xrange = TRUE )"},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"data data data frame object. data frame possess columns specifying trait (genotypes highlight.genotype.* = TRUE). trait Name column specifying trait character string. trait column type \"numeric\" quantitative traits type \"factor\" qualitative traits. genotype Name column specifying group character string. Required highlight.genotype.* =  TRUE. hist logical. TRUE, histogram plotted. Default TRUE. hist.col histograme colour. hist.border logical. TRUE, histogram border also plotted. Default TRUE. hist.border.col histogram border colour. hist.alpha Alpha transparency histogram. bw.adjust Multiplicative bin width adjustment. Default 0.5 means use half default bandwidth. density logical. TRUE, kernel density plotted. Default TRUE. density.col kernel density colour. density.fill kernel density fill colour. density.alpha Alpha transparency kernel density normal.curve logical. TRUE, normal curve plotted. Default TRUE. normal.curve.col colour normal curve. normal.curve.linetype Linetype normal curve. See aes_linetype_size_shape. highlight.mean logical. TRUE, mean value highlighted vertical line. Default TRUE. highlight.mean.col colour vertical line representing mean. show.counts TRUE, group wise counts plotted text annotation. Default TRUE. count.text.col colour count text annotation. count.text.size size count text annotation. highlight.genotype.vline logical. TRUE, mean values genotypes specified highlights plotted vertical lines. highlight.genotype.pointrange logical. TRUE, mean ± stand error values genotypes specified highlights plotted separate pointrange plot. highlights genotypes highlighted character vector. highlight.col colour(s) used highlight genotypes specified highlights plot character vector. Must valid colour values R (named colours, hexadecimal representation, index colours [1:8] default R palette() etc.). standardize.xrange logical. TRUE, original plot pointrange plot x axis ranges standardized. Default TRUE.","code":""},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"Either frequency distribution plot histogram kernel   density ggplot2 object, list containing ggplot2   pointrange plot histogram/kernel density plot.","code":""},{"path":"https://aravind-j.github.io/avial/reference/freq_distribution.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Frequency Distribution and Density Plots — freq_distribution","text":"","code":"library(agridat) library(ggplot2) library(patchwork) library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union  soydata <- australia.soybean soydata$gen <- as.character(soydata$gen) checks <- c(\"G01\", \"G05\")  check_cols <- c(\"#B2182B\", \"#2166AC\")  quant_traits <- c(\"yield\", \"height\", \"lodging\",                   \"size\", \"protein\", \"oil\") set.seed(123) soydata <-   soydata %>%   mutate(     across(       .cols = all_of(quant_traits),       .fns = ~factor(cut(.x, breaks = quantile(.x, na.rm = TRUE),                          include.lowest = TRUE),                      labels = sample(1:9, size = 4)),       .names = \"{.col}_score\"     )   )  qual_traits <- c(\"yield_score\", \"height_score\", \"lodging_score\",                  \"size_score\", \"protein_score\", \"oil_score\")   # Frequency distribution as histogram freq_hist1 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE,                     normal.curve = FALSE, highlight.mean = FALSE,                     show.counts = FALSE) freq_hist1   # Frequency distribution as histogram with normal curve freq_hist2 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE,                     normal.curve = TRUE, normal.curve.col = \"blue\",                     highlight.mean = FALSE,                     show.counts = FALSE) freq_hist2   # Frequency distribution as histogram with mean highlighted freq_hist3 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = TRUE, highlight.mean.col = \"red\",                     show.counts = FALSE) freq_hist3   # Frequency distribution as histogram with count value freq_hist4 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = FALSE,                     show.counts = TRUE, count.text.col = \"red\") freq_hist4   # Frequency distribution as histogram with check values # highlighted as vertical lines freq_hist5 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE, highlights = checks,                     highlight.col = check_cols) freq_hist5   # Frequency distribution as histogram with check values # highlighted as a separate pointrange plot freq_hist6 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     density = FALSE, normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE,                     highlight.genotype.pointrange = TRUE,                     highlights = checks,                     highlight.col = check_cols) freq_hist6[[1]] <-   freq_hist6[[1]] +   theme(axis.ticks.x = element_blank(),         axis.text.x = element_blank()) wrap_plots(freq_hist6[[1]], plot_spacer(), freq_hist6[[2]],            ncol = 1, heights = c(1, -0.5, 4))   # Frequency distribution as kernel density plot freq_dens1 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE, highlight.mean = FALSE,                     show.counts = FALSE) freq_dens1   # Frequency distribution as kernel density plot with mean highlighted freq_dens2 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = TRUE, highlight.mean.col = \"red\",                     show.counts = FALSE) freq_dens2   # Frequency distribution as kernel density plot with count value freq_dens3 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = FALSE,                     show.counts = TRUE, count.text.col = \"red\") freq_dens3   # Frequency distribution as kernel density plot with check values # highlighted as vertical lines freq_dens4 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE, highlights = checks,                     highlight.col = check_cols) freq_dens4   # Frequency distribution as kernel density plot with check values # highlighted as a separate pointrange plot freq_dens5 <-   freq_distribution(data = soydata, trait = \"lodging\",                     hist = FALSE,                     density = TRUE,                     density.fill = \"lemonchiffon\",                     normal.curve = FALSE,                     highlight.mean = FALSE, show.counts = FALSE,                     genotype = \"gen\",                     highlight.genotype.vline = TRUE,                     highlight.genotype.pointrange = TRUE,                     highlights = checks,                     highlight.col = check_cols) freq_dens5[[1]] <-   freq_dens5[[1]] +   theme(axis.ticks.x = element_blank(),         axis.text.x = element_blank()) wrap_plots(freq_dens5[[1]], plot_spacer(), freq_dens5[[2]],            ncol = 1, heights = c(1, -0.5, 4))   # Frequency counts of categorical data as a bar plot freq_bar1 <-   freq_distribution(data = soydata, trait = \"lodging_score\",                     hist = TRUE,                     hist.col = \"lemonchiffon\") freq_bar1   # Frequency counts of categorical data as a bar plot with check values # highlighted as vertical lines freq_bar2 <-   freq_distribution(data = soydata, trait = \"lodging_score\",                     hist = TRUE,                     hist.col = \"lemonchiffon\",                     genotype = \"gen\",                     highlight.genotype.vline = TRUE,                     highlights = checks,                     highlight.col = check_cols) freq_bar2   # Frequency counts of categorical data as a bar plot with check values # highlighted as a separate pointrange plot freq_bar3 <- freq_distribution(data = soydata, trait = \"lodging_score\",                                hist = TRUE,                                hist.col = \"lemonchiffon\",                                genotype = \"gen\",                                highlight.genotype.vline = TRUE,                                highlight.genotype.pointrange = TRUE,                                highlights = checks,                                highlight.col = check_cols) freq_bar3[[1]] <-   freq_bar3[[1]] +   theme(axis.ticks.x = element_blank(),         axis.text.x = element_blank()) wrap_plots(freq_bar3[[1]], plot_spacer(), freq_bar3[[2]],            ncol = 1, heights = c(1, -0.5, 4))"},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":null,"dir":"Reference","previous_headings":"","what":"Ordering by keeping defined groups together — group_constrained_order","title":"Ordering by keeping defined groups together — group_constrained_order","text":"function modifies order vector specified groups elements remain together. useful keeping specific dendrogram leaves together. location group block determined earliest member group original vector. elements involved group, relative order unchanged.","code":""},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ordering by keeping defined groups together — group_constrained_order","text":"","code":"group_constrained_order(x, groups, group_order = c(\"groups\", \"original\"))"},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ordering by keeping defined groups together — group_constrained_order","text":"x original vector elements order. groups groups elements kept together list vectors. groups must disjoint. group_order group internal order follow one \"groups\" \"original\" order.","code":""},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ordering by keeping defined groups together — group_constrained_order","text":"vector original elements specified groups   elements kept together.","code":""},{"path":"https://aravind-j.github.io/avial/reference/group_constrained_order.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ordering by keeping defined groups together — group_constrained_order","text":"","code":"set.seed(123)  # data mat <- matrix(rnorm(8 * 4), nrow = 8) rownames(mat) <- LETTERS[1:8]  # hierarchical clustering hc <- hclust(dist(mat)) plot(hc, main = \"Original dendrogram\")   # groups of leaves to keep together (may not be adjacent in dendrogram) groups <- list(   g1 = c(\"B\", \"F\"),   g2 = c(\"C\", \"D\", \"E\") )  # Dendrogram original order orig_order <- hc$labels[hc$order] orig_order #> [1] \"C\" \"F\" \"H\" \"B\" \"E\" \"A\" \"D\" \"G\"  # Get group constrained order new_order <-   group_constrained_order(x = orig_order,                           groups = groups,                           group_order = \"original\")  new_order #> [1] \"C\" \"E\" \"D\" \"F\" \"B\" \"H\" \"A\" \"G\"  # Reorder dendrogram labels hc2 <- hc hc2$order <- match(new_order, hc$labels)  plot(hc2, main = \"Dendrogram with grouped leaves\")"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Group-wise Bar Plots — groupwise_bar","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"Plot Group-wise Bar Plots","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"","code":"groupwise_bar(   data,   group,   trait,   bar.border = TRUE,   bar.alpha = 0.8,   by = c(\"group\", \"trait\"),   relative.freq = FALSE,   subset = c(\"facet\", \"none\"),   na.rm = TRUE,   include.overall = TRUE,   background.bar = TRUE,   background.bar.alpha = 0.25,   show.counts = TRUE,   count.text.size = 3,   position = c(\"dodge\", \"stack\"),   ncol = NULL,   nrow = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"data data data frame object. data frame possess columns specifying group trait. group Name column specifying group character string. trait Name column specifying trait character string. bar.border logical. TRUE, bar border also plotted. Default TRUE. bar.alpha Alpha transparency group-wise bar plot. factor according bars grouped. Either \"group\" \"trait\". relative.freq logical. TRUE, relative frequency proportion plotted instead counts. Default FALSE. subset method subsetting plots according argument \"group\". Either \"facet\" getting plot using faceting ggplot2 \"list\" getting list plots. na.rm logical. TRUE, NA factor levels excluded plot. Default TRUE. include.overall logical. TRUE, overall total data also plotted. Default TRUE. background.bar logical. TRUE, overall data plotted background bar plot = \"group\", include.overall = TRUE, position = \"dodge\". Default TRUE. background.bar.alpha Alpha transparency background bar plot. show.counts logical. TRUE, group wise counts plotted text annotation. Default TRUE. count.text.size size count text annotation. position Bar position adjustment. Either \"dodge\" \"stack\". ncol Number columns subset = \"facet\". nrow Number rows subset = \"facet\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"group-wise bar plot ggplot2 plot grob list   ggplot2 plot grobs.","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_bar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Group-wise Bar Plots — groupwise_bar","text":"","code":"library(agridat) library(ggplot2) library(patchwork)  soydata <- australia.soybean  clrs <- c(\"#B2182B\", \"#2166AC\", \"#009E53\", \"#E69F00\", \"gray25\") clrs_dark <- colorspace::darken(clrs, amount = 0.2)  soydata$lodging <- cut(soydata$lodging,                        breaks = quantile(soydata$lodging, na.rm = TRUE),                        include.lowest = TRUE) levels(soydata$lodging) <- 1:4  # soydata[soydata$loc == \"Nambour\", ]$lodging <- NA # #soydata[soydata$lodging == 1, ]$lodging # soydata[!(is.na(soydata$lodging)) & soydata$lodging == 1, ]$lodging <- NA  # set.seed(123) # ind <- sample(1:nrow(soydata), size = nrow(soydata) * 0.1, replace = FALSE) # soydata[ind, ]$lodging <- NA  # Group-wise side-by-side bar plot with counts ----  outg_group_dodge_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"none\", position = \"dodge\")  outg_group_dodge_count   outg_group_dodge_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"facet\", position = \"dodge\")  outg_group_dodge_count_facet   outg_group_dodge_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_group_dodge_count_list, nrow = 2, guides = \"collect\")   outg_group_dodge_count_list <-   lapply(seq_along(outg_group_dodge_count_list), function(i) {     outg_group_dodge_count_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_group_dodge_count_list, nrow = 2, guides = \"collect\")   # Group-wise side-by-side bar plot with relative frequencies ----  outg_group_dodge_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"none\", position = \"dodge\")  outg_group_dodge_rfreq   outg_group_dodge_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"dodge\")  outg_group_dodge_rfreq_facet   outg_group_dodge_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_dodge_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_group_dodge_rfreq_list, nrow = 2, guides = \"collect\")   outg_group_dodge_rfreq_list <-   lapply(seq_along(outg_group_dodge_rfreq_list), function(i) {     outg_group_dodge_rfreq_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_group_dodge_rfreq_list, nrow = 2, guides = \"collect\")   # Group-wise stacked bar plot with counts ----  outg_group_stack_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"none\", position = \"stack\")  outg_group_stack_count   outg_group_stack_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"facet\", position = \"stack\")  outg_group_stack_count_facet   outg_group_stack_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 subset = \"list\", position = \"stack\")  wrap_plots(outg_group_stack_count_list, nrow = 2, guides = \"collect\")   outg_group_stack_count_list <-   lapply(seq_along(outg_group_stack_count_list), function(i) {     outg_group_stack_count_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_group_stack_count_list, nrow = 2, guides = \"collect\")   # Group-wise stacked bar plot with relative frequencies ----  outg_group_stack_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"none\", position = \"stack\")  outg_group_stack_rfreq   outg_group_stack_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"stack\")  outg_group_stack_rfreq_facet   outg_group_stack_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_group_stack_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"group\",                 relative.freq = TRUE,                 subset = \"list\", position = \"stack\")  wrap_plots(outg_group_stack_rfreq_list, nrow = 2, guides = \"collect\")   outg_group_stack_rfreq_list <-   lapply(seq_along(outg_group_stack_rfreq_list), function(i) {     outg_group_stack_rfreq_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_group_stack_rfreq_list, nrow = 2, guides = \"collect\")    # Trait-wise side-by-side bar plot with counts ----  outg_trait_dodge_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"none\", position = \"dodge\")  outg_trait_dodge_count   outg_trait_dodge_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"facet\", position = \"dodge\")  outg_trait_dodge_count_facet   outg_trait_dodge_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_trait_dodge_count_list, nrow = 2, guides = \"collect\")   outg_trait_dodge_count_list <-   lapply(seq_along(outg_trait_dodge_count_list), function(i) {     outg_trait_dodge_count_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_trait_dodge_count_list, nrow = 2, guides = \"collect\")   # Trait-wise side-by-side bar plot with relative frequencies ----  outg_trait_dodge_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"none\", position = \"dodge\")  outg_trait_dodge_rfreq   outg_trait_dodge_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"dodge\")  outg_trait_dodge_rfreq_facet   outg_trait_dodge_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_dodge_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"list\", position = \"dodge\")  wrap_plots(outg_trait_dodge_rfreq_list, nrow = 2, guides = \"collect\")   outg_trait_dodge_rfreq_list <-   lapply(seq_along(outg_trait_dodge_rfreq_list), function(i) {     outg_trait_dodge_rfreq_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_trait_dodge_rfreq_list, nrow = 2, guides = \"collect\")   # Trait-wise stacked bar plot with counts ----  outg_trait_stack_count <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"none\", position = \"stack\")  outg_trait_stack_count   outg_trait_stack_count +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_count_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"facet\", position = \"stack\")  outg_trait_stack_count_facet   outg_trait_stack_count_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_count_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 subset = \"list\", position = \"stack\")  wrap_plots(outg_trait_stack_count_list, nrow = 2, guides = \"collect\")   outg_trait_stack_count_list <-   lapply(seq_along(outg_trait_stack_count_list), function(i) {     outg_trait_stack_count_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_trait_stack_count_list, nrow = 2, guides = \"collect\")   # Trait-wise stacked bar plot with relative frequencies ----  outg_trait_stack_rfreq <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"none\", position = \"stack\")  outg_trait_stack_rfreq   outg_trait_stack_rfreq +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_rfreq_facet <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"facet\", position = \"stack\")  outg_trait_stack_rfreq_facet   outg_trait_stack_rfreq_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark)   outg_trait_stack_rfreq_list <-   groupwise_bar(data = soydata, group = \"loc\", trait = \"lodging\",                 bar.border = TRUE, by = \"trait\",                 relative.freq = TRUE,                 subset = \"list\", position = \"stack\")  wrap_plots(outg_trait_stack_rfreq_list, nrow = 2, guides = \"collect\")   outg_trait_stack_rfreq_list <-   lapply(seq_along(outg_trait_stack_rfreq_list), function(i) {     outg_trait_stack_rfreq_list[[i]] +       scale_fill_manual(values = clrs) +       scale_colour_manual(values = clrs_dark)   })  wrap_plots(outg_trait_stack_rfreq_list, nrow = 2, guides = \"collect\")"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Group-wise Dumbell Plots — groupwise_dumbell","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"Plot Group-wise Dumbell Plots","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"","code":"groupwise_dumbell(   data,   group,   trait,   genotype,   subset = c(\"facet\", \"list\", \"none\"),   diff.sort = c(\"none\", \"ascending\", \"descending\"),   segment = TRUE,   segment.size = 3,   segment.colour = \"gray\",   segment.alpha = 0.5,   point.size = 3,   point.alpha = 0.8,   error.bar = TRUE,   error.bar.width = 0.1,   ncol = NULL,   nrow = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"data data data frame object. data frame possess columns specifying group trait. group Name column specifying group character string. trait Name column specifying trait character string. genotype Name column specifying genotype character string. subset method subsetting plots according argument \"group\". Either \"facet\" getting plot using faceting ggplot2 \"list\" getting list plots. diff.sort order sorting genotypes plotting. Either \"ascending\", \"descending\" \"none\". segment logical. TRUE, dumbell segment plotted. Default TRUE. segment.size size dumbell segment. segment.colour colour dumbell segment. segment.alpha Alpha transparency dumbell segment. point.size size points. point.alpha Alpha transparency points. error.bar logical. TRUE, error bars depicting standard errors plotted. Default TRUE. error.bar.width width error bars. ncol Number columns subset = \"facet\". nrow Number rows subset = \"facet\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"group-wise dumbell plot ggplot2 plot grob   list ggplot2 plot grobs.","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_dumbell.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Group-wise Dumbell Plots — groupwise_dumbell","text":"","code":"library(agridat) library(ggplot2) library(patchwork)  soydata <- australia.soybean # soydata[soydata$loc == \"Nambour\", ]$lodging <- NA  checks <- c(\"G01\", \"G05\")  checkdata <- soydata[soydata$gen %in% checks, ]  clrs <- c(\"#B2182B\", \"#2166AC\", \"#009E53\", \"#E69F00\") clrs_dark <- colorspace::darken(clrs, amount = 0.2)  # Group-wise dumbell plot with error bar outg_dumbell1 <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"none\", diff.sort = \"descending\") outg_dumbell1   outg_dumbell1 +   scale_colour_manual(values = clrs)   # Group-wise dumbell plot without error bar outg_dumbell2 <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"none\", diff.sort = \"descending\",                     error.bar = FALSE) outg_dumbell2   outg_dumbell2 +   scale_colour_manual(values = clrs)   # Group-wise points with error bar as facets outg_facet <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"facet\") outg_facet   outg_facet +   scale_colour_manual(values = clrs)   # Group-wise points with error bar as list outg_list <-   groupwise_dumbell(data = checkdata, group = \"loc\",                     trait = \"lodging\", genotype = \"gen\",                     subset = \"list\")  wrap_plots(outg_list, nrow = 2, guides = \"collect\")   outg_list <-   lapply(seq_along(outg_list), function(i) {     outg_list[[i]] +       scale_colour_manual(values = clrs[i])   }) #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_list, nrow = 2, guides = \"collect\")"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot Group-wise Histograms — groupwise_histogram","title":"Plot Group-wise Histograms — groupwise_histogram","text":"Plot Group-wise Histograms","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot Group-wise Histograms — groupwise_histogram","text":"","code":"groupwise_histogram(   data,   group,   trait,   background.hist = TRUE,   background.hist.alpha = 0.25,   background.density = TRUE,   background.density.alpha = 0.1,   hist = TRUE,   hist.border = TRUE,   hist.position = c(\"identity\", \"stack\"),   hist.alpha = 0.8,   bw.adjust = 0.5,   density = TRUE,   density.alpha = 0.1,   normal.curve = TRUE,   normal.curve.linetype = \"solid\",   highlight.mean = TRUE,   show.counts = TRUE,   count.text.size = 3,   subset = c(\"facet\", \"list\", \"none\"),   ncol = NULL,   nrow = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot Group-wise Histograms — groupwise_histogram","text":"data data data frame object. data frame possess columns specifying group trait. group Name column specifying group character string. trait Name column specifying trait character string. background.hist logical. TRUE, background data histogram plotted. Default TRUE. background.hist.alpha Alpha transparency background data histogram. background.density logical. TRUE, background data kernel density plotted. Default TRUE. background.density.alpha Alpha transparency background data kernel density. hist logical. TRUE, group-wise histogram plotted. Default TRUE. hist.border logical. TRUE, histogram border also plotted. Default TRUE. hist.position Histogram position adjustment. Either \"identity\" \"stack\". hist.alpha Alpha transparency group-wise histogram. bw.adjust Multiplicative bin width adjustment. Default 0.5 means use half default bandwidth. density logical. TRUE, group-wise kernel density plotted. Default TRUE. density.alpha Alpha transparency group-wise kernel density normal.curve logical. TRUE, normal curve plotted. Default TRUE. normal.curve.linetype Linetype normal curve. See aes_linetype_size_shape. highlight.mean logical. TRUE, mean value highlighted vertical line. Default TRUE. show.counts logical. TRUE, group wise counts plotted text annotation. Default TRUE. count.text.size size count text annotation. subset method subsetting plots according argument \"group\". Either \"facet\" getting plot using faceting ggplot2 \"list\" getting list plots. ncol Number columns subset = \"facet\". nrow Number rows subset = \"facet\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot Group-wise Histograms — groupwise_histogram","text":"group-wise histogram ggplot2 plot grob list   ggplot2 plot grobs.","code":""},{"path":"https://aravind-j.github.io/avial/reference/groupwise_histogram.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot Group-wise Histograms — groupwise_histogram","text":"","code":"library(agridat) library(ggplot2) library(patchwork)  soydata <- australia.soybean # soydata[soydata$loc == \"Nambour\", ]$lodging <- NA  clrs <- c(\"#B2182B\", \"#2166AC\", \"#009E53\", \"#E69F00\") clrs_dark <- colorspace::darken(clrs, amount = 0.2)  # Group-wise histogram ---- outg_hist <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = FALSE,                       hist.alpha = 0.5,                       density = FALSE,                       subset = \"none\") outg_hist   outg_hist +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise histogram - stacked ---- outg_hist_stack <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = FALSE,                       hist.position = \"stack\",                       density = FALSE,                       normal.curve = FALSE,                       subset = \"none\") outg_hist_stack   outg_hist_stack +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise histogram with facet ---- outg_hist_facet <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist.alpha = 0.5,                       density = FALSE,                       subset = \"facet\") outg_hist_facet   outg_hist_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise histogram as list ---- outg_hist_list <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist.alpha = 0.5,                       density = FALSE,                       subset = \"list\")  wrap_plots(outg_hist_list, nrow = 2, guides = \"collect\")   outg_hist_list <-   lapply(seq_along(outg_hist_list), function(i) {     outg_hist_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_hist_list, nrow = 2, guides = \"collect\")   # Group-wise density ---- outg_density <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = TRUE,                       hist = FALSE,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"none\") outg_density   outg_density +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density with facet ---- outg_density_facet <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = TRUE,                       hist = FALSE,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"facet\") outg_density_facet   outg_density_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density as list ---- outg_density_list <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = TRUE,                       hist = FALSE,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"list\")  wrap_plots(outg_density_list, nrow = 2, guides = \"collect\")   outg_density_list <-   lapply(seq_along(outg_density_list), function(i) {     outg_density_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_density_list, nrow = 2, guides = \"collect\")   # Group-wise density + histogram ---- outg_density_hist <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = FALSE,                       background.density = FALSE,                       hist = TRUE,                       hist.alpha = 0.3,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"none\") outg_density_hist   outg_density_hist +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density + histogram with facet ---- outg_density_hist_facet <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist = TRUE,                       hist.alpha = 0.3,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"facet\") outg_density_hist_facet   outg_density_hist_facet +   scale_fill_manual(values = clrs) +   scale_colour_manual(values = clrs_dark) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.   # Group-wise density + histogram as list ---- outg_density_hist_list <-   groupwise_histogram(data = soydata, group = \"loc\", trait = \"lodging\",                       background.hist = TRUE,                       background.density = FALSE,                       hist = TRUE,                       hist.alpha = 0.3,                       density = TRUE,                       normal.curve = FALSE,                       subset = \"list\")  wrap_plots(outg_density_hist_list, nrow = 2, guides = \"collect\")   outg_density_hist_list <-   lapply(seq_along(outg_density_hist_list), function(i) {     outg_density_hist_list[[i]] +       scale_fill_manual(values = clrs[i]) +       scale_colour_manual(values = clrs_dark[i])   }) #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale. #> Scale for fill is already present. #> Adding another scale for fill, which will replace the existing scale. #> Scale for colour is already present. #> Adding another scale for colour, which will replace the existing scale.  wrap_plots(outg_density_hist_list, nrow = 2, guides = \"collect\")"},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":null,"dir":"Reference","previous_headings":"","what":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"Perform chi-square test homogeneity variance (Bartlett's test) test equality several error variances mean squared errors (Gomez Gomez 1984) . MathJax = {output: {font: \"mathjax-newcm\", fontPath: \"../../mathjaxr/doc/mathjax/font\",}};","code":""},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"","code":"mse.bartlett.test(mse, df)"},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"mse vector error variances mean squared errors environment (years /locations). df vector degrees freedom corresponding mse.","code":""},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"list chi-square value test statistic, corresponding   degrees freedom p value.","code":""},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"Gomez KA, Gomez AA (1984). Statistical Procedures Agricultural Research, 2nd ed edition. Wiley, New York. ISBN 978-0-471-87092-0.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/mse.bartlett.test.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bartlett's Test of Homogeneity of Error Variances — mse.bartlett.test","text":"","code":"# Examples from Page 467-471 Gomez KA and AA Gomez (1984) Statistical # Procedures for Agricultural Research. 2nd ed. Wiley, New York, 680 p.  # Different degrees of freedom mse <- c(6.73920, 1.93496, 1.15500, 10.58450) df <- c(19, 16, 17, 19)  mse.bartlett.test(mse = c(6.73920, 1.93496, 1.15500, 10.58450),                   df = c(19, 16, 17, 19)) #> $chisq.value #> [1] 24.98754 #>  #> $df #> [1] 3 #>  #> $p.value #> [1] 1.553341e-05 #>   # Same degrees of freedom mse <- c(11.459848, 17.696970, 10.106818) df <- c(20, 20, 20)  mse.bartlett.test(mse = c(11.459848, 17.696970, 10.106818),                   df = c(20, 20, 20)) #> $chisq.value #> [1] 1.814362 #>  #> $df #> [1] 2 #>  #> $p.value #> [1] 0.4036606 #>"},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"pairwise.augmentedRCBD performs pairwise t tests adjusted means object class augmentedRCBD.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"","code":"pairwise.augmentedRCBD(   aug,   cl = NULL,   alpha = NULL,   p.adjust = c(\"none\", \"tukey\", \"sidak\") )"},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"aug object class augmentedRCBD. cl cluster object created makeCluster parallel evaluations. alpha Type error probability (Significance level). p.adjust p value adjustment method. Either \"none\", \"tukey\" \"sidak\".","code":""},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"data frame pairwise comparisons treatments.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"default pairwise comparison augmentedRCBD employs pairs.emmGrid function emmeans slow large number comparisons. function attempts faster parallel computing package parallel-package.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/pairwise.augmentedRCBD.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform Pairwise t Tests of Adjusted Means from augmentedRCBD Output — pairwise.augmentedRCBD","text":"","code":"library(augmentedRCBD) #>  #> -------------------------------------------------------------------------------- #> Welcome to augmentedRCBD version 0.1.7 #>  #>  #> # To know how to use this package type: #>   browseVignettes(package = 'augmentedRCBD') #>   for the package vignette. #>  #> # To know whats new in this version type: #>   news(package='augmentedRCBD') #>   for the NEWS file. #>  #> # To cite the methods in the package type: #>   citation(package='augmentedRCBD') #>  #> # To suppress this message use: #>   suppressPackageStartupMessages(library(augmentedRCBD)) #> --------------------------------------------------------------------------------  blk <- c(rep(1,7),rep(2,6),rep(3,7)) trt <- c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10) y1 <- c(92, 79, 87, 81, 96, 89, 82, 79, 81, 81, 91, 79, 78, 83, 77, 78, 78,         70, 75, 74) y2 <- c(258, 224, 238, 278, 347, 300, 289, 260, 220, 237, 227, 281, 311, 250,         240, 268, 287, 226, 395, 450) data <- data.frame(blk, trt, y1, y2) # Convert block and treatment to factors data$blk <- as.factor(data$blk) data$trt <- as.factor(data$trt) # Results for variable y1 out1 <- augmentedRCBD(data$blk, data$trt, data$y1, method.comp = \"lsd\",                       alpha = 0.05, group = TRUE, console = TRUE) #>  #> Augmented Design Details #> ======================== #>                                         #> Number of blocks           \"3\"          #> Number of treatments       \"12\"         #> Number of check treatments \"4\"          #> Number of test treatments  \"8\"          #> Check treatments           \"1, 2, 3, 4\" #>  #>  #> ANOVA, Treatment Adjusted #> ========================= #>                                      Df Sum Sq Mean Sq F value Pr(>F)   #> Block (ignoring Treatments)           2  360.1  180.04   6.675 0.0298 * #> Treatment (eliminating Blocks)       11  285.1   25.92   0.961 0.5499   #>   Treatment: Check                    3   52.9   17.64   0.654 0.6092   #>   Treatment: Test and Test vs. Check  8  232.2   29.02   1.076 0.4779   #> Residuals                             6  161.8   26.97                  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ANOVA, Block Adjusted #> ===================== #>                                Df Sum Sq Mean Sq F value Pr(>F) #> Treatment (ignoring Blocks)    11  575.7   52.33   1.940  0.215 #>   Treatment: Check              3   52.9   17.64   0.654  0.609 #>   Treatment: Test               7  505.9   72.27   2.679  0.125 #>   Treatment: Test vs. Check     1   16.9   16.87   0.626  0.459 #> Block (eliminating Treatments)  2   69.5   34.75   1.288  0.342 #> Residuals                       6  161.8   26.97                #>  #> Coefficient of Variation #> ======================== #> 6.372367 #>  #> Overall Adjusted Mean #> ===================== #> 81.0625 #>  #> Standard Errors #> =============== #>                                          Std. Error of Diff.  CD (5%) #> Control Treatment Means                             4.240458 10.37603 #> Two Test Treatments (Same Block)                    7.344688 17.97180 #> Two Test Treatments (Different Blocks)              8.211611 20.09309 #> A Test Treatment and a Control Treatment            6.704752 16.40594 #>  #> Treatment Means #> =============== #>  Treatment Block Means   SE r   Min   Max Adjusted Means #>          1       84.67 3.84 3 79.00 92.00          84.67 #>         10     3 74.00 <NA> 1 74.00 74.00          77.25 #>         11     1 89.00 <NA> 1 89.00 89.00          86.50 #>         12     1 82.00 <NA> 1 82.00 82.00          79.50 #>          2       79.00 1.15 3 77.00 81.00          79.00 #>          3       82.00 2.65 3 78.00 87.00          82.00 #>          4       83.33 3.93 3 78.00 91.00          83.33 #>          5     2 79.00 <NA> 1 79.00 79.00          78.25 #>          6     3 75.00 <NA> 1 75.00 75.00          78.25 #>          7     1 96.00 <NA> 1 96.00 96.00          93.50 #>          8     3 70.00 <NA> 1 70.00 70.00          73.25 #>          9     2 78.00 <NA> 1 78.00 78.00          77.25 #>  #>  #> Comparisons #> =========== #>  #> Method : lsd #>  #>                   contrast estimate   SE df t.ratio p.value sig #>    treatment1 - treatment2     5.67 4.24  6   1.336   0.230     #>    treatment1 - treatment3     2.67 4.24  6   0.629   0.553     #>    treatment1 - treatment4     1.33 4.24  6   0.314   0.764     #>    treatment1 - treatment5     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment6     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment7    -8.83 6.36  6  -1.389   0.214     #>    treatment1 - treatment8    11.42 6.36  6   1.795   0.123     #>    treatment1 - treatment9     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment10     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment11    -1.83 6.36  6  -0.288   0.783     #>   treatment1 - treatment12     5.17 6.36  6   0.812   0.448     #>    treatment2 - treatment3    -3.00 4.24  6  -0.707   0.506     #>    treatment2 - treatment4    -4.33 4.24  6  -1.022   0.346     #>    treatment2 - treatment5     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment6     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment7   -14.50 6.36  6  -2.280   0.063     #>    treatment2 - treatment8     5.75 6.36  6   0.904   0.401     #>    treatment2 - treatment9     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment10     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment11    -7.50 6.36  6  -1.179   0.283     #>   treatment2 - treatment12    -0.50 6.36  6  -0.079   0.940     #>    treatment3 - treatment4    -1.33 4.24  6  -0.314   0.764     #>    treatment3 - treatment5     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment6     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment7   -11.50 6.36  6  -1.808   0.121     #>    treatment3 - treatment8     8.75 6.36  6   1.376   0.218     #>    treatment3 - treatment9     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment10     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment11    -4.50 6.36  6  -0.707   0.506     #>   treatment3 - treatment12     2.50 6.36  6   0.393   0.708     #>    treatment4 - treatment5     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment6     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment7   -10.17 6.36  6  -1.598   0.161     #>    treatment4 - treatment8    10.08 6.36  6   1.585   0.164     #>    treatment4 - treatment9     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment10     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment11    -3.17 6.36  6  -0.498   0.636     #>   treatment4 - treatment12     3.83 6.36  6   0.603   0.569     #>    treatment5 - treatment6     0.00 8.21  6   0.000   1.000     #>    treatment5 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment5 - treatment8     5.00 8.21  6   0.609   0.565     #>    treatment5 - treatment9     1.00 7.34  6   0.136   0.896     #>   treatment5 - treatment10     1.00 8.21  6   0.122   0.907     #>   treatment5 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment5 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment6 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment6 - treatment8     5.00 7.34  6   0.681   0.521     #>    treatment6 - treatment9     1.00 8.21  6   0.122   0.907     #>   treatment6 - treatment10     1.00 7.34  6   0.136   0.896     #>   treatment6 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment6 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment7 - treatment8    20.25 8.21  6   2.466   0.049   * #>    treatment7 - treatment9    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment10    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment11     7.00 7.34  6   0.953   0.377     #>   treatment7 - treatment12    14.00 7.34  6   1.906   0.105     #>    treatment8 - treatment9    -4.00 8.21  6  -0.487   0.643     #>   treatment8 - treatment10    -4.00 7.34  6  -0.545   0.606     #>   treatment8 - treatment11   -13.25 8.21  6  -1.614   0.158     #>   treatment8 - treatment12    -6.25 8.21  6  -0.761   0.475     #>   treatment9 - treatment10     0.00 8.21  6   0.000   1.000     #>   treatment9 - treatment11    -9.25 8.21  6  -1.126   0.303     #>   treatment9 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment10 - treatment11    -9.25 8.21  6  -1.126   0.303     #>  treatment10 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment11 - treatment12     7.00 7.34  6   0.953   0.377     #>  #> Treatment Groups #> ================ #>  #> Method : lsd #>  #>  Treatment Adjusted Means   SE df lower.CL upper.CL Group #>          8          73.25 5.61  6    59.52    86.98    1  #>          9          77.25 5.61  6    63.52    90.98    12 #>         10          77.25 5.61  6    63.52    90.98    12 #>          5          78.25 5.61  6    64.52    91.98    12 #>          6          78.25 5.61  6    64.52    91.98    12 #>          2          79.00 3.00  6    71.66    86.34    12 #>         12          79.50 5.61  6    65.77    93.23    12 #>          3          82.00 3.00  6    74.66    89.34    12 #>          4          83.33 3.00  6    76.00    90.67    12 #>          1          84.67 3.00  6    77.33    92.00    12 #>         11          86.50 5.61  6    72.77   100.23    12 #>          7          93.50 5.61  6    79.77   107.23     2 # Results for variable y2 out2 <- augmentedRCBD(data$blk, data$trt, data$y1, method.comp = \"lsd\",                       alpha = 0.05, group = TRUE, console = TRUE) #>  #> Augmented Design Details #> ======================== #>                                         #> Number of blocks           \"3\"          #> Number of treatments       \"12\"         #> Number of check treatments \"4\"          #> Number of test treatments  \"8\"          #> Check treatments           \"1, 2, 3, 4\" #>  #>  #> ANOVA, Treatment Adjusted #> ========================= #>                                      Df Sum Sq Mean Sq F value Pr(>F)   #> Block (ignoring Treatments)           2  360.1  180.04   6.675 0.0298 * #> Treatment (eliminating Blocks)       11  285.1   25.92   0.961 0.5499   #>   Treatment: Check                    3   52.9   17.64   0.654 0.6092   #>   Treatment: Test and Test vs. Check  8  232.2   29.02   1.076 0.4779   #> Residuals                             6  161.8   26.97                  #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ANOVA, Block Adjusted #> ===================== #>                                Df Sum Sq Mean Sq F value Pr(>F) #> Treatment (ignoring Blocks)    11  575.7   52.33   1.940  0.215 #>   Treatment: Check              3   52.9   17.64   0.654  0.609 #>   Treatment: Test               7  505.9   72.27   2.679  0.125 #>   Treatment: Test vs. Check     1   16.9   16.87   0.626  0.459 #> Block (eliminating Treatments)  2   69.5   34.75   1.288  0.342 #> Residuals                       6  161.8   26.97                #>  #> Coefficient of Variation #> ======================== #> 6.372367 #>  #> Overall Adjusted Mean #> ===================== #> 81.0625 #>  #> Standard Errors #> =============== #>                                          Std. Error of Diff.  CD (5%) #> Control Treatment Means                             4.240458 10.37603 #> Two Test Treatments (Same Block)                    7.344688 17.97180 #> Two Test Treatments (Different Blocks)              8.211611 20.09309 #> A Test Treatment and a Control Treatment            6.704752 16.40594 #>  #> Treatment Means #> =============== #>  Treatment Block Means   SE r   Min   Max Adjusted Means #>          1       84.67 3.84 3 79.00 92.00          84.67 #>         10     3 74.00 <NA> 1 74.00 74.00          77.25 #>         11     1 89.00 <NA> 1 89.00 89.00          86.50 #>         12     1 82.00 <NA> 1 82.00 82.00          79.50 #>          2       79.00 1.15 3 77.00 81.00          79.00 #>          3       82.00 2.65 3 78.00 87.00          82.00 #>          4       83.33 3.93 3 78.00 91.00          83.33 #>          5     2 79.00 <NA> 1 79.00 79.00          78.25 #>          6     3 75.00 <NA> 1 75.00 75.00          78.25 #>          7     1 96.00 <NA> 1 96.00 96.00          93.50 #>          8     3 70.00 <NA> 1 70.00 70.00          73.25 #>          9     2 78.00 <NA> 1 78.00 78.00          77.25 #>  #>  #> Comparisons #> =========== #>  #> Method : lsd #>  #>                   contrast estimate   SE df t.ratio p.value sig #>    treatment1 - treatment2     5.67 4.24  6   1.336   0.230     #>    treatment1 - treatment3     2.67 4.24  6   0.629   0.553     #>    treatment1 - treatment4     1.33 4.24  6   0.314   0.764     #>    treatment1 - treatment5     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment6     6.42 6.36  6   1.009   0.352     #>    treatment1 - treatment7    -8.83 6.36  6  -1.389   0.214     #>    treatment1 - treatment8    11.42 6.36  6   1.795   0.123     #>    treatment1 - treatment9     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment10     7.42 6.36  6   1.166   0.288     #>   treatment1 - treatment11    -1.83 6.36  6  -0.288   0.783     #>   treatment1 - treatment12     5.17 6.36  6   0.812   0.448     #>    treatment2 - treatment3    -3.00 4.24  6  -0.707   0.506     #>    treatment2 - treatment4    -4.33 4.24  6  -1.022   0.346     #>    treatment2 - treatment5     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment6     0.75 6.36  6   0.118   0.910     #>    treatment2 - treatment7   -14.50 6.36  6  -2.280   0.063     #>    treatment2 - treatment8     5.75 6.36  6   0.904   0.401     #>    treatment2 - treatment9     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment10     1.75 6.36  6   0.275   0.792     #>   treatment2 - treatment11    -7.50 6.36  6  -1.179   0.283     #>   treatment2 - treatment12    -0.50 6.36  6  -0.079   0.940     #>    treatment3 - treatment4    -1.33 4.24  6  -0.314   0.764     #>    treatment3 - treatment5     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment6     3.75 6.36  6   0.590   0.577     #>    treatment3 - treatment7   -11.50 6.36  6  -1.808   0.121     #>    treatment3 - treatment8     8.75 6.36  6   1.376   0.218     #>    treatment3 - treatment9     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment10     4.75 6.36  6   0.747   0.483     #>   treatment3 - treatment11    -4.50 6.36  6  -0.707   0.506     #>   treatment3 - treatment12     2.50 6.36  6   0.393   0.708     #>    treatment4 - treatment5     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment6     5.08 6.36  6   0.799   0.455     #>    treatment4 - treatment7   -10.17 6.36  6  -1.598   0.161     #>    treatment4 - treatment8    10.08 6.36  6   1.585   0.164     #>    treatment4 - treatment9     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment10     6.08 6.36  6   0.956   0.376     #>   treatment4 - treatment11    -3.17 6.36  6  -0.498   0.636     #>   treatment4 - treatment12     3.83 6.36  6   0.603   0.569     #>    treatment5 - treatment6     0.00 8.21  6   0.000   1.000     #>    treatment5 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment5 - treatment8     5.00 8.21  6   0.609   0.565     #>    treatment5 - treatment9     1.00 7.34  6   0.136   0.896     #>   treatment5 - treatment10     1.00 8.21  6   0.122   0.907     #>   treatment5 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment5 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment6 - treatment7   -15.25 8.21  6  -1.857   0.113     #>    treatment6 - treatment8     5.00 7.34  6   0.681   0.521     #>    treatment6 - treatment9     1.00 8.21  6   0.122   0.907     #>   treatment6 - treatment10     1.00 7.34  6   0.136   0.896     #>   treatment6 - treatment11    -8.25 8.21  6  -1.005   0.354     #>   treatment6 - treatment12    -1.25 8.21  6  -0.152   0.884     #>    treatment7 - treatment8    20.25 8.21  6   2.466   0.049   * #>    treatment7 - treatment9    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment10    16.25 8.21  6   1.979   0.095     #>   treatment7 - treatment11     7.00 7.34  6   0.953   0.377     #>   treatment7 - treatment12    14.00 7.34  6   1.906   0.105     #>    treatment8 - treatment9    -4.00 8.21  6  -0.487   0.643     #>   treatment8 - treatment10    -4.00 7.34  6  -0.545   0.606     #>   treatment8 - treatment11   -13.25 8.21  6  -1.614   0.158     #>   treatment8 - treatment12    -6.25 8.21  6  -0.761   0.475     #>   treatment9 - treatment10     0.00 8.21  6   0.000   1.000     #>   treatment9 - treatment11    -9.25 8.21  6  -1.126   0.303     #>   treatment9 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment10 - treatment11    -9.25 8.21  6  -1.126   0.303     #>  treatment10 - treatment12    -2.25 8.21  6  -0.274   0.793     #>  treatment11 - treatment12     7.00 7.34  6   0.953   0.377     #>  #> Treatment Groups #> ================ #>  #> Method : lsd #>  #>  Treatment Adjusted Means   SE df lower.CL upper.CL Group #>          8          73.25 5.61  6    59.52    86.98    1  #>          9          77.25 5.61  6    63.52    90.98    12 #>         10          77.25 5.61  6    63.52    90.98    12 #>          5          78.25 5.61  6    64.52    91.98    12 #>          6          78.25 5.61  6    64.52    91.98    12 #>          2          79.00 3.00  6    71.66    86.34    12 #>         12          79.50 5.61  6    65.77    93.23    12 #>          3          82.00 3.00  6    74.66    89.34    12 #>          4          83.33 3.00  6    76.00    90.67    12 #>          1          84.67 3.00  6    77.33    92.00    12 #>         11          86.50 5.61  6    72.77   100.23    12 #>          7          93.50 5.61  6    79.77   107.23     2  # Make cluster ---- library(parallel) # Check if running under R CMD check and adjust cores accordingly if (nzchar(Sys.getenv(\"_R_CHECK_LIMIT_CORES_\"))) {   ncores <- 2 } else {   ncores <- max(2L, parallel::detectCores() - 4) }  cl <- makeCluster(getOption(\"cl.cores\", ncores))  # Pairwise t test without p value adjustment ---- pout1 <- pairwise.augmentedRCBD(out1, cl = cl,                                 p.adjust = \"none\") pout1 #>       contrast   estimate       SE df     t.ratio    p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.68898284     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 0.79368531     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.52980915     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.77011499     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.44738825     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.23616021     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.62449310     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.62449310     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 0.23575127     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 1.86049532     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.68898284     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 0.30300033     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 0.79327167     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 0.80281330     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 0.50524328     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 0.39921196     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 0.90705045     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 0.89615381     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.09516823     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.39434533     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.00000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.62264457     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.69391854     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.47290592     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.34659999     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.64616381     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.64616381     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 0.37735543     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 1.84225214     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.69699967     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.05702215     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 0.72206273     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 0.58826470     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.11599954     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.11599954     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.10526990     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.52457343     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.20672833     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 0.50581089     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 0.34624985     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.08541796     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.08541796     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.07380701     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.57596055     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.19718670     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 0.76383979     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.40380253     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.40380253     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.13713052     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.76031005     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.49475672     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.52293671     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.52293671     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.18022066     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 1.81669827     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.60078804     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.00000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.43507937     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.10384619     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.47858869     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.09294955     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 1.95128006     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 1.90483177     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 0.64346045     stopCluster(cl)  cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout2 <- pairwise.augmentedRCBD(out2, cl = cl,                                 p.adjust = \"none\") pout2 #>       contrast   estimate       SE df     t.ratio    p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.68898284     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 0.79368531     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.52980915     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.77011499     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.44738825     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.23616021     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.62449310     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.62449310     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 0.23575127     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 1.86049532     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.68898284     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 0.30300033     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 0.79327167     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 0.80281330     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 0.50524328     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 0.39921196     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 0.90705045     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 0.89615381     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.09516823     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.39434533     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.00000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.62264457     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.69391854     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.47290592     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.34659999     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.64616381     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.64616381     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 0.37735543     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 1.84225214     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.69699967     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.05702215     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 0.72206273     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 0.58826470     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.11599954     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.11599954     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.10526990     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.52457343     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.20672833     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 0.50581089     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 0.34624985     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.08541796     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.08541796     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.07380701     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.57596055     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.19718670     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 0.76383979     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.40380253     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.40380253     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.13713052     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.76031005     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.49475672     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.52293671     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.52293671     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.18022066     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 1.81669827     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.60078804     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.00000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.43507937     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.10384619     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.11267166     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.47858869     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.09294955     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 1.95128006     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 1.90483177     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 0.64346045     stopCluster(cl)  # Pairwise t test with tukey adjustment ---- cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout1_tukey <- pairwise.augmentedRCBD(out1, cl = cl,                                       p.adjust = \"tukey\") pout1_tukey #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 0.9985579     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 0.9999418     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 0.9937163     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 0.9999919     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 0.9995791     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 0.9995791     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9682124     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 0.9985579     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 1.0000000     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 0.9999981     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 0.9995942     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 0.9984198     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 0.9999846     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 0.9999996     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 0.9993581     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 0.9993581     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9771900     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 0.9983271     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 1.0000000     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 0.9999482     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 1.0000000     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 0.9998426     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 0.9999975     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 0.9999975     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 1.0000000     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 0.9947262     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 0.9999740     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 0.9999501     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 0.9999501     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 1.0000000     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9855836     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 0.9997378     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 0.9999941     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 0.9999823     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.8043427     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9265988     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)  cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout2_tukey <- pairwise.augmentedRCBD(out2, cl = cl,                                       p.adjust = \"tukey\") pout2_tukey #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 0.9985579     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 0.9999418     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 0.9937163     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 0.9999919     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 0.9995791     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 0.9995791     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9682124     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 0.9985579     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 1.0000000     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 0.9999981     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 0.9995942     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 0.9984198     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 0.9999846     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 0.9999996     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 0.9993581     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 0.9993581     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9771900     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 0.9983271     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 1.0000000     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 0.9999482     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 1.0000000     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 0.9998426     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 0.9999975     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 0.9999975     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 1.0000000     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 0.9947262     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 0.9999740     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 0.9999501     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 0.9999501     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 1.0000000     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9855836     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 0.9997378     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 0.9999941     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 1.0000000     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 0.9999823     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.8043427     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9265988     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)  # Pairwise t test with sidak p value adjustment ---- cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout1_sidak <- pairwise.augmentedRCBD(out1, cl = cl,                                       p.adjust = \"sidak\") pout1_sidak #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.0000000     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.0000000     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.0000000     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.0000000     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.0000000     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.0000000     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9999506     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.0000000     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.9986402     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.0000000     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.0000000     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.0000000     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.0000000     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.0000000     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.0000000     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.0000000     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9999880     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.0000000     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.9993519     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.0000000     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.9936569     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.0000000     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.0000000     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.0000000     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.9999408     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.0000000     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.0000000     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.0000000     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.0000000     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.9999980     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9999984     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.0000000     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.0000000     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.0000000     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.9629870     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9986402     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)  cl <- makeCluster(getOption(\"cl.cores\", ncores)) pout2_sidak <- pairwise.augmentedRCBD(out2, cl = cl,                                       p.adjust = \"sidak\") pout2_sidak #>       contrast   estimate       SE df     t.ratio   p.value sig #> 1     1 - (10)   7.416667 6.704752  6  1.10618056 1.0000000     #> 2     1 - (11)  -1.833333 6.704752  6 -0.27343789 1.0000000     #> 3     1 - (12)   5.166667 6.704752  6  0.77059769 1.0000000     #> 4        1 - 2   5.666667 4.240458  6  1.33633373 1.0000000     #> 5        1 - 3   2.666667 4.240458  6  0.62886293 1.0000000     #> 6        1 - 4   1.333333 4.240458  6  0.31443147 1.0000000     #> 7      1 - (5)   6.416667 6.704752  6  0.95703262 1.0000000     #> 8      1 - (6)   6.416667 6.704752  6  0.95703262 1.0000000     #> 9      1 - (7)  -8.833333 6.704752  6 -1.31747347 1.0000000     #> 10     1 - (8)  11.416667 6.704752  6  1.70277232 0.9999506     #> 11     1 - (9)   7.416667 6.704752  6  1.10618056 1.0000000     #> 12 (10) - (11)  -9.250000 8.211611  6 -1.12645375 1.0000000     #> 13 (10) - (12)  -2.250000 8.211611  6 -0.27400226 1.0000000     #> 14    (10) - 2  -1.750000 6.704752  6 -0.26100890 1.0000000     #> 15    (10) - 3  -4.750000 6.704752  6 -0.70845272 1.0000000     #> 16    (10) - 4  -6.083333 6.704752  6 -0.90731664 1.0000000     #> 17  (10) - (5)  -1.000000 8.211611  6 -0.12177878 1.0000000     #> 18  (10) - (6)  -1.000000 7.344688  6 -0.13615282 1.0000000     #> 19  (10) - (7) -16.250000 8.211611  6 -1.97890523 0.9986402     #> 20  (10) - (8)   4.000000 7.344688  6  0.54461128 1.0000000     #> 21  (10) - (9)   0.000000 8.211611  6  0.00000000 1.0000000     #> 22 (11) - (12)   7.000000 7.344688  6  0.95306973 1.0000000     #> 23    (11) - 2   7.500000 6.704752  6  1.11860955 1.0000000     #> 24    (11) - 3   4.500000 6.704752  6  0.67116573 1.0000000     #> 25    (11) - 4   3.166667 6.704752  6  0.47230181 1.0000000     #> 26  (11) - (5)   8.250000 8.211611  6  1.00467496 1.0000000     #> 27  (11) - (6)   8.250000 8.211611  6  1.00467496 1.0000000     #> 28  (11) - (7)  -7.000000 7.344688  6 -0.95306973 1.0000000     #> 29  (11) - (8)  13.250000 8.211611  6  1.61356888 0.9999880     #> 30  (11) - (9)   9.250000 8.211611  6  1.12645375 1.0000000     #> 31    (12) - 2   0.500000 6.704752  6  0.07457397 1.0000000     #> 32    (12) - 3  -2.500000 6.704752  6 -0.37286985 1.0000000     #> 33    (12) - 4  -3.833333 6.704752  6 -0.57173377 1.0000000     #> 34  (12) - (5)   1.250000 8.211611  6  0.15222348 1.0000000     #> 35  (12) - (6)   1.250000 8.211611  6  0.15222348 1.0000000     #> 36  (12) - (7) -14.000000 7.344688  6 -1.90613947 0.9993519     #> 37  (12) - (8)   6.250000 8.211611  6  0.76111740 1.0000000     #> 38  (12) - (9)   2.250000 8.211611  6  0.27400226 1.0000000     #> 39       2 - 3  -3.000000 4.240458  6 -0.70747080 1.0000000     #> 40       2 - 4  -4.333333 4.240458  6 -1.02190227 1.0000000     #> 41     2 - (5)   0.750000 6.704752  6  0.11186096 1.0000000     #> 42     2 - (6)   0.750000 6.704752  6  0.11186096 1.0000000     #> 43     2 - (7) -14.500000 6.704752  6 -2.16264514 0.9936569     #> 44     2 - (8)   5.750000 6.704752  6  0.85760066 1.0000000     #> 45     2 - (9)   1.750000 6.704752  6  0.26100890 1.0000000     #> 46       3 - 4  -1.333333 4.240458  6 -0.31443147 1.0000000     #> 47     3 - (5)   3.750000 6.704752  6  0.55930478 1.0000000     #> 48     3 - (6)   3.750000 6.704752  6  0.55930478 1.0000000     #> 49     3 - (7) -11.500000 6.704752  6 -1.71520132 0.9999408     #> 50     3 - (8)   8.750000 6.704752  6  1.30504448 1.0000000     #> 51     3 - (9)   4.750000 6.704752  6  0.70845272 1.0000000     #> 52     4 - (5)   5.083333 6.704752  6  0.75816870 1.0000000     #> 53     4 - (6)   5.083333 6.704752  6  0.75816870 1.0000000     #> 54     4 - (7) -10.166667 6.704752  6 -1.51633739 0.9999980     #> 55     4 - (8)  10.083333 6.704752  6  1.50390840 0.9999984     #> 56     4 - (9)   6.083333 6.704752  6  0.90731664 1.0000000     #> 57   (5) - (6)   0.000000 8.211611  6  0.00000000 1.0000000     #> 58   (5) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 59   (5) - (8)   5.000000 8.211611  6  0.60889392 1.0000000     #> 60   (5) - (9)   1.000000 7.344688  6  0.13615282 1.0000000     #> 61   (6) - (7) -15.250000 8.211611  6 -1.85712645 0.9996254     #> 62   (6) - (8)   5.000000 7.344688  6  0.68076409 1.0000000     #> 63   (6) - (9)   1.000000 8.211611  6  0.12177878 1.0000000     #> 64   (7) - (8)  20.250000 8.211611  6  2.46602036 0.9629870     #> 65   (7) - (9)  16.250000 8.211611  6  1.97890523 0.9986402     #> 66   (8) - (9)  -4.000000 8.211611  6 -0.48711513 1.0000000     stopCluster(cl)"},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse Output Files from MStrat — parse_mstrat_out","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"Prepare raw output files generated MStrat (Schoen Brown 1993; Gouesnard et al. 2001; Gouesnard et al. 2002) .","code":""},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"","code":"parse_mstrat_out(   data.file,   genotype = NULL,   variable.file,   kernel.file,   redundance.output = NULL,   core.output = NULL )"},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"data.file path .dat input file used generating output files MStrat. genotype Name column/variable genotype names character string. variable.file path .var input file used generating output files MStrat. kernel.file path .ker input file used generating output files MStrat. redundance.output path redundance output file generated MStrat. core.output path core output file generated MStrat.","code":""},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"list following components: MStrat Core   Output raw output core sets constructed MStrat. MStrat Core Optimised optimised core set output   MStrat. MStrat Redundance Output raw output   Redundance estimation MStrat MStrat Redundance   Plot list plots Redundance estimation.","code":""},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2001). “MSTRAT: algorithm building germ plasm core collections maximizing allelic phenotypic richness.” Journal Heredity, 92(1), 93–94. Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2002). “MStrat Documentation 1.1.” Evolutionary genomics population management (GE\\(^{\\textrm{2}}\\)pop), Institut Amelioration Genetique et Adaptation des Plantes mediterraneennes et tropicales (agAp Institute), CIRAD, Montpellier, France. Schoen DJ, Brown AH (1993). “Conservation allelic richness wild crop relatives aided assessment genetic markers.” Proceedings National Academy Sciences, 90(22), 10623–10627.","code":""},{"path":[]},{"path":"https://aravind-j.github.io/avial/reference/parse_mstrat_out.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parse Output Files from MStrat — parse_mstrat_out","text":"","code":"if (FALSE) { # interactive()  parse_mstrat_out(data.file = \"MStrat_input_data.dat\",                  genotype = \"Accession\",                  variable.file = \"MStrat_input_variable.var\",                  kernel.file = \"MStrat_input_kernel.ker\",                  redundance.output = \"MStrat - Redundance.out\",                  core.output = \"MStrat - Core.out\") }"},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::%>% details.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Pipe operator — %>%","text":"lhs value magrittr placeholder. rhs function call using magrittr semantics.","code":""},{"path":"https://aravind-j.github.io/avial/reference/pipe.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Pipe operator — %>%","text":"result calling `rhs(lhs)`.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Input Files for MStrat — prep_mstrat_input","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"Prepare input files MStrat, software building germplasm core collections maximizing allelic phenotypic richness (Schoen Brown 1993; Gouesnard et al. 2001; Gouesnard et al. 2002) .","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"","code":"prep_mstrat_input(   data,   genotype,   qualitative,   quantitative,   active,   target,   center = TRUE,   scale = TRUE,   weights.qualitative = NULL,   weights.quantitative = NULL,   nclass.quantitative = NULL,   always.selected = NULL,   file.name = \"MStrat_input\",   folder.path = getwd() )"},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"data data data frame object. data frame possess columns genotype names multiple quantitative /qualitative trait/variable data. genotype Name column genotype names character string. qualitative Name columns qualitative traits character vector. quantitative Name columns quantitative traits character vector. active Name traits/variables declared active. target Name traits/variables declared target. center either logical value numeric-alike vector length     equal number columns x,     ‘numeric-alike’ means .numeric(.)     applied successfully .numeric(.) true. scale either logical value numeric-alike vector length     equal number columns x. weights.qualitative vector weight applied qualitative traits. NULL numeric vector length number qualitative traits. NULL, default weight 1 given. weights.quantitative vector weight applied quantitative traits. NULL numeric vector length number quantitative traits. NULL, default weight 1 given. nclass.quantitative number classes quantitative trait data divided . NULL integer vector length number quantitative traits. NULL, default 5 applied. MStat limits maximum number classes 1000. always.selected character vector names individuals genotype always selected core collection. maximum length accepted MStrat 500. file.name character string name file data saved. folder.path path folder input files saved.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"NA values considered missing value converted   9999 code MStrat.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2001). “MSTRAT: algorithm building germ plasm core collections maximizing allelic phenotypic richness.” Journal Heredity, 92(1), 93–94. Gouesnard B, Bataillon TM, Decoux G, Rozale C, Schoen DJ, David JL (2002). “MStrat Documentation 1.1.” Evolutionary genomics population management (GE\\(^{\\textrm{2}}\\)pop), Institut Amelioration Genetique et Adaptation des Plantes mediterraneennes et tropicales (agAp Institute), CIRAD, Montpellier, France. Schoen DJ, Brown AH (1993). “Conservation allelic richness wild crop relatives aided assessment genetic markers.” Proceedings National Academy Sciences, 90(22), 10623–10627.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_mstrat_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare Input Files for MStrat — prep_mstrat_input","text":"","code":"library(EvaluateCore) #> Registered S3 method overwritten by 'vegan': #>   method     from       #>   rev.hclust dendextend #>  #> -------------------------------------------------------------------------------- #> Welcome to EvaluateCore version 0.1.4 #>  #>  #> # To know whats new in this version type: #>   news(package='EvaluateCore') #>   for the NEWS file. #>  #> # To cite the methods in the package type: #>   citation(package='EvaluateCore') #>  #> # To suppress this message use: #>   suppressPackageStartupMessages(library(EvaluateCore)) #> --------------------------------------------------------------------------------  data(cassava_EC) data <- cassava_EC  quant <- c(\"NMSR\", \"TTRN\", \"TFWSR\", \"TTSW\", \"TTPW\", \"AVPW\",            \"ARSR\", \"SRDM\") qual <- c(\"CUAL\", \"LNGS\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Prepare genotype column data$Accession <- rownames(data) rownames(data) <- NULL data$Accession <- as.factor(data$Accession)  # Convert qualitative data as factors data[, qual] <- lapply(data[, qual],                        function(x) factor(as.factor(x)))  active = c(\"LNGS\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",            \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\",            \"TTRN\", \"TFWSR\", \"TTSW\", \"TTPW\", \"AVPW\") target = c(\"NMSR\", \"TTRN\", \"ARSR\", \"SRDM\",            \"CUAL\", \"LNGS\", \"TNPR9M\",            \"PL9M\", \"STRP\", \"STRC\",            \"PSTR\")  sel <- c(\"TMe-2906\", \"TMe-3412\", \"TMe-1374\", \"TMe-768\", \"TMe-14\",          \"TMe-3284\", \"TMe-937\", \"TMe-1859\", \"TMe-3265\", \"TMe-1739\",          \"TMe-972\", \"TMe-769\", \"TMe-3243\", \"TMe-3719\", \"TMe-1095\",          \"TMe-893\", \"TMe-1262\", \"TMe-2083\", \"TMe-376\", \"TMe-3633\",          \"TMe-1738\", \"TMe-2428\", \"TMe-259\", \"TMe-3457\", \"TMe-1406\",          \"TMe-977\", \"TMe-3006\", \"TMe-925\", \"TMe-3671\", \"TMe-2779\",          \"TMe-1293\", \"TMe-268\", \"TMe-266\", \"TMe-3562\", \"TMe-801\")  prep_mstrat_input(data = data, genotype = \"Accession\",                   qualitative = qual, quantitative = quant,                   active = active, target = target,                   center = TRUE, scale = TRUE,                   weights.qualitative = NULL,                   weights.quantitative = NULL,                   nclass.quantitative = NULL, always.selected = sel,                   file.name = \"MStrat_input\",                   folder.path = tempdir()) #> The following MStrat input files created at /var/folders/q_/pk8v8g4n3qq2gcnsvqjc3sd40000gn/T//RtmpVW71TE: #> MStrat_input_data.dat #> MStrat_input_variable.var #> MStrat_input_kernel.ker"},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":null,"dir":"Reference","previous_headings":"","what":"Prepare Input Files for PowerCore — prep_powercore_input","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"Prepare input files PowerCore, program applying advanced M strategy heuristic search establishing core sets (Kim et al. 2007; Kim et al. 2007) .","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"","code":"prep_powercore_input(   data,   genotype,   qualitative,   quantitative,   center = TRUE,   scale = TRUE,   always.selected = NULL,   file.name = \"PowerCore_input\",   folder.path = getwd() )"},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"data data data frame object. data frame possess columns genotype names multiple quantitative /qualitative trait/variable data. genotype Name column genotype names character string. qualitative Name columns qualitative traits character vector. quantitative Name columns quantitative traits character vector. center either logical value numeric-alike vector length     equal number columns x,     ‘numeric-alike’ means .numeric(.)     applied successfully .numeric(.) true. scale either logical value numeric-alike vector length     equal number columns x. always.selected character vector names individuals genotype always selected core collection. maximum length accepted MStrat 500. file.name character string name file data saved. folder.path path folder input files saved.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"Kim K, Chung H, Cho G, Ma K, Chandrabalan D, Gwag J, Kim T, Cho E, Park Y (2007). “PowerCore: program applying advanced M strategy heuristic search establishing core sets.” Bioinformatics, 23(16), 2155–2162. Kim K, Chung H, Cho G, Ma K, Chandrabalan D, Gwag J, Kim T, Cho E, Park Y (2007). “PowerCore (v. 1.0): Program Applying Advanced M Strategy Using Heuristic Search Establishing Core Allele Mining Sets - User Manual.” Genetic Resources Division, National Institute Agricultural Biotechnology (NIAB), Rural Development Administration (RDA), R. Korea.","code":""},{"path":"https://aravind-j.github.io/avial/reference/prep_powercore_input.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Prepare Input Files for PowerCore — prep_powercore_input","text":"","code":"library(EvaluateCore)  data(cassava_EC) data <- cassava_EC  quant <- c(\"NMSR\", \"TTRN\", \"TFWSR\", \"TTSW\", \"TTPW\", \"AVPW\",            \"ARSR\", \"SRDM\") qual <- c(\"CUAL\", \"LNGS\", \"LFRT\", \"LBTEF\", \"CBTR\", \"NMLB\",           \"ANGB\", \"CUAL9M\", \"LVC9M\", \"TNPR9M\", \"PL9M\", \"STRP\", \"STRC\",           \"PSTR\")  # Prepare genotype column data$Accession <- rownames(data) rownames(data) <- NULL data$Accession <- as.factor(data$Accession)  # Convert qualitative data as factors data[, qual] <- lapply(data[, qual],                        function(x) factor(as.factor(x)))  sel <- c(\"TMe-2906\", \"TMe-3412\", \"TMe-1374\", \"TMe-768\", \"TMe-14\",          \"TMe-3284\", \"TMe-937\", \"TMe-1859\", \"TMe-3265\", \"TMe-1739\",          \"TMe-972\", \"TMe-769\", \"TMe-3243\", \"TMe-3719\", \"TMe-1095\",          \"TMe-893\", \"TMe-1262\", \"TMe-2083\", \"TMe-376\", \"TMe-3633\",          \"TMe-1738\", \"TMe-2428\", \"TMe-259\", \"TMe-3457\", \"TMe-1406\",          \"TMe-977\", \"TMe-3006\", \"TMe-925\", \"TMe-3671\", \"TMe-2779\",          \"TMe-1293\", \"TMe-268\", \"TMe-266\", \"TMe-3562\", \"TMe-801\")  prep_powercore_input(data = data, genotype = \"Accession\",                      qualitative = qual, quantitative = quant,                      center = TRUE, scale = TRUE,                      always.selected = sel,                      file.name = \"PowerCore_input\",                      folder.path = tempdir()) #> PowerCore output file created at /var/folders/q_/pk8v8g4n3qq2gcnsvqjc3sd40000gn/T//RtmpVW71TE/PowerCore_input.csv"},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":null,"dir":"Reference","previous_headings":"","what":"Remove scales from ggplot objects — remove_scales","title":"Remove scales from ggplot objects — remove_scales","text":"Useful avoiding warning Scale * already present. Adding another scale *, replace existing scale.","code":""},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Remove scales from ggplot objects — remove_scales","text":"","code":"remove_scales(g, scales)"},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Remove scales from ggplot objects — remove_scales","text":"g ggplot object. scales scales removed character vector.","code":""},{"path":"https://aravind-j.github.io/avial/reference/remove_scales.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Remove scales from ggplot objects — remove_scales","text":"ggplot object without scales specified.","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":null,"dir":"Reference","previous_headings":"","what":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"function reorders tips (leaves) dendrogram elements belonging predefined groups appear together. useful hierarchical clustering visualization desired clusters groups appear contiguously.","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"","code":"reorder_dend_by_groups(dend, groups_list)"},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"dend object class dendrogram. groups_list list character vectors. vector contains tip labels kept adjacent. e.g., list(g1=c(\"\",\"b\"), g2=c(\"c\",\"d\")).","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"reordered dendrogram object.","code":""},{"path":"https://aravind-j.github.io/avial/reference/reorder_dend_by_groups.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Reorder a dendrogram to keep specified groups of tips adjacent — reorder_dend_by_groups","text":"","code":"set.seed(42)  mat <- matrix(rnorm(12*3), nrow = 12) rownames(mat) <- paste0(\"T\", 1:12)  # Hierarchical clustering hc <- hclust(dist(mat)) dend <- as.dendrogram(hc)  # Plot plot(dend, main = \"Original dendrogram\")    # Groups that align with the dendrogram structure #' groups_preserved <- list(   GroupA = c(\"T1\", \"T2\", \"T3\"),   GroupB = c(\"T4\", \"T5\", \"T6\"),   GroupC = c(\"T7\", \"T8\") )  dend_preserved <- reorder_dend_by_groups(dend, groups_preserved) #> Warning: The following groups could not be kept together due to dendrogram topology constraints: #> T1, T2, T3 #> T4, T5, T6 #> T7, T8  plot(dend_preserved, main = \"Groups preserved (no warning)\")   # Define groups that do NOT align with dendrogram # These tips are far apart in the dendrogram groups_split <- list(   GroupX = c(\"T1\", \"T5\", \"T9\"),   GroupY = c(\"T2\", \"T6\", \"T10\") )  dend_split <- reorder_dend_by_groups(dend, groups_split) #> Warning: The following groups could not be kept together due to dendrogram topology constraints: #> T1, T5, T9 #> T2, T6, T10  # Plot plot(dend_split, main = \"Groups split (warning expected)\")"},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"Summary Statistics Qualitative Quantitative trait data","code":""},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"","code":"summary_quant(data, group = NULL, trait, out.format = c(\"long\", \"wide\"))  summary_qual(data, group = NULL, trait, out.format = c(\"long\", \"wide\"))"},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"data data data frame object. data frame possess columns specifying trait group. group Name column specifying group character string. trait Name column specifying trait character string. trait column type \"numeric\" quantitative traits type \"factor\" qualitative traits. .format output format.","code":""},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"tibble data.frame summary statistics.","code":""},{"path":"https://aravind-j.github.io/avial/reference/summary_quant.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summary Statistics of Qualitative and Quantitative trait data — summary_quant","text":"","code":"library(agridat) library(dplyr)  soydata <- australia.soybean  soydata$year <- as.factor(soydata$year)  quant_traits <- c(\"yield\", \"height\", \"lodging\",                   \"size\", \"protein\", \"oil\") set.seed(123) soydata <-   soydata %>%   mutate(     across(       .cols = all_of(quant_traits),       .fns = ~factor(cut(.x, breaks = quantile(.x, na.rm = TRUE),                          include.lowest = TRUE),                      labels = sample(1:9, size = 4)),       .names = \"{.col}_score\"     )   )  qual_traits <- c(\"yield_score\", \"height_score\", \"lodging_score\",                  \"size_score\", \"protein_score\", \"oil_score\")  summary_quant(data = soydata, trait = quant_traits) #> # A tibble: 6 × 12 #>   Trait   count miss.count   mean    min   max    sd     se    skew skew.pvalue #>   <chr>   <int>      <int>  <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl>       <dbl> #> 1 yield     464          0  2.05   0.282  4.38 0.752 0.0349 0.0249     8.24e- 1 #> 2 height    464          0  0.883  0.25   1.73 0.272 0.0126 0.278      1.47e- 2 #> 3 lodging   464          0  2.31   1      4.75 0.976 0.0453 0.414      3.66e- 4 #> 4 size      464          0 11.1    4     23.6  4.45  0.207  0.892      4.29e-12 #> 5 protein   464          0 40.3   33.2   48.5  2.93  0.136  0.280      1.42e- 2 #> 6 oil       464          0 19.9   13.0   26.8  2.67  0.124  0.00142    9.90e- 1 #> # ℹ 2 more variables: kurt <dbl>, kurt.pvalue <dbl> summary_qual(data = soydata, trait = qual_traits) #> # A tibble: 6 × 4 #>   Trait         count miss.count freq                               #>   <chr>         <int>      <int> <chr>                              #> 1 yield_score     464          0 3 (117), 6 (115), 9 (116), 2 (116) #> 2 height_score    464          0 2 (116), 6 (116), 3 (117), 5 (115) #> 3 lodging_score   464          0 4 (143), 6 (114), 8 (107), 1 (100) #> 4 size_score      464          0 5 (116), 3 (118), 8 (116), 1 (114) #> 5 protein_score   464          0 9 (116), 1 (120), 5 (114), 3 (114) #> 6 oil_score       464          0 8 (116), 2 (116), 7 (116), 9 (116)  summary_quant(data = soydata, group = \"loc\", trait = quant_traits) #> # A tibble: 24 × 13 #>    loc        Trait   count miss.count   mean    min   max    sd     se    skew #>    <fct>      <chr>   <int>      <int>  <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl> #>  1 Brookstead yield     116          0  2.01   0.385  3.90 0.825 0.0766  0.108  #>  2 Brookstead height    116          0  0.983  0.54   1.37 0.185 0.0172 -0.529  #>  3 Brookstead lodging   116          0  2.52   1      4.5  0.744 0.0691 -0.176  #>  4 Brookstead size      116          0 11.7    5.4   23.6  4.82  0.448   0.931  #>  5 Brookstead protein   116          0 40.9   34.2   48.5  2.74  0.254   0.381  #>  6 Brookstead oil       116          0 19.2   14.7   25.0  2.23  0.207   0.441  #>  7 Lawes      yield     116          0  2.37   0.586  4.38 0.654 0.0607 -0.129  #>  8 Lawes      height    116          0  1.02   0.405  1.73 0.324 0.0300  0.219  #>  9 Lawes      lodging   116          0  2.82   1      4.75 1.11  0.103   0.0210 #> 10 Lawes      size      116          0 11.7    6.55  23.2  4.32  0.401   1.01   #> # ℹ 14 more rows #> # ℹ 3 more variables: skew.pvalue <dbl>, kurt <dbl>, kurt.pvalue <dbl> summary_qual(data = soydata, group = \"loc\", trait = qual_traits) #> # A tibble: 24 × 5 #>    loc        Trait         count miss.count freq                           #>    <fct>      <chr>         <int>      <int> <chr>                          #>  1 Brookstead yield_score     116          0 3 (35), 6 (23), 9 (31), 2 (27) #>  2 Brookstead height_score    116          0 2 (12), 6 (17), 3 (39), 5 (48) #>  3 Brookstead lodging_score   116          0 4 (18), 6 (24), 8 (53), 1 (21) #>  4 Brookstead size_score      116          0 5 (24), 3 (31), 8 (31), 1 (30) #>  5 Brookstead protein_score   116          0 9 (19), 1 (31), 5 (33), 3 (33) #>  6 Brookstead oil_score       116          0 8 (36), 2 (41), 7 (21), 9 (18) #>  7 Lawes      yield_score     116          0 3 (16), 6 (20), 9 (31), 2 (49) #>  8 Lawes      height_score    116          0 2 (17), 6 (28), 3 (23), 5 (48) #>  9 Lawes      lodging_score   116          0 4 (20), 6 (27), 8 (19), 1 (50) #> 10 Lawes      size_score      116          0 5 (16), 3 (34), 8 (37), 1 (29) #> # ℹ 14 more rows  summary_quant(data = soydata, group = c(\"loc\", \"year\"), trait = quant_traits) #> # A tibble: 48 × 14 #> # Groups:   loc [4] #>    loc     year  Trait count miss.count   mean    min   max    sd     se    skew #>    <fct>   <fct> <chr> <int>      <int>  <dbl>  <dbl> <dbl> <dbl>  <dbl>   <dbl> #>  1 Brooks… 1970  yield    58          0  1.56   0.385  3.16 0.736 0.0967  0.392  #>  2 Brooks… 1970  heig…    58          0  1.02   0.54   1.32 0.186 0.0244 -0.787  #>  3 Brooks… 1970  lodg…    58          0  2.35   1      4.5  0.756 0.0993  0.0412 #>  4 Brooks… 1970  size     58          0 10.8    5.4   22.1  4.60  0.604   1.04   #>  5 Brooks… 1970  prot…    58          0 40.1   34.2   45.8  2.43  0.319   0.299  #>  6 Brooks… 1970  oil      58          0 19.5   15.7   25.0  2.16  0.284   0.595  #>  7 Brooks… 1971  yield    58          0  2.46   1.11   3.90 0.649 0.0852  0.481  #>  8 Brooks… 1971  heig…    58          0  0.949  0.57   1.37 0.179 0.0235 -0.345  #>  9 Brooks… 1971  lodg…    58          0  2.69   1      4.5  0.699 0.0917 -0.359  #> 10 Brooks… 1971  size     58          0 12.6    6.85  23.6  4.93  0.647   0.853  #> # ℹ 38 more rows #> # ℹ 3 more variables: skew.pvalue <dbl>, kurt <dbl>, kurt.pvalue <dbl> summary_qual(data = soydata, group = c(\"loc\", \"year\"), trait = qual_traits) #> # A tibble: 48 × 6 #> # Groups:   loc [4] #>    loc        year  Trait         count miss.count freq                          #>    <fct>      <fct> <chr>         <int>      <int> <chr>                         #>  1 Brookstead 1970  yield_score      58          0 3 (33), 6 (9), 9 (9), 2 (7)   #>  2 Brookstead 1970  height_score     58          0 2 (4), 6 (8), 3 (16), 5 (30)  #>  3 Brookstead 1970  lodging_score    58          0 4 (12), 6 (14), 8 (25), 1 (7) #>  4 Brookstead 1970  size_score       58          0 5 (18), 3 (14), 8 (12), 1 (1… #>  5 Brookstead 1970  protein_score    58          0 9 (12), 1 (19), 5 (17), 3 (1… #>  6 Brookstead 1970  oil_score        58          0 8 (17), 2 (18), 7 (13), 9 (1… #>  7 Brookstead 1971  yield_score      58          0 3 (2), 6 (14), 9 (22), 2 (20) #>  8 Brookstead 1971  height_score     58          0 2 (8), 6 (9), 3 (23), 5 (18)  #>  9 Brookstead 1971  lodging_score    58          0 4 (6), 6 (10), 8 (28), 1 (14) #> 10 Brookstead 1971  size_score       58          0 5 (6), 3 (17), 8 (19), 1 (16) #> # ℹ 38 more rows"},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"Plot List ggplot2 Barplots Grid Uniform Width Bars","code":""},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"","code":"uniform_barwidth_grid(plot.list, level.count, nrow, ncol)"},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"plot.list list bar plots ggplot2 objects. level.count count levels/bars plot. nrow Number rows grid. ncol Number columns grid.","code":""},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"plot grid patchwork object.","code":""},{"path":"https://aravind-j.github.io/avial/reference/uniform_barwidth_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Plot a List of ggplot2 Barplots as a Grid with with Uniform Width of the Bars — uniform_barwidth_grid","text":"","code":"# Load ggplot2 library(ggplot2) library(patchwork)  # Barplot 1 ---- data1 <- data.frame(   category = c(\"A\", \"B\", \"C\"),   value = c(10, 15, 8) )  plot1 <- ggplot(data1, aes(x = category, y = value, fill = category)) +   geom_bar(stat = \"identity\") +   ggtitle(\"Plot 1: 3 bars\") +   theme_minimal()  # Barplot 2 ---- data2 <- data.frame(   category = c(\"W\", \"X\", \"Y\", \"Z\", \"V\"),   value = c(12, 9, 20, 7, 14) )  plot2 <- ggplot(data2, aes(x = category, y = value, fill = category)) +   geom_bar(stat = \"identity\") +   ggtitle(\"Plot 2: 5 bars\") +   theme_minimal()  # Barplot 3 ---- data3 <- data.frame(   category = c(\"P\", \"Q\", \"R\", \"S\"),   value = c(5, 18, 11, 9) )  plot3 <- ggplot(data3, aes(x = category, y = value, fill = category)) +   geom_bar(stat = \"identity\") +   ggtitle(\"Plot 3: 4 bars\") +   theme_minimal()  # Plot originals with patchwork wrap_plots(plot1, plot2, plot3, nrow = 2)   # Plot to get uniform bar widths uniform_barwidth_grid(list(plot1, plot2, plot3),                       level.count = c(3, 5, 4), nrow = 2, ncol = 2) #> Warning: data length is not a multiple of split variable #> Warning: data length is not a multiple of split variable"}]
